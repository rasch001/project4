{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report: GPT-2 Training – Architecture and Methodology\n",
    "\n",
    "This report explains the GPT-2 training process, its architecture, and the mathematical foundations behind its operation. The GPT-2 model is a large-scale language model based on the Transformer architecture. Its primary training objective is **next-token prediction** – predicting the next token in a sequence given the preceding tokens.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Overview of GPT-2\n",
    "\n",
    "GPT-2 is designed as a decoder-only Transformer model. Unlike encoder–decoder architectures used in machine translation, GPT-2 uses only the Transformer decoder, which is autoregressive. This means that it generates text sequentially by predicting one token at a time, using the previously generated tokens as context.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Model Architecture\n",
    "\n",
    "The GPT-2 architecture consists of several key components:\n",
    "\n",
    "### 2.1 Token and Positional Embeddings\n",
    "\n",
    "Each input token is mapped to a high-dimensional vector using a token embedding matrix. Since the Transformer has no inherent sense of token order, positional embeddings are added to provide sequential information.\n",
    "\n",
    "- **Token Embedding:**  \n",
    "  Given a vocabulary size $V$ and embedding dimension $d_{model}$, the token embedding matrix is:\n",
    "  $$ E \\in \\mathbb{R}^{V \\times d_{model}} $$\n",
    "\n",
    "- **Positional Embedding:**  \n",
    "  For a sequence of maximum length $L$, the positional embedding matrix is:\n",
    "  $$ P \\in \\mathbb{R}^{L \\times d_{model}} $$\n",
    "\n",
    "For an input sequence of token IDs $x = [x_1, x_2, \\dots, x_n]$, the input representation is computed as:\n",
    "$$\n",
    "\\text{InputEmbedding}(x) = E[x] + P[0:n]\n",
    "$$\n",
    "\n",
    "### 2.2 Transformer Blocks\n",
    "\n",
    "GPT-2 consists of $N$ identical Transformer blocks. Each block contains two sub-layers: a multi-head self-attention mechanism and a feedforward neural network.\n",
    "\n",
    "#### 2.2.1 Multi-Head Self-Attention\n",
    "\n",
    "The self-attention mechanism allows the model to focus on different parts of the sequence simultaneously. In multi-head attention, the input is linearly projected into query ($Q$), key ($K$), and value ($V$) vectors. For each attention head, the output is computed as:\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "where:\n",
    "- $d_k$ is the dimension of each head.\n",
    "- The softmax function ensures the weights sum to one.\n",
    "\n",
    "Multiple heads allow the model to capture diverse relationships:\n",
    "$$\n",
    "\\text{MultiHead}(x) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h)W^O\n",
    "$$\n",
    "with $W^O$ being a learned projection matrix.\n",
    "\n",
    "#### 2.2.2 Feedforward Network\n",
    "\n",
    "After self-attention, a position-wise feedforward network is applied. The feedforward network consists of several linear layers interleaved with non-linear activation functions (typically ReLU). Mathematically, for an input $z$, the feedforward network can be described as:\n",
    "$$\n",
    "\\text{FFN}(z) = W_2 \\cdot \\text{ReLU}(W_1 \\cdot z + b_1) + b_2\n",
    "$$\n",
    "\n",
    "In our implementation, this feedforward network is extended into 10 sequential layers, each applying:\n",
    "$$\n",
    "z_{i+1} = \\text{ReLU}(W_i z_i + b_i)\n",
    "$$\n",
    "for $i = 1, \\dots, 10$, with $z_1 = z$.\n",
    "\n",
    "#### 2.2.3 Residual Connections and Layer Normalization\n",
    "\n",
    "Both sub-layers (self-attention and feedforward) use residual connections and layer normalization:\n",
    "$$\n",
    "\\tilde{z} = \\text{LayerNorm}(z + \\text{Sublayer}(z))\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Training Objective: Next-Token Prediction\n",
    "\n",
    "GPT-2 is trained using the next-token prediction task. The goal is to maximize the likelihood of the next token given the previous tokens.\n",
    "\n",
    "Given a sequence of tokens $x = [x_1, x_2, \\dots, x_n]$, the training objective is to maximize the conditional probability:\n",
    "$$\n",
    "P(x) = \\prod_{t=1}^{n} P(x_t \\mid x_{<t})\n",
    "$$\n",
    "\n",
    "### 3.1 Loss Function\n",
    "\n",
    "The loss function used is the cross-entropy loss. For each token prediction, if $\\hat{y}_t$ is the model's output logits for token $t$, and $y_t$ is the true token, the loss is given by:\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{t=1}^{n} \\log P(y_t \\mid x_{<t}) = -\\sum_{t=1}^{n} \\log \\left( \\frac{\\exp(\\hat{y}_{t, y_t})}{\\sum_{j=1}^{V} \\exp(\\hat{y}_{t, j})} \\right)\n",
    "$$\n",
    "\n",
    "This loss is averaged over all tokens in the batch and serves as the training signal for backpropagation.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Autoregressive Text Generation\n",
    "\n",
    "Once trained, GPT-2 generates text autoregressively. Starting from a prompt, the model predicts one token at a time. At each step, the most recently generated tokens are used as input, and the model computes a probability distribution over the vocabulary for the next token:\n",
    "$$\n",
    "x_{t+1} \\sim \\text{softmax}\\left(\\frac{\\hat{y}_{t}}{\\text{temperature}}\\right)\n",
    "$$\n",
    "\n",
    "The **temperature** parameter controls randomness:\n",
    "- A temperature $< 1$ makes the model more confident (less random).\n",
    "- A temperature $> 1$ increases randomness.\n",
    "\n",
    "The generation loop terminates when an end-of-sequence token is produced or after a predetermined maximum length.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Summary\n",
    "\n",
    "The GPT-2 model leverages the power of the Transformer architecture to model complex language patterns through self-attention and deep feedforward networks. The training process uses next-token prediction with a cross-entropy loss, guiding the model to capture long-range dependencies in text. Through iterative, autoregressive generation, GPT-2 is capable of producing coherent and contextually relevant text.\n",
    "\n",
    "This report outlines the mathematical underpinnings and architecture of GPT-2, providing insight into how the model processes input tokens, learns representations, and generates text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus loaded. Corpus length: 10929707\n",
      "Epoch 1 Batch 0: Loss = 11.4190\n",
      "Epoch 1 Batch 10: Loss = 11.1744\n",
      "Epoch 1 Batch 20: Loss = 10.7312\n",
      "Epoch 1 Batch 30: Loss = 10.2201\n",
      "Epoch 1 Batch 40: Loss = 9.7670\n",
      "Epoch 1 Batch 50: Loss = 9.4062\n",
      "Epoch 1 Batch 60: Loss = 9.2608\n",
      "Epoch 1 Batch 70: Loss = 8.8148\n",
      "Epoch 1 Batch 80: Loss = 8.6505\n",
      "Epoch 1 Batch 90: Loss = 8.4590\n",
      "Epoch 1 Batch 100: Loss = 8.1591\n",
      "Epoch 1 Batch 110: Loss = 7.7683\n",
      "Epoch 1 Batch 120: Loss = 7.7228\n",
      "Epoch 1 Batch 130: Loss = 7.4813\n",
      "Epoch 1 Batch 140: Loss = 7.3624\n",
      "Epoch 1 Batch 150: Loss = 7.5579\n",
      "Epoch 1 Batch 160: Loss = 7.0650\n",
      "Epoch 1 Batch 170: Loss = 6.8210\n",
      "Epoch 1 Batch 180: Loss = 6.9552\n",
      "Epoch 1 Batch 190: Loss = 6.6358\n",
      "Epoch 1 Batch 200: Loss = 6.5709\n",
      "Epoch 1 Batch 210: Loss = 6.6885\n",
      "Epoch 1 Batch 220: Loss = 6.4037\n",
      "Epoch 1 Batch 230: Loss = 6.5217\n",
      "Epoch 1 Batch 240: Loss = 6.3920\n",
      "Epoch 1 Batch 250: Loss = 6.2245\n",
      "Epoch 1 Batch 260: Loss = 6.3280\n",
      "Epoch 1 Batch 270: Loss = 6.1458\n",
      "Epoch 1 Batch 280: Loss = 6.0368\n",
      "Epoch 1 Batch 290: Loss = 6.0589\n",
      "Epoch 1 Batch 300: Loss = 5.9763\n",
      "Epoch 1 Batch 310: Loss = 5.7769\n",
      "Epoch 1 Batch 320: Loss = 5.8635\n",
      "Epoch 1 Batch 330: Loss = 5.7681\n",
      "Epoch 1 Batch 340: Loss = 5.5714\n",
      "Epoch 1 Batch 350: Loss = 6.1564\n",
      "Epoch 1 Batch 360: Loss = 5.9589\n",
      "Epoch 1 Batch 370: Loss = 5.7777\n",
      "Epoch 1 Batch 380: Loss = 5.8135\n",
      "Epoch 1 Batch 390: Loss = 5.8732\n",
      "Epoch 1 Batch 400: Loss = 5.5194\n",
      "Epoch 1 Batch 410: Loss = 5.7952\n",
      "Epoch 1 Batch 420: Loss = 5.6840\n",
      "Epoch 1 Batch 430: Loss = 5.6652\n",
      "Epoch 1 Batch 440: Loss = 5.4270\n",
      "Epoch 1 Batch 450: Loss = 5.4842\n",
      "Epoch 1 Batch 460: Loss = 5.6649\n",
      "Epoch 1 Batch 470: Loss = 5.6415\n",
      "Epoch 1 Batch 480: Loss = 5.7489\n",
      "Epoch 1 Batch 490: Loss = 5.4641\n",
      "Epoch 1 Batch 500: Loss = 5.4073\n",
      "Epoch 1 Batch 510: Loss = 5.3471\n",
      "Epoch 1 Batch 520: Loss = 5.3341\n",
      "Epoch 1 Batch 530: Loss = 5.2676\n",
      "Epoch 1 Batch 540: Loss = 5.2377\n",
      "Epoch 1 Batch 550: Loss = 5.3947\n",
      "Epoch 1 Batch 560: Loss = 5.3970\n",
      "Epoch 1 Batch 570: Loss = 5.3420\n",
      "Epoch 1 Batch 580: Loss = 5.2347\n",
      "Epoch 1 Batch 590: Loss = 5.1077\n",
      "Epoch 1 Batch 600: Loss = 5.2009\n",
      "Epoch 1 Batch 610: Loss = 5.2877\n",
      "Epoch 1 Batch 620: Loss = 4.9987\n",
      "Epoch 1 Batch 630: Loss = 5.5005\n",
      "Epoch 1 Batch 640: Loss = 5.2088\n",
      "Epoch 1 Batch 650: Loss = 5.2401\n",
      "Epoch 1 Batch 660: Loss = 5.2789\n",
      "Epoch 1 Batch 670: Loss = 5.0469\n",
      "Epoch 1 Batch 680: Loss = 5.0475\n",
      "Epoch 1 Batch 690: Loss = 5.2143\n",
      "Epoch 1 Batch 700: Loss = 4.9770\n",
      "Epoch 1 Batch 710: Loss = 4.5337\n",
      "Epoch 1 Batch 720: Loss = 5.0276\n",
      "Epoch 1 Batch 730: Loss = 4.9716\n",
      "Epoch 1 Batch 740: Loss = 5.1312\n",
      "Epoch 1 Batch 750: Loss = 4.6785\n",
      "Epoch 1 Batch 760: Loss = 4.9240\n",
      "Epoch 1 Batch 770: Loss = 4.6595\n",
      "Epoch 1 Batch 780: Loss = 4.7959\n",
      "Epoch 1 Batch 790: Loss = 4.8026\n",
      "Epoch 1 Batch 800: Loss = 4.6201\n",
      "Epoch 1 Batch 810: Loss = 4.5760\n",
      "Epoch 1 Batch 820: Loss = 4.7953\n",
      "Epoch 1 Batch 830: Loss = 4.7259\n",
      "Epoch 1 Batch 840: Loss = 4.6941\n",
      "Epoch 1 Batch 850: Loss = 4.6388\n",
      "Epoch 1 Batch 860: Loss = 4.7153\n",
      "Epoch 1 Batch 870: Loss = 4.7371\n",
      "Epoch 1 Batch 880: Loss = 4.8764\n",
      "Epoch 1 Batch 890: Loss = 4.5506\n",
      "Epoch 1 Batch 900: Loss = 4.7362\n",
      "Epoch 1 Batch 910: Loss = 4.2035\n",
      "Epoch 1 Batch 920: Loss = 4.3956\n",
      "Epoch 1 Batch 930: Loss = 4.5344\n",
      "Epoch 1 Batch 940: Loss = 4.3927\n",
      "Epoch 1 Batch 950: Loss = 4.3994\n",
      "Epoch 1 Batch 960: Loss = 4.4408\n",
      "Epoch 1 Batch 970: Loss = 4.5241\n",
      "Epoch 1 Batch 980: Loss = 4.5589\n",
      "Epoch 1 Batch 990: Loss = 4.2799\n",
      "Epoch 1 Batch 1000: Loss = 4.4196\n",
      "Epoch 1 Batch 1010: Loss = 4.3920\n",
      "Epoch 1 Batch 1020: Loss = 4.3672\n",
      "Epoch 1 Batch 1030: Loss = 4.3283\n",
      "Epoch 1 Batch 1040: Loss = 4.2754\n",
      "Epoch 1 Batch 1050: Loss = 4.1401\n",
      "Epoch 1 Batch 1060: Loss = 4.3814\n",
      "Epoch 1 Batch 1070: Loss = 4.1495\n",
      "Epoch 1 Batch 1080: Loss = 4.2699\n",
      "Epoch 1 Batch 1090: Loss = 4.0507\n",
      "Epoch 1 Batch 1100: Loss = 3.9660\n",
      "Epoch 1 Batch 1110: Loss = 4.5336\n",
      "Epoch 1 Batch 1120: Loss = 4.1918\n",
      "Epoch 1 Batch 1130: Loss = 3.9872\n",
      "Epoch 1 Batch 1140: Loss = 4.0006\n",
      "Epoch 1 Batch 1150: Loss = 4.4148\n",
      "Epoch 1 Batch 1160: Loss = 4.1410\n",
      "Epoch 1 Batch 1170: Loss = 4.2162\n",
      "Epoch 1 Batch 1180: Loss = 4.6649\n",
      "Epoch 1 Batch 1190: Loss = 3.8659\n",
      "Epoch 1 Batch 1200: Loss = 4.4079\n",
      "Epoch 1 Batch 1210: Loss = 3.9463\n",
      "Epoch 1 Batch 1220: Loss = 4.1394\n",
      "Epoch 1 Batch 1230: Loss = 3.9870\n",
      "Epoch 1 Batch 1240: Loss = 3.8332\n",
      "Epoch 1 Batch 1250: Loss = 3.7921\n",
      "Epoch 1 Batch 1260: Loss = 4.0490\n",
      "Epoch 1 Batch 1270: Loss = 3.6470\n",
      "Epoch 1 Batch 1280: Loss = 3.9480\n",
      "Epoch 1 Batch 1290: Loss = 3.9796\n",
      "Epoch 1 Batch 1300: Loss = 4.2038\n",
      "Epoch 1 Batch 1310: Loss = 3.5991\n",
      "Epoch 1 Batch 1320: Loss = 3.8926\n",
      "Epoch 1 Batch 1330: Loss = 3.9745\n",
      "Epoch 1 Batch 1340: Loss = 3.7544\n",
      "Epoch 1 Batch 1350: Loss = 3.9477\n",
      "Epoch 1 Batch 1360: Loss = 3.7824\n",
      "Epoch 1 Batch 1370: Loss = 3.8920\n",
      "Epoch 1 Batch 1380: Loss = 4.0875\n",
      "Epoch 1 Batch 1390: Loss = 3.5479\n",
      "Epoch 1 Batch 1400: Loss = 3.8053\n",
      "Epoch 1 Batch 1410: Loss = 3.8973\n",
      "Epoch 1 Batch 1420: Loss = 3.8138\n",
      "Epoch 1 Batch 1430: Loss = 3.9564\n",
      "Epoch 1 Batch 1440: Loss = 3.9755\n",
      "Epoch 1 Batch 1450: Loss = 4.2299\n",
      "Epoch 1 Batch 1460: Loss = 3.6718\n",
      "Epoch 1 Batch 1470: Loss = 3.9570\n",
      "Epoch 1 Batch 1480: Loss = 3.6606\n",
      "Epoch 1 Batch 1490: Loss = 3.6467\n",
      "Epoch 1 Batch 1500: Loss = 3.5895\n",
      "Epoch 1 Batch 1510: Loss = 3.7446\n",
      "Epoch 1 Batch 1520: Loss = 3.3589\n",
      "Epoch 1 Batch 1530: Loss = 4.1944\n",
      "Epoch 1 Batch 1540: Loss = 3.8428\n",
      "Epoch 1 Batch 1550: Loss = 3.7742\n",
      "Epoch 1 Batch 1560: Loss = 3.8347\n",
      "Epoch 1 Batch 1570: Loss = 4.0151\n",
      "Epoch 1 Batch 1580: Loss = 3.8753\n",
      "Epoch 1 Batch 1590: Loss = 3.6540\n",
      "Epoch 1 Batch 1600: Loss = 3.3824\n",
      "Epoch 1 Batch 1610: Loss = 3.5747\n",
      "Epoch 1 Batch 1620: Loss = 3.3120\n",
      "Epoch 1 Batch 1630: Loss = 3.8716\n",
      "Epoch 1 Batch 1640: Loss = 3.7691\n",
      "Epoch 1 Batch 1650: Loss = 3.8145\n",
      "Epoch 1 Batch 1660: Loss = 3.6424\n",
      "Epoch 1 Batch 1670: Loss = 3.4896\n",
      "Epoch 1 Batch 1680: Loss = 3.3524\n",
      "Epoch 1 Batch 1690: Loss = 3.6528\n",
      "Epoch 1 Batch 1700: Loss = 3.7825\n",
      "Epoch 1 Batch 1710: Loss = 3.5287\n",
      "Epoch 1 Batch 1720: Loss = 3.6287\n",
      "Epoch 1 Batch 1730: Loss = 3.4205\n",
      "Epoch 1 Batch 1740: Loss = 3.3062\n",
      "Epoch 1 Batch 1750: Loss = 3.6766\n",
      "Epoch 1 Batch 1760: Loss = 3.8522\n",
      "Epoch 1 Batch 1770: Loss = 3.3750\n",
      "Epoch 1 Batch 1780: Loss = 3.3881\n",
      "Epoch 1 Batch 1790: Loss = 3.6705\n",
      "Epoch 1 Batch 1800: Loss = 3.6252\n",
      "Epoch 1 Batch 1810: Loss = 3.1158\n",
      "Epoch 1 Batch 1820: Loss = 3.1154\n",
      "Epoch 1 Batch 1830: Loss = 3.4622\n",
      "Epoch 1 Batch 1840: Loss = 3.6567\n",
      "Epoch 1 Batch 1850: Loss = 3.4809\n",
      "Epoch 1 Batch 1860: Loss = 3.6479\n",
      "Epoch 1 Batch 1870: Loss = 3.3349\n",
      "Epoch 1 Batch 1880: Loss = 3.3228\n",
      "Epoch 1 Batch 1890: Loss = 3.3859\n",
      "Epoch 1 Batch 1900: Loss = 3.5568\n",
      "Epoch 1 Batch 1910: Loss = 3.6218\n",
      "Epoch 1 Batch 1920: Loss = 3.3190\n",
      "Epoch 1 Batch 1930: Loss = 3.5243\n",
      "Epoch 1 Batch 1940: Loss = 3.4839\n",
      "Epoch 1 Batch 1950: Loss = 3.3276\n",
      "Epoch 1 Batch 1960: Loss = 3.0541\n",
      "Epoch 1 Batch 1970: Loss = 3.4887\n",
      "Epoch 1 Batch 1980: Loss = 3.3342\n",
      "Epoch 1 Batch 1990: Loss = 3.3650\n",
      "Epoch 1 Batch 2000: Loss = 3.1092\n",
      "Epoch 1 Average Loss: 4.8387\n",
      "Epoch 2 Batch 0: Loss = 3.2413\n",
      "Epoch 2 Batch 10: Loss = 3.2311\n",
      "Epoch 2 Batch 20: Loss = 2.9263\n",
      "Epoch 2 Batch 30: Loss = 3.1804\n",
      "Epoch 2 Batch 40: Loss = 3.1430\n",
      "Epoch 2 Batch 50: Loss = 3.4827\n",
      "Epoch 2 Batch 60: Loss = 3.2586\n",
      "Epoch 2 Batch 70: Loss = 3.2761\n",
      "Epoch 2 Batch 80: Loss = 3.0705\n",
      "Epoch 2 Batch 90: Loss = 3.0357\n",
      "Epoch 2 Batch 100: Loss = 3.2810\n",
      "Epoch 2 Batch 110: Loss = 3.1360\n",
      "Epoch 2 Batch 120: Loss = 3.0515\n",
      "Epoch 2 Batch 130: Loss = 2.9957\n",
      "Epoch 2 Batch 140: Loss = 2.9790\n",
      "Epoch 2 Batch 150: Loss = 2.9832\n",
      "Epoch 2 Batch 160: Loss = 3.0036\n",
      "Epoch 2 Batch 170: Loss = 2.9990\n",
      "Epoch 2 Batch 180: Loss = 2.8824\n",
      "Epoch 2 Batch 190: Loss = 2.7229\n",
      "Epoch 2 Batch 200: Loss = 2.9238\n",
      "Epoch 2 Batch 210: Loss = 3.2534\n",
      "Epoch 2 Batch 220: Loss = 3.1569\n",
      "Epoch 2 Batch 230: Loss = 2.8780\n",
      "Epoch 2 Batch 240: Loss = 2.8463\n",
      "Epoch 2 Batch 250: Loss = 3.1291\n",
      "Epoch 2 Batch 260: Loss = 2.8739\n",
      "Epoch 2 Batch 270: Loss = 2.7288\n",
      "Epoch 2 Batch 280: Loss = 3.1011\n",
      "Epoch 2 Batch 290: Loss = 2.8115\n",
      "Epoch 2 Batch 300: Loss = 2.9780\n",
      "Epoch 2 Batch 310: Loss = 3.2266\n",
      "Epoch 2 Batch 320: Loss = 2.8757\n",
      "Epoch 2 Batch 330: Loss = 3.1046\n",
      "Epoch 2 Batch 340: Loss = 3.0635\n",
      "Epoch 2 Batch 350: Loss = 3.1946\n",
      "Epoch 2 Batch 360: Loss = 2.7087\n",
      "Epoch 2 Batch 370: Loss = 2.6949\n",
      "Epoch 2 Batch 380: Loss = 2.8399\n",
      "Epoch 2 Batch 390: Loss = 2.8586\n",
      "Epoch 2 Batch 400: Loss = 2.9064\n",
      "Epoch 2 Batch 410: Loss = 3.0218\n",
      "Epoch 2 Batch 420: Loss = 3.1653\n",
      "Epoch 2 Batch 430: Loss = 2.6892\n",
      "Epoch 2 Batch 440: Loss = 2.8444\n",
      "Epoch 2 Batch 450: Loss = 2.7284\n",
      "Epoch 2 Batch 460: Loss = 3.0685\n",
      "Epoch 2 Batch 470: Loss = 2.7601\n",
      "Epoch 2 Batch 480: Loss = 2.7458\n",
      "Epoch 2 Batch 490: Loss = 2.9813\n",
      "Epoch 2 Batch 500: Loss = 2.7617\n",
      "Epoch 2 Batch 510: Loss = 2.7736\n",
      "Epoch 2 Batch 520: Loss = 2.9223\n",
      "Epoch 2 Batch 530: Loss = 2.7945\n",
      "Epoch 2 Batch 540: Loss = 2.7595\n",
      "Epoch 2 Batch 550: Loss = 3.0589\n",
      "Epoch 2 Batch 560: Loss = 2.8537\n",
      "Epoch 2 Batch 570: Loss = 2.8174\n",
      "Epoch 2 Batch 580: Loss = 2.8696\n",
      "Epoch 2 Batch 590: Loss = 2.7082\n",
      "Epoch 2 Batch 600: Loss = 2.8689\n",
      "Epoch 2 Batch 610: Loss = 2.5862\n",
      "Epoch 2 Batch 620: Loss = 2.8112\n",
      "Epoch 2 Batch 630: Loss = 3.2306\n",
      "Epoch 2 Batch 640: Loss = 3.0138\n",
      "Epoch 2 Batch 650: Loss = 2.6789\n",
      "Epoch 2 Batch 660: Loss = 2.6419\n",
      "Epoch 2 Batch 670: Loss = 2.5326\n",
      "Epoch 2 Batch 680: Loss = 2.8709\n",
      "Epoch 2 Batch 690: Loss = 2.4521\n",
      "Epoch 2 Batch 700: Loss = 2.8044\n",
      "Epoch 2 Batch 710: Loss = 2.5672\n",
      "Epoch 2 Batch 720: Loss = 2.5076\n",
      "Epoch 2 Batch 730: Loss = 2.7952\n",
      "Epoch 2 Batch 740: Loss = 2.6377\n",
      "Epoch 2 Batch 750: Loss = 2.7658\n",
      "Epoch 2 Batch 760: Loss = 2.6976\n",
      "Epoch 2 Batch 770: Loss = 2.7873\n",
      "Epoch 2 Batch 780: Loss = 3.1190\n",
      "Epoch 2 Batch 790: Loss = 2.6056\n",
      "Epoch 2 Batch 800: Loss = 2.7193\n",
      "Epoch 2 Batch 810: Loss = 2.4524\n",
      "Epoch 2 Batch 820: Loss = 2.5309\n",
      "Epoch 2 Batch 830: Loss = 2.6185\n",
      "Epoch 2 Batch 840: Loss = 2.5927\n",
      "Epoch 2 Batch 850: Loss = 2.7328\n",
      "Epoch 2 Batch 860: Loss = 2.5880\n",
      "Epoch 2 Batch 870: Loss = 2.5076\n",
      "Epoch 2 Batch 880: Loss = 2.5981\n",
      "Epoch 2 Batch 890: Loss = 2.6158\n",
      "Epoch 2 Batch 900: Loss = 2.7917\n",
      "Epoch 2 Batch 910: Loss = 2.5125\n",
      "Epoch 2 Batch 920: Loss = 2.5920\n",
      "Epoch 2 Batch 930: Loss = 2.9425\n",
      "Epoch 2 Batch 940: Loss = 2.7025\n",
      "Epoch 2 Batch 950: Loss = 2.5485\n",
      "Epoch 2 Batch 960: Loss = 2.3860\n",
      "Epoch 2 Batch 970: Loss = 2.5268\n",
      "Epoch 2 Batch 980: Loss = 2.2556\n",
      "Epoch 2 Batch 990: Loss = 2.5370\n",
      "Epoch 2 Batch 1000: Loss = 2.8655\n",
      "Epoch 2 Batch 1010: Loss = 2.9644\n",
      "Epoch 2 Batch 1020: Loss = 2.6861\n",
      "Epoch 2 Batch 1030: Loss = 2.4531\n",
      "Epoch 2 Batch 1040: Loss = 2.5904\n",
      "Epoch 2 Batch 1050: Loss = 2.4433\n",
      "Epoch 2 Batch 1060: Loss = 2.5612\n",
      "Epoch 2 Batch 1070: Loss = 2.6334\n",
      "Epoch 2 Batch 1080: Loss = 2.9984\n",
      "Epoch 2 Batch 1090: Loss = 2.4399\n",
      "Epoch 2 Batch 1100: Loss = 2.6637\n",
      "Epoch 2 Batch 1110: Loss = 2.5266\n",
      "Epoch 2 Batch 1120: Loss = 3.0164\n",
      "Epoch 2 Batch 1130: Loss = 2.3834\n",
      "Epoch 2 Batch 1140: Loss = 2.5699\n",
      "Epoch 2 Batch 1150: Loss = 2.7441\n",
      "Epoch 2 Batch 1160: Loss = 2.4344\n",
      "Epoch 2 Batch 1170: Loss = 2.3477\n",
      "Epoch 2 Batch 1180: Loss = 2.4356\n",
      "Epoch 2 Batch 1190: Loss = 2.6107\n",
      "Epoch 2 Batch 1200: Loss = 2.7233\n",
      "Epoch 2 Batch 1210: Loss = 2.9113\n",
      "Epoch 2 Batch 1220: Loss = 2.4912\n",
      "Epoch 2 Batch 1230: Loss = 2.2093\n",
      "Epoch 2 Batch 1240: Loss = 2.4529\n",
      "Epoch 2 Batch 1250: Loss = 2.4624\n",
      "Epoch 2 Batch 1260: Loss = 2.4214\n",
      "Epoch 2 Batch 1270: Loss = 2.7123\n",
      "Epoch 2 Batch 1280: Loss = 2.5977\n",
      "Epoch 2 Batch 1290: Loss = 2.5339\n",
      "Epoch 2 Batch 1300: Loss = 2.3609\n",
      "Epoch 2 Batch 1310: Loss = 2.2717\n",
      "Epoch 2 Batch 1320: Loss = 2.2254\n",
      "Epoch 2 Batch 1330: Loss = 2.1362\n",
      "Epoch 2 Batch 1340: Loss = 2.6143\n",
      "Epoch 2 Batch 1350: Loss = 2.3358\n",
      "Epoch 2 Batch 1360: Loss = 2.3662\n",
      "Epoch 2 Batch 1370: Loss = 2.6452\n",
      "Epoch 2 Batch 1380: Loss = 2.0796\n",
      "Epoch 2 Batch 1390: Loss = 2.3376\n",
      "Epoch 2 Batch 1400: Loss = 2.1250\n",
      "Epoch 2 Batch 1410: Loss = 2.2386\n",
      "Epoch 2 Batch 1420: Loss = 2.6339\n",
      "Epoch 2 Batch 1430: Loss = 2.7270\n",
      "Epoch 2 Batch 1440: Loss = 2.3168\n",
      "Epoch 2 Batch 1450: Loss = 2.4116\n",
      "Epoch 2 Batch 1460: Loss = 2.2846\n",
      "Epoch 2 Batch 1470: Loss = 2.1443\n",
      "Epoch 2 Batch 1480: Loss = 2.4635\n",
      "Epoch 2 Batch 1490: Loss = 2.5563\n",
      "Epoch 2 Batch 1500: Loss = 2.3494\n",
      "Epoch 2 Batch 1510: Loss = 2.4033\n",
      "Epoch 2 Batch 1520: Loss = 2.4112\n",
      "Epoch 2 Batch 1530: Loss = 2.2909\n",
      "Epoch 2 Batch 1540: Loss = 2.5354\n",
      "Epoch 2 Batch 1550: Loss = 2.5818\n",
      "Epoch 2 Batch 1560: Loss = 2.6396\n",
      "Epoch 2 Batch 1570: Loss = 2.6888\n",
      "Epoch 2 Batch 1580: Loss = 1.9899\n",
      "Epoch 2 Batch 1590: Loss = 2.2241\n",
      "Epoch 2 Batch 1600: Loss = 2.4320\n",
      "Epoch 2 Batch 1610: Loss = 2.3997\n",
      "Epoch 2 Batch 1620: Loss = 2.4682\n",
      "Epoch 2 Batch 1630: Loss = 2.3266\n",
      "Epoch 2 Batch 1640: Loss = 2.6043\n",
      "Epoch 2 Batch 1650: Loss = 2.1852\n",
      "Epoch 2 Batch 1660: Loss = 2.3072\n",
      "Epoch 2 Batch 1670: Loss = 2.3760\n",
      "Epoch 2 Batch 1680: Loss = 2.1494\n",
      "Epoch 2 Batch 1690: Loss = 2.1466\n",
      "Epoch 2 Batch 1700: Loss = 2.3789\n",
      "Epoch 2 Batch 1710: Loss = 1.8557\n",
      "Epoch 2 Batch 1720: Loss = 2.3651\n",
      "Epoch 2 Batch 1730: Loss = 2.4560\n",
      "Epoch 2 Batch 1740: Loss = 2.0407\n",
      "Epoch 2 Batch 1750: Loss = 2.1055\n",
      "Epoch 2 Batch 1760: Loss = 2.2619\n",
      "Epoch 2 Batch 1770: Loss = 2.3287\n",
      "Epoch 2 Batch 1780: Loss = 2.3919\n",
      "Epoch 2 Batch 1790: Loss = 2.2085\n",
      "Epoch 2 Batch 1800: Loss = 2.1206\n",
      "Epoch 2 Batch 1810: Loss = 2.1302\n",
      "Epoch 2 Batch 1820: Loss = 2.1919\n",
      "Epoch 2 Batch 1830: Loss = 2.5352\n",
      "Epoch 2 Batch 1840: Loss = 2.1952\n",
      "Epoch 2 Batch 1850: Loss = 1.9813\n",
      "Epoch 2 Batch 1860: Loss = 2.1093\n",
      "Epoch 2 Batch 1870: Loss = 2.4541\n",
      "Epoch 2 Batch 1880: Loss = 2.2863\n",
      "Epoch 2 Batch 1890: Loss = 2.2416\n",
      "Epoch 2 Batch 1900: Loss = 2.1753\n",
      "Epoch 2 Batch 1910: Loss = 2.1261\n",
      "Epoch 2 Batch 1920: Loss = 2.3694\n",
      "Epoch 2 Batch 1930: Loss = 2.3515\n",
      "Epoch 2 Batch 1940: Loss = 2.0353\n",
      "Epoch 2 Batch 1950: Loss = 2.2971\n",
      "Epoch 2 Batch 1960: Loss = 2.3206\n",
      "Epoch 2 Batch 1970: Loss = 2.2037\n",
      "Epoch 2 Batch 1980: Loss = 2.1559\n",
      "Epoch 2 Batch 1990: Loss = 2.1730\n",
      "Epoch 2 Batch 2000: Loss = 2.3323\n",
      "Epoch 2 Average Loss: 2.6019\n",
      "Epoch 3 Batch 0: Loss = 2.1622\n",
      "Epoch 3 Batch 10: Loss = 2.4070\n",
      "Epoch 3 Batch 20: Loss = 2.1838\n",
      "Epoch 3 Batch 30: Loss = 2.0687\n",
      "Epoch 3 Batch 40: Loss = 1.9593\n",
      "Epoch 3 Batch 50: Loss = 1.9835\n",
      "Epoch 3 Batch 60: Loss = 2.1706\n",
      "Epoch 3 Batch 70: Loss = 2.0389\n",
      "Epoch 3 Batch 80: Loss = 2.3871\n",
      "Epoch 3 Batch 90: Loss = 2.0107\n",
      "Epoch 3 Batch 100: Loss = 1.9622\n",
      "Epoch 3 Batch 110: Loss = 1.9180\n",
      "Epoch 3 Batch 120: Loss = 1.9208\n",
      "Epoch 3 Batch 130: Loss = 2.1285\n",
      "Epoch 3 Batch 140: Loss = 2.2364\n",
      "Epoch 3 Batch 150: Loss = 2.2329\n",
      "Epoch 3 Batch 160: Loss = 2.1329\n",
      "Epoch 3 Batch 170: Loss = 2.5454\n",
      "Epoch 3 Batch 180: Loss = 2.0994\n",
      "Epoch 3 Batch 190: Loss = 2.1404\n",
      "Epoch 3 Batch 200: Loss = 2.1160\n",
      "Epoch 3 Batch 210: Loss = 2.0941\n",
      "Epoch 3 Batch 220: Loss = 2.0188\n",
      "Epoch 3 Batch 230: Loss = 1.8732\n",
      "Epoch 3 Batch 240: Loss = 2.1908\n",
      "Epoch 3 Batch 250: Loss = 2.1287\n",
      "Epoch 3 Batch 260: Loss = 1.8822\n",
      "Epoch 3 Batch 270: Loss = 2.1097\n",
      "Epoch 3 Batch 280: Loss = 2.1531\n",
      "Epoch 3 Batch 290: Loss = 2.2119\n",
      "Epoch 3 Batch 300: Loss = 2.3287\n",
      "Epoch 3 Batch 310: Loss = 1.7199\n",
      "Epoch 3 Batch 320: Loss = 2.1588\n",
      "Epoch 3 Batch 330: Loss = 2.1276\n",
      "Epoch 3 Batch 340: Loss = 2.0550\n",
      "Epoch 3 Batch 350: Loss = 2.2479\n",
      "Epoch 3 Batch 360: Loss = 2.0277\n",
      "Epoch 3 Batch 370: Loss = 2.1233\n",
      "Epoch 3 Batch 380: Loss = 2.1505\n",
      "Epoch 3 Batch 390: Loss = 2.0929\n",
      "Epoch 3 Batch 400: Loss = 2.0617\n",
      "Epoch 3 Batch 410: Loss = 2.0445\n",
      "Epoch 3 Batch 420: Loss = 1.9960\n",
      "Epoch 3 Batch 430: Loss = 2.1875\n",
      "Epoch 3 Batch 440: Loss = 2.2019\n",
      "Epoch 3 Batch 450: Loss = 1.8923\n",
      "Epoch 3 Batch 460: Loss = 1.9197\n",
      "Epoch 3 Batch 470: Loss = 2.0264\n",
      "Epoch 3 Batch 480: Loss = 2.0737\n",
      "Epoch 3 Batch 490: Loss = 2.1095\n",
      "Epoch 3 Batch 500: Loss = 1.9126\n",
      "Epoch 3 Batch 510: Loss = 1.6916\n",
      "Epoch 3 Batch 520: Loss = 1.9719\n",
      "Epoch 3 Batch 530: Loss = 1.9194\n",
      "Epoch 3 Batch 540: Loss = 2.0422\n",
      "Epoch 3 Batch 550: Loss = 2.0089\n",
      "Epoch 3 Batch 560: Loss = 1.9117\n",
      "Epoch 3 Batch 570: Loss = 2.1318\n",
      "Epoch 3 Batch 580: Loss = 2.2344\n",
      "Epoch 3 Batch 590: Loss = 1.8961\n",
      "Epoch 3 Batch 600: Loss = 2.4905\n",
      "Epoch 3 Batch 610: Loss = 1.9758\n",
      "Epoch 3 Batch 620: Loss = 1.8355\n",
      "Epoch 3 Batch 630: Loss = 1.9586\n",
      "Epoch 3 Batch 640: Loss = 2.3634\n",
      "Epoch 3 Batch 650: Loss = 2.0809\n",
      "Epoch 3 Batch 660: Loss = 1.8548\n",
      "Epoch 3 Batch 670: Loss = 1.9779\n",
      "Epoch 3 Batch 680: Loss = 1.9470\n",
      "Epoch 3 Batch 690: Loss = 2.2031\n",
      "Epoch 3 Batch 700: Loss = 1.9762\n",
      "Epoch 3 Batch 710: Loss = 2.1369\n",
      "Epoch 3 Batch 720: Loss = 1.9402\n",
      "Epoch 3 Batch 730: Loss = 2.1353\n",
      "Epoch 3 Batch 740: Loss = 2.0790\n",
      "Epoch 3 Batch 750: Loss = 1.6924\n",
      "Epoch 3 Batch 760: Loss = 1.9138\n",
      "Epoch 3 Batch 770: Loss = 1.8510\n",
      "Epoch 3 Batch 780: Loss = 1.8688\n",
      "Epoch 3 Batch 790: Loss = 1.9672\n",
      "Epoch 3 Batch 800: Loss = 2.2297\n",
      "Epoch 3 Batch 810: Loss = 2.0471\n",
      "Epoch 3 Batch 820: Loss = 2.0216\n",
      "Epoch 3 Batch 830: Loss = 1.8536\n",
      "Epoch 3 Batch 840: Loss = 2.2112\n",
      "Epoch 3 Batch 850: Loss = 1.7914\n",
      "Epoch 3 Batch 860: Loss = 2.0092\n",
      "Epoch 3 Batch 870: Loss = 1.9070\n",
      "Epoch 3 Batch 880: Loss = 1.9706\n",
      "Epoch 3 Batch 890: Loss = 1.7444\n",
      "Epoch 3 Batch 900: Loss = 1.9181\n",
      "Epoch 3 Batch 910: Loss = 1.8484\n",
      "Epoch 3 Batch 920: Loss = 2.0104\n",
      "Epoch 3 Batch 930: Loss = 2.0838\n",
      "Epoch 3 Batch 940: Loss = 1.9796\n",
      "Epoch 3 Batch 950: Loss = 1.7555\n",
      "Epoch 3 Batch 960: Loss = 2.0679\n",
      "Epoch 3 Batch 970: Loss = 2.0864\n",
      "Epoch 3 Batch 980: Loss = 1.7727\n",
      "Epoch 3 Batch 990: Loss = 1.9951\n",
      "Epoch 3 Batch 1000: Loss = 1.8722\n",
      "Epoch 3 Batch 1010: Loss = 1.7494\n",
      "Epoch 3 Batch 1020: Loss = 1.7287\n",
      "Epoch 3 Batch 1030: Loss = 1.7826\n",
      "Epoch 3 Batch 1040: Loss = 1.9985\n",
      "Epoch 3 Batch 1050: Loss = 1.9856\n",
      "Epoch 3 Batch 1060: Loss = 1.9054\n",
      "Epoch 3 Batch 1070: Loss = 2.1797\n",
      "Epoch 3 Batch 1080: Loss = 1.8800\n",
      "Epoch 3 Batch 1090: Loss = 1.8114\n",
      "Epoch 3 Batch 1100: Loss = 1.7016\n",
      "Epoch 3 Batch 1110: Loss = 2.2025\n",
      "Epoch 3 Batch 1120: Loss = 2.2589\n",
      "Epoch 3 Batch 1130: Loss = 1.7596\n",
      "Epoch 3 Batch 1140: Loss = 1.9875\n",
      "Epoch 3 Batch 1150: Loss = 1.8013\n",
      "Epoch 3 Batch 1160: Loss = 1.9100\n",
      "Epoch 3 Batch 1170: Loss = 1.7899\n",
      "Epoch 3 Batch 1180: Loss = 1.9062\n",
      "Epoch 3 Batch 1190: Loss = 1.8115\n",
      "Epoch 3 Batch 1200: Loss = 1.8816\n",
      "Epoch 3 Batch 1210: Loss = 1.8868\n",
      "Epoch 3 Batch 1220: Loss = 1.8240\n",
      "Epoch 3 Batch 1230: Loss = 1.9460\n",
      "Epoch 3 Batch 1240: Loss = 2.0486\n",
      "Epoch 3 Batch 1250: Loss = 1.8825\n",
      "Epoch 3 Batch 1260: Loss = 1.8119\n",
      "Epoch 3 Batch 1270: Loss = 1.7150\n",
      "Epoch 3 Batch 1280: Loss = 1.8394\n",
      "Epoch 3 Batch 1290: Loss = 1.6394\n",
      "Epoch 3 Batch 1300: Loss = 1.8315\n",
      "Epoch 3 Batch 1310: Loss = 1.8600\n",
      "Epoch 3 Batch 1320: Loss = 1.9721\n",
      "Epoch 3 Batch 1330: Loss = 1.8355\n",
      "Epoch 3 Batch 1340: Loss = 1.7948\n",
      "Epoch 3 Batch 1350: Loss = 1.8698\n",
      "Epoch 3 Batch 1360: Loss = 2.0437\n",
      "Epoch 3 Batch 1370: Loss = 1.7657\n",
      "Epoch 3 Batch 1380: Loss = 1.8718\n",
      "Epoch 3 Batch 1390: Loss = 1.7849\n",
      "Epoch 3 Batch 1400: Loss = 1.7182\n",
      "Epoch 3 Batch 1410: Loss = 1.9897\n",
      "Epoch 3 Batch 1420: Loss = 1.6475\n",
      "Epoch 3 Batch 1430: Loss = 1.7509\n",
      "Epoch 3 Batch 1440: Loss = 1.9234\n",
      "Epoch 3 Batch 1450: Loss = 1.8291\n",
      "Epoch 3 Batch 1460: Loss = 1.8020\n",
      "Epoch 3 Batch 1470: Loss = 1.5630\n",
      "Epoch 3 Batch 1480: Loss = 1.8251\n",
      "Epoch 3 Batch 1490: Loss = 1.7930\n",
      "Epoch 3 Batch 1500: Loss = 1.6805\n",
      "Epoch 3 Batch 1510: Loss = 1.9983\n",
      "Epoch 3 Batch 1520: Loss = 1.9635\n",
      "Epoch 3 Batch 1530: Loss = 1.8076\n",
      "Epoch 3 Batch 1540: Loss = 1.7521\n",
      "Epoch 3 Batch 1550: Loss = 1.9847\n",
      "Epoch 3 Batch 1560: Loss = 1.7475\n",
      "Epoch 3 Batch 1570: Loss = 1.8552\n",
      "Epoch 3 Batch 1580: Loss = 1.5584\n",
      "Epoch 3 Batch 1590: Loss = 1.7389\n",
      "Epoch 3 Batch 1600: Loss = 1.5692\n",
      "Epoch 3 Batch 1610: Loss = 1.7829\n",
      "Epoch 3 Batch 1620: Loss = 1.8131\n",
      "Epoch 3 Batch 1630: Loss = 1.7895\n",
      "Epoch 3 Batch 1640: Loss = 1.8857\n",
      "Epoch 3 Batch 1650: Loss = 1.6893\n",
      "Epoch 3 Batch 1660: Loss = 1.7720\n",
      "Epoch 3 Batch 1670: Loss = 1.7680\n",
      "Epoch 3 Batch 1680: Loss = 1.7936\n",
      "Epoch 3 Batch 1690: Loss = 1.6511\n",
      "Epoch 3 Batch 1700: Loss = 1.8099\n",
      "Epoch 3 Batch 1710: Loss = 1.7118\n",
      "Epoch 3 Batch 1720: Loss = 1.5459\n",
      "Epoch 3 Batch 1730: Loss = 1.9574\n",
      "Epoch 3 Batch 1740: Loss = 1.6052\n",
      "Epoch 3 Batch 1750: Loss = 1.8099\n",
      "Epoch 3 Batch 1760: Loss = 1.7468\n",
      "Epoch 3 Batch 1770: Loss = 1.7536\n",
      "Epoch 3 Batch 1780: Loss = 1.8691\n",
      "Epoch 3 Batch 1790: Loss = 1.5513\n",
      "Epoch 3 Batch 1800: Loss = 1.8675\n",
      "Epoch 3 Batch 1810: Loss = 2.2032\n",
      "Epoch 3 Batch 1820: Loss = 1.6602\n",
      "Epoch 3 Batch 1830: Loss = 1.7093\n",
      "Epoch 3 Batch 1840: Loss = 1.7905\n",
      "Epoch 3 Batch 1850: Loss = 1.8916\n",
      "Epoch 3 Batch 1860: Loss = 1.8920\n",
      "Epoch 3 Batch 1870: Loss = 2.2347\n",
      "Epoch 3 Batch 1880: Loss = 1.5809\n",
      "Epoch 3 Batch 1890: Loss = 1.7176\n",
      "Epoch 3 Batch 1900: Loss = 1.7233\n",
      "Epoch 3 Batch 1910: Loss = 1.5384\n",
      "Epoch 3 Batch 1920: Loss = 1.7442\n",
      "Epoch 3 Batch 1930: Loss = 1.7437\n",
      "Epoch 3 Batch 1940: Loss = 2.0193\n",
      "Epoch 3 Batch 1950: Loss = 1.7077\n",
      "Epoch 3 Batch 1960: Loss = 1.8640\n",
      "Epoch 3 Batch 1970: Loss = 1.8744\n",
      "Epoch 3 Batch 1980: Loss = 1.7850\n",
      "Epoch 3 Batch 1990: Loss = 1.5518\n",
      "Epoch 3 Batch 2000: Loss = 1.7906\n",
      "Epoch 3 Average Loss: 1.9138\n",
      "\n",
      "Generated Text:\n",
      "In a village <EOS> challenge challenge challenge Hitler tails reviews reviews Michał Are Are Are Dongshui popcorn Goffman Goffman exclusive Mechanisation migration protest declining neutrino magnitude Ribfest Peppers Retaliation Bryant Seleucid reformed Learned journalist Roads boiled totalitarian Imogen Derfflinger fall incumbent incumbent incumbent incumbent Webb Corben pp. styles styles timed generally generally generally generally\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "# ----------------------------\n",
    "# Simple Tokenizer Definition\n",
    "# ----------------------------\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, corpus):\n",
    "        # Split corpus by whitespace and build vocabulary.\n",
    "        tokens = corpus.split()\n",
    "        unique_tokens = set(tokens)\n",
    "        # Reserve indices 0-3 for special tokens.\n",
    "        self.vocab = {\"<PAD>\": 0, \"<UNK>\": 1, \"<MASK>\": 2, \"<EOS>\": 3}\n",
    "        for i, token in enumerate(unique_tokens):\n",
    "            self.vocab[token] = i + 4\n",
    "        self.inv_vocab = {i: token for token, i in self.vocab.items()}\n",
    "        self.mask_token_id = self.vocab[\"<MASK>\"]\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = text.split()\n",
    "        # Append EOS token at the end.\n",
    "        token_ids = [self.vocab.get(token, self.vocab[\"<UNK>\"]) for token in tokens] + [self.vocab[\"<EOS>\"]]\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        tokens = [self.inv_vocab.get(i, \"<UNK>\") for i in token_ids]\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "# ----------------------------\n",
    "# Transformer Components\n",
    "# ----------------------------\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=n_heads, dropout=dropout)\n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Feedforward network that is 10 layers deep.\n",
    "        ff_layers = []\n",
    "        for _ in range(10):\n",
    "            ff_layers.append(nn.Linear(d_model, d_model))\n",
    "            ff_layers.append(nn.ReLU())\n",
    "        self.feedforward = nn.Sequential(*ff_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_length, batch_size, d_model)\n",
    "        attn_output, _ = self.attn(x, x, x)\n",
    "        x = self.layernorm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feedforward(x)\n",
    "        x = self.layernorm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "# ----------------------------\n",
    "# GPT-2–like Model Definition\n",
    "# ----------------------------\n",
    "class GPT2Model(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=255, n_heads=5, n_layers=4, max_seq_length=128, dropout=0.1):\n",
    "        super(GPT2Model, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_embedding = nn.Embedding(max_seq_length, d_model)\n",
    "        \n",
    "        # Stack transformer blocks.\n",
    "        self.layers = nn.ModuleList([TransformerBlock(d_model, n_heads, dropout) for _ in range(n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        # Final projection to vocabulary.\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # input_ids shape: (batch_size, seq_length)\n",
    "        batch_size, seq_length = input_ids.size()\n",
    "        positions = torch.arange(0, seq_length, device=input_ids.device).unsqueeze(0).expand(batch_size, seq_length)\n",
    "        x = self.token_embedding(input_ids) + self.positional_embedding(positions)\n",
    "        x = x.transpose(0, 1)  # (seq_length, batch_size, d_model)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.ln_f(x)\n",
    "        x = x.transpose(0, 1)  # back to (batch_size, seq_length, d_model)\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "# ----------------------------\n",
    "# Dataset Definition\n",
    "# ----------------------------\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, seq_length=128, mask_prob=0.15):\n",
    "        self.tokenizer = tokenizer\n",
    "        tokens = tokenizer.encode(text)\n",
    "        # Split tokens into sequences of fixed length.\n",
    "        self.sequences = []\n",
    "        for i in range(0, len(tokens) - seq_length, seq_length):\n",
    "            self.sequences.append(tokens[i:i+seq_length])\n",
    "        self.seq_length = seq_length\n",
    "        self.mask_prob = mask_prob\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        input_seq = []\n",
    "        target_seq = []\n",
    "        # Randomly mask tokens with mask_prob.\n",
    "        for token in seq:\n",
    "            if random.random() < self.mask_prob:\n",
    "                input_seq.append(self.tokenizer.mask_token_id)\n",
    "            else:\n",
    "                input_seq.append(token)\n",
    "            target_seq.append(token)\n",
    "        input_seq = torch.tensor(input_seq, dtype=torch.long)\n",
    "        target_seq = torch.tensor(target_seq, dtype=torch.long)\n",
    "        return input_seq, target_seq\n",
    "\n",
    "# ----------------------------\n",
    "# Training Setup\n",
    "# ----------------------------\n",
    "def train(model, dataloader, optimizer, device, epochs=3):\n",
    "    model.train()\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=0)  # ignore PAD token\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch_idx, (input_seq, target_seq) in enumerate(dataloader):\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_seq)  # (batch_size, seq_length, vocab_size)\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), target_seq.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1} Batch {batch_idx}: Loss = {loss.item():.4f}\")\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1} Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Text Generation Function\n",
    "# ----------------------------\n",
    "def generate_text(model, tokenizer, prompt, max_length=50, temperature=1.0, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    # Encode the prompt.\n",
    "    input_ids = torch.tensor(tokenizer.encode(prompt), dtype=torch.long).unsqueeze(0).to(device)\n",
    "    generated = input_ids.tolist()[0]\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Use only the last max_seq_length tokens.\n",
    "            input_seq = input_ids[:, -model.max_seq_length:]\n",
    "            logits = model(input_seq)\n",
    "            # Focus on the last token's logits.\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probabilities = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "            generated.append(next_token.item())\n",
    "            if next_token.item() == tokenizer.vocab[\"<EOS>\"]:\n",
    "                break\n",
    "    return tokenizer.decode(generated)\n",
    "\n",
    "# ----------------------------\n",
    "# Main Execution\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Load a generic text corpus from Hugging Face.\n",
    "    # Here we use the WikiText-2 raw dataset.\n",
    "    dataset_hf = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "    # Concatenate all text entries into one large corpus.\n",
    "    corpus = \"\\n\".join(dataset_hf[\"text\"])\n",
    "    print(\"Corpus loaded. Corpus length:\", len(corpus))\n",
    "\n",
    "    # Initialize the tokenizer and dataset.\n",
    "    tokenizer = SimpleTokenizer(corpus)\n",
    "    # You might choose a smaller sequence length for testing; adjust as needed.\n",
    "    dataset = TextDataset(corpus, tokenizer, seq_length=128, mask_prob=0.15)\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    # Define device.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Instantiate model.\n",
    "    vocab_size = len(tokenizer.vocab)\n",
    "    model = GPT2Model(vocab_size, d_model=255, n_heads=5, n_layers=4, max_seq_length=128, dropout=0.1)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Define optimizer.\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    # Train the model.\n",
    "    train(model, dataloader, optimizer, device, epochs=3)\n",
    "    \n",
    "    # Generate sample text.\n",
    "    prompt = \"In a village\"\n",
    "    sample = generate_text(model, tokenizer, prompt, max_length=50, temperature=1.0, device=device)\n",
    "    print(\"\\nGenerated Text:\")\n",
    "    print(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus loaded. Corpus length: 10929707\n",
      "Epoch 1 Batch 0: Loss = 11.3968\n",
      "Epoch 1 Batch 10: Loss = 11.1051\n",
      "Epoch 1 Batch 20: Loss = 10.6287\n",
      "Epoch 1 Batch 30: Loss = 10.2193\n",
      "Epoch 1 Batch 40: Loss = 9.8461\n",
      "Epoch 1 Batch 50: Loss = 9.4496\n",
      "Epoch 1 Batch 60: Loss = 9.0708\n",
      "Epoch 1 Batch 70: Loss = 8.7852\n",
      "Epoch 1 Batch 80: Loss = 8.6846\n",
      "Epoch 1 Batch 90: Loss = 8.3715\n",
      "Epoch 1 Batch 100: Loss = 8.1207\n",
      "Epoch 1 Batch 110: Loss = 7.7645\n",
      "Epoch 1 Batch 120: Loss = 7.8222\n",
      "Epoch 1 Batch 130: Loss = 7.5582\n",
      "Epoch 1 Batch 140: Loss = 7.2951\n",
      "Epoch 1 Batch 150: Loss = 7.1902\n",
      "Epoch 1 Batch 160: Loss = 6.8851\n",
      "Epoch 1 Batch 170: Loss = 6.8737\n",
      "Epoch 1 Batch 180: Loss = 6.7968\n",
      "Epoch 1 Batch 190: Loss = 6.9937\n",
      "Epoch 1 Batch 200: Loss = 6.8764\n",
      "Epoch 1 Batch 210: Loss = 6.5269\n",
      "Epoch 1 Batch 220: Loss = 6.6886\n",
      "Epoch 1 Batch 230: Loss = 6.5595\n",
      "Epoch 1 Batch 240: Loss = 6.2645\n",
      "Epoch 1 Batch 250: Loss = 6.3690\n",
      "Epoch 1 Batch 260: Loss = 6.1817\n",
      "Epoch 1 Batch 270: Loss = 6.1428\n",
      "Epoch 1 Batch 280: Loss = 6.2256\n",
      "Epoch 1 Batch 290: Loss = 6.2673\n",
      "Epoch 1 Batch 300: Loss = 5.8855\n",
      "Epoch 1 Batch 310: Loss = 6.0130\n",
      "Epoch 1 Batch 320: Loss = 6.1998\n",
      "Epoch 1 Batch 330: Loss = 6.1557\n",
      "Epoch 1 Batch 340: Loss = 5.8455\n",
      "Epoch 1 Batch 350: Loss = 5.9758\n",
      "Epoch 1 Batch 360: Loss = 6.1151\n",
      "Epoch 1 Batch 370: Loss = 5.7487\n",
      "Epoch 1 Batch 380: Loss = 5.6981\n",
      "Epoch 1 Batch 390: Loss = 5.7664\n",
      "Epoch 1 Batch 400: Loss = 5.5144\n",
      "Epoch 1 Batch 410: Loss = 5.6667\n",
      "Epoch 1 Batch 420: Loss = 5.9189\n",
      "Epoch 1 Batch 430: Loss = 5.8525\n",
      "Epoch 1 Batch 440: Loss = 5.5260\n",
      "Epoch 1 Batch 450: Loss = 5.5415\n",
      "Epoch 1 Batch 460: Loss = 5.6397\n",
      "Epoch 1 Batch 470: Loss = 5.4145\n",
      "Epoch 1 Batch 480: Loss = 5.2824\n",
      "Epoch 1 Batch 490: Loss = 5.5006\n",
      "Epoch 1 Batch 500: Loss = 5.2936\n",
      "Epoch 1 Batch 510: Loss = 5.4081\n",
      "Epoch 1 Batch 520: Loss = 5.2582\n",
      "Epoch 1 Batch 530: Loss = 5.2169\n",
      "Epoch 1 Batch 540: Loss = 5.3610\n",
      "Epoch 1 Batch 550: Loss = 5.1746\n",
      "Epoch 1 Batch 560: Loss = 5.4255\n",
      "Epoch 1 Batch 570: Loss = 5.3413\n",
      "Epoch 1 Batch 580: Loss = 5.3958\n",
      "Epoch 1 Batch 590: Loss = 5.1173\n",
      "Epoch 1 Batch 600: Loss = 4.9405\n",
      "Epoch 1 Batch 610: Loss = 5.3354\n",
      "Epoch 1 Batch 620: Loss = 5.3100\n",
      "Epoch 1 Batch 630: Loss = 5.3035\n",
      "Epoch 1 Batch 640: Loss = 4.9895\n",
      "Epoch 1 Batch 650: Loss = 4.9314\n",
      "Epoch 1 Batch 660: Loss = 5.3989\n",
      "Epoch 1 Batch 670: Loss = 5.1295\n",
      "Epoch 1 Batch 680: Loss = 4.9877\n",
      "Epoch 1 Batch 690: Loss = 5.0863\n",
      "Epoch 1 Batch 700: Loss = 5.1589\n",
      "Epoch 1 Batch 710: Loss = 4.7869\n",
      "Epoch 1 Batch 720: Loss = 4.9187\n",
      "Epoch 1 Batch 730: Loss = 4.7833\n",
      "Epoch 1 Batch 740: Loss = 4.9765\n",
      "Epoch 1 Batch 750: Loss = 4.8522\n",
      "Epoch 1 Batch 760: Loss = 4.9577\n",
      "Epoch 1 Batch 770: Loss = 4.9550\n",
      "Epoch 1 Batch 780: Loss = 4.9454\n",
      "Epoch 1 Batch 790: Loss = 5.1794\n",
      "Epoch 1 Batch 800: Loss = 4.8562\n",
      "Epoch 1 Batch 810: Loss = 4.7980\n",
      "Epoch 1 Batch 820: Loss = 4.6271\n",
      "Epoch 1 Batch 830: Loss = 5.0154\n",
      "Epoch 1 Batch 840: Loss = 4.7603\n",
      "Epoch 1 Batch 850: Loss = 4.3726\n",
      "Epoch 1 Batch 860: Loss = 4.6924\n",
      "Epoch 1 Batch 870: Loss = 5.0330\n",
      "Epoch 1 Batch 880: Loss = 4.6455\n",
      "Epoch 1 Batch 890: Loss = 4.4110\n",
      "Epoch 1 Batch 900: Loss = 4.6176\n",
      "Epoch 1 Batch 910: Loss = 4.5219\n",
      "Epoch 1 Batch 920: Loss = 4.6408\n",
      "Epoch 1 Batch 930: Loss = 4.7139\n",
      "Epoch 1 Batch 940: Loss = 4.1961\n",
      "Epoch 1 Batch 950: Loss = 4.2798\n",
      "Epoch 1 Batch 960: Loss = 4.5297\n",
      "Epoch 1 Batch 970: Loss = 4.6771\n",
      "Epoch 1 Batch 980: Loss = 4.4314\n",
      "Epoch 1 Batch 990: Loss = 4.4945\n",
      "Epoch 1 Batch 1000: Loss = 4.4117\n",
      "Epoch 1 Batch 1010: Loss = 4.1093\n",
      "Epoch 1 Batch 1020: Loss = 4.6128\n",
      "Epoch 1 Batch 1030: Loss = 4.3525\n",
      "Epoch 1 Batch 1040: Loss = 4.3828\n",
      "Epoch 1 Batch 1050: Loss = 4.3827\n",
      "Epoch 1 Batch 1060: Loss = 4.3390\n",
      "Epoch 1 Batch 1070: Loss = 4.2365\n",
      "Epoch 1 Batch 1080: Loss = 4.3893\n",
      "Epoch 1 Batch 1090: Loss = 4.1075\n",
      "Epoch 1 Batch 1100: Loss = 4.4096\n",
      "Epoch 1 Batch 1110: Loss = 4.4718\n",
      "Epoch 1 Batch 1120: Loss = 4.2817\n",
      "Epoch 1 Batch 1130: Loss = 3.8822\n",
      "Epoch 1 Batch 1140: Loss = 4.3054\n",
      "Epoch 1 Batch 1150: Loss = 4.3994\n",
      "Epoch 1 Batch 1160: Loss = 4.3140\n",
      "Epoch 1 Batch 1170: Loss = 4.1889\n",
      "Epoch 1 Batch 1180: Loss = 4.0501\n",
      "Epoch 1 Batch 1190: Loss = 3.9738\n",
      "Epoch 1 Batch 1200: Loss = 4.2275\n",
      "Epoch 1 Batch 1210: Loss = 4.2330\n",
      "Epoch 1 Batch 1220: Loss = 4.1545\n",
      "Epoch 1 Batch 1230: Loss = 4.0669\n",
      "Epoch 1 Batch 1240: Loss = 3.8031\n",
      "Epoch 1 Batch 1250: Loss = 3.6962\n",
      "Epoch 1 Batch 1260: Loss = 3.8988\n",
      "Epoch 1 Batch 1270: Loss = 4.0193\n",
      "Epoch 1 Batch 1280: Loss = 3.7532\n",
      "Epoch 1 Batch 1290: Loss = 3.9722\n",
      "Epoch 1 Batch 1300: Loss = 3.9957\n",
      "Epoch 1 Batch 1310: Loss = 4.3772\n",
      "Epoch 1 Batch 1320: Loss = 4.4290\n",
      "Epoch 1 Batch 1330: Loss = 3.7635\n",
      "Epoch 1 Batch 1340: Loss = 4.1961\n",
      "Epoch 1 Batch 1350: Loss = 3.9502\n",
      "Epoch 1 Batch 1360: Loss = 4.0315\n",
      "Epoch 1 Batch 1370: Loss = 3.9043\n",
      "Epoch 1 Batch 1380: Loss = 3.7102\n",
      "Epoch 1 Batch 1390: Loss = 4.1247\n",
      "Epoch 1 Batch 1400: Loss = 3.9237\n",
      "Epoch 1 Batch 1410: Loss = 3.8426\n",
      "Epoch 1 Batch 1420: Loss = 3.8792\n",
      "Epoch 1 Batch 1430: Loss = 3.5383\n",
      "Epoch 1 Batch 1440: Loss = 3.8418\n",
      "Epoch 1 Batch 1450: Loss = 3.4414\n",
      "Epoch 1 Batch 1460: Loss = 3.9175\n",
      "Epoch 1 Batch 1470: Loss = 3.5536\n",
      "Epoch 1 Batch 1480: Loss = 3.7417\n",
      "Epoch 1 Batch 1490: Loss = 3.5718\n",
      "Epoch 1 Batch 1500: Loss = 3.7172\n",
      "Epoch 1 Batch 1510: Loss = 3.7739\n",
      "Epoch 1 Batch 1520: Loss = 3.8117\n",
      "Epoch 1 Batch 1530: Loss = 3.8813\n",
      "Epoch 1 Batch 1540: Loss = 3.6754\n",
      "Epoch 1 Batch 1550: Loss = 3.6740\n",
      "Epoch 1 Batch 1560: Loss = 3.6849\n",
      "Epoch 1 Batch 1570: Loss = 3.5832\n",
      "Epoch 1 Batch 1580: Loss = 3.3867\n",
      "Epoch 1 Batch 1590: Loss = 3.4485\n",
      "Epoch 1 Batch 1600: Loss = 3.4191\n",
      "Epoch 1 Batch 1610: Loss = 3.7115\n",
      "Epoch 1 Batch 1620: Loss = 3.4692\n",
      "Epoch 1 Batch 1630: Loss = 3.7029\n",
      "Epoch 1 Batch 1640: Loss = 3.6374\n",
      "Epoch 1 Batch 1650: Loss = 3.4917\n",
      "Epoch 1 Batch 1660: Loss = 3.6063\n",
      "Epoch 1 Batch 1670: Loss = 3.6065\n",
      "Epoch 1 Batch 1680: Loss = 4.2914\n",
      "Epoch 1 Batch 1690: Loss = 3.4688\n",
      "Epoch 1 Batch 1700: Loss = 3.2933\n",
      "Epoch 1 Batch 1710: Loss = 3.4161\n",
      "Epoch 1 Batch 1720: Loss = 3.4769\n",
      "Epoch 1 Batch 1730: Loss = 3.4249\n",
      "Epoch 1 Batch 1740: Loss = 3.5241\n",
      "Epoch 1 Batch 1750: Loss = 3.5022\n",
      "Epoch 1 Batch 1760: Loss = 3.2132\n",
      "Epoch 1 Batch 1770: Loss = 3.5240\n",
      "Epoch 1 Batch 1780: Loss = 3.7277\n",
      "Epoch 1 Batch 1790: Loss = 3.2661\n",
      "Epoch 1 Batch 1800: Loss = 3.1716\n",
      "Epoch 1 Batch 1810: Loss = 3.5895\n",
      "Epoch 1 Batch 1820: Loss = 3.5666\n",
      "Epoch 1 Batch 1830: Loss = 3.3767\n",
      "Epoch 1 Batch 1840: Loss = 3.0742\n",
      "Epoch 1 Batch 1850: Loss = 3.0690\n",
      "Epoch 1 Batch 1860: Loss = 3.6609\n",
      "Epoch 1 Batch 1870: Loss = 3.4858\n",
      "Epoch 1 Batch 1880: Loss = 3.4466\n",
      "Epoch 1 Batch 1890: Loss = 3.3295\n",
      "Epoch 1 Batch 1900: Loss = 3.6998\n",
      "Epoch 1 Batch 1910: Loss = 3.3884\n",
      "Epoch 1 Batch 1920: Loss = 3.4601\n",
      "Epoch 1 Batch 1930: Loss = 3.2960\n",
      "Epoch 1 Batch 1940: Loss = 2.6902\n",
      "Epoch 1 Batch 1950: Loss = 3.1054\n",
      "Epoch 1 Batch 1960: Loss = 2.9857\n",
      "Epoch 1 Batch 1970: Loss = 3.0583\n",
      "Epoch 1 Batch 1980: Loss = 3.3380\n",
      "Epoch 1 Batch 1990: Loss = 3.3030\n",
      "Epoch 1 Batch 2000: Loss = 2.8108\n",
      "Epoch 1 Average Loss: 4.8370\n",
      "Epoch 2 Batch 0: Loss = 3.2284\n",
      "Epoch 2 Batch 10: Loss = 3.1374\n",
      "Epoch 2 Batch 20: Loss = 3.5713\n",
      "Epoch 2 Batch 30: Loss = 2.9931\n",
      "Epoch 2 Batch 40: Loss = 3.1466\n",
      "Epoch 2 Batch 50: Loss = 2.8487\n",
      "Epoch 2 Batch 60: Loss = 3.2972\n",
      "Epoch 2 Batch 70: Loss = 3.3358\n",
      "Epoch 2 Batch 80: Loss = 3.1198\n",
      "Epoch 2 Batch 90: Loss = 3.1118\n",
      "Epoch 2 Batch 100: Loss = 3.2445\n",
      "Epoch 2 Batch 110: Loss = 3.4143\n",
      "Epoch 2 Batch 120: Loss = 2.9599\n",
      "Epoch 2 Batch 130: Loss = 2.8373\n",
      "Epoch 2 Batch 140: Loss = 3.1109\n",
      "Epoch 2 Batch 150: Loss = 2.8243\n",
      "Epoch 2 Batch 160: Loss = 3.3583\n",
      "Epoch 2 Batch 170: Loss = 3.0697\n",
      "Epoch 2 Batch 180: Loss = 2.8965\n",
      "Epoch 2 Batch 190: Loss = 2.7860\n",
      "Epoch 2 Batch 200: Loss = 3.0737\n",
      "Epoch 2 Batch 210: Loss = 3.1058\n",
      "Epoch 2 Batch 220: Loss = 3.2144\n",
      "Epoch 2 Batch 230: Loss = 2.5664\n",
      "Epoch 2 Batch 240: Loss = 3.2722\n",
      "Epoch 2 Batch 250: Loss = 2.8434\n",
      "Epoch 2 Batch 260: Loss = 2.8757\n",
      "Epoch 2 Batch 270: Loss = 2.6976\n",
      "Epoch 2 Batch 280: Loss = 3.0937\n",
      "Epoch 2 Batch 290: Loss = 3.0172\n",
      "Epoch 2 Batch 300: Loss = 2.8408\n",
      "Epoch 2 Batch 310: Loss = 2.7370\n",
      "Epoch 2 Batch 320: Loss = 2.7093\n",
      "Epoch 2 Batch 330: Loss = 2.9285\n",
      "Epoch 2 Batch 340: Loss = 2.8323\n",
      "Epoch 2 Batch 350: Loss = 3.0788\n",
      "Epoch 2 Batch 360: Loss = 3.0951\n",
      "Epoch 2 Batch 370: Loss = 2.5985\n",
      "Epoch 2 Batch 380: Loss = 2.5372\n",
      "Epoch 2 Batch 390: Loss = 2.8384\n",
      "Epoch 2 Batch 400: Loss = 2.7934\n",
      "Epoch 2 Batch 410: Loss = 3.0341\n",
      "Epoch 2 Batch 420: Loss = 2.6869\n",
      "Epoch 2 Batch 430: Loss = 2.6262\n",
      "Epoch 2 Batch 440: Loss = 2.7707\n",
      "Epoch 2 Batch 450: Loss = 2.7398\n",
      "Epoch 2 Batch 460: Loss = 3.0249\n",
      "Epoch 2 Batch 470: Loss = 2.9214\n",
      "Epoch 2 Batch 480: Loss = 2.7019\n",
      "Epoch 2 Batch 490: Loss = 3.0637\n",
      "Epoch 2 Batch 500: Loss = 2.5954\n",
      "Epoch 2 Batch 510: Loss = 2.8391\n",
      "Epoch 2 Batch 520: Loss = 2.8312\n",
      "Epoch 2 Batch 530: Loss = 3.1528\n",
      "Epoch 2 Batch 540: Loss = 2.9346\n",
      "Epoch 2 Batch 550: Loss = 2.6488\n",
      "Epoch 2 Batch 560: Loss = 2.7743\n",
      "Epoch 2 Batch 570: Loss = 2.8900\n",
      "Epoch 2 Batch 580: Loss = 3.0814\n",
      "Epoch 2 Batch 590: Loss = 2.5201\n",
      "Epoch 2 Batch 600: Loss = 2.7656\n",
      "Epoch 2 Batch 610: Loss = 2.5833\n",
      "Epoch 2 Batch 620: Loss = 2.7676\n",
      "Epoch 2 Batch 630: Loss = 2.7665\n",
      "Epoch 2 Batch 640: Loss = 2.4910\n",
      "Epoch 2 Batch 650: Loss = 3.1455\n",
      "Epoch 2 Batch 660: Loss = 2.5896\n",
      "Epoch 2 Batch 670: Loss = 2.7450\n",
      "Epoch 2 Batch 680: Loss = 2.4436\n",
      "Epoch 2 Batch 690: Loss = 2.6964\n",
      "Epoch 2 Batch 700: Loss = 2.6406\n",
      "Epoch 2 Batch 710: Loss = 2.7940\n",
      "Epoch 2 Batch 720: Loss = 2.8866\n",
      "Epoch 2 Batch 730: Loss = 3.0719\n",
      "Epoch 2 Batch 740: Loss = 2.9476\n",
      "Epoch 2 Batch 750: Loss = 2.7529\n",
      "Epoch 2 Batch 760: Loss = 2.4652\n",
      "Epoch 2 Batch 770: Loss = 2.8291\n",
      "Epoch 2 Batch 780: Loss = 2.7471\n",
      "Epoch 2 Batch 790: Loss = 2.5872\n",
      "Epoch 2 Batch 800: Loss = 2.4883\n",
      "Epoch 2 Batch 810: Loss = 2.5570\n",
      "Epoch 2 Batch 820: Loss = 2.5394\n",
      "Epoch 2 Batch 830: Loss = 2.8227\n",
      "Epoch 2 Batch 840: Loss = 2.3609\n",
      "Epoch 2 Batch 850: Loss = 2.9635\n",
      "Epoch 2 Batch 860: Loss = 2.6230\n",
      "Epoch 2 Batch 870: Loss = 2.5605\n",
      "Epoch 2 Batch 880: Loss = 2.4776\n",
      "Epoch 2 Batch 890: Loss = 2.7475\n",
      "Epoch 2 Batch 900: Loss = 2.6123\n",
      "Epoch 2 Batch 910: Loss = 2.4377\n",
      "Epoch 2 Batch 920: Loss = 2.5736\n",
      "Epoch 2 Batch 930: Loss = 2.8254\n",
      "Epoch 2 Batch 940: Loss = 2.4817\n",
      "Epoch 2 Batch 950: Loss = 2.8132\n",
      "Epoch 2 Batch 960: Loss = 2.3278\n",
      "Epoch 2 Batch 970: Loss = 2.4228\n",
      "Epoch 2 Batch 980: Loss = 2.6182\n",
      "Epoch 2 Batch 990: Loss = 2.5877\n",
      "Epoch 2 Batch 1000: Loss = 2.5884\n",
      "Epoch 2 Batch 1010: Loss = 2.7972\n",
      "Epoch 2 Batch 1020: Loss = 2.3814\n",
      "Epoch 2 Batch 1030: Loss = 2.2670\n",
      "Epoch 2 Batch 1040: Loss = 2.5850\n",
      "Epoch 2 Batch 1050: Loss = 2.2755\n",
      "Epoch 2 Batch 1060: Loss = 2.5650\n",
      "Epoch 2 Batch 1070: Loss = 2.7948\n",
      "Epoch 2 Batch 1080: Loss = 2.7949\n",
      "Epoch 2 Batch 1090: Loss = 2.3227\n",
      "Epoch 2 Batch 1100: Loss = 2.5144\n",
      "Epoch 2 Batch 1110: Loss = 2.3962\n",
      "Epoch 2 Batch 1120: Loss = 2.4391\n",
      "Epoch 2 Batch 1130: Loss = 2.5622\n",
      "Epoch 2 Batch 1140: Loss = 2.2189\n",
      "Epoch 2 Batch 1150: Loss = 2.5561\n",
      "Epoch 2 Batch 1160: Loss = 2.4827\n",
      "Epoch 2 Batch 1170: Loss = 2.5781\n",
      "Epoch 2 Batch 1180: Loss = 2.5654\n",
      "Epoch 2 Batch 1190: Loss = 2.6685\n",
      "Epoch 2 Batch 1200: Loss = 2.3144\n",
      "Epoch 2 Batch 1210: Loss = 2.7181\n",
      "Epoch 2 Batch 1220: Loss = 2.6184\n",
      "Epoch 2 Batch 1230: Loss = 2.3866\n",
      "Epoch 2 Batch 1240: Loss = 2.4729\n",
      "Epoch 2 Batch 1250: Loss = 2.6751\n",
      "Epoch 2 Batch 1260: Loss = 2.5234\n",
      "Epoch 2 Batch 1270: Loss = 2.4782\n",
      "Epoch 2 Batch 1280: Loss = 2.5447\n",
      "Epoch 2 Batch 1290: Loss = 2.6393\n",
      "Epoch 2 Batch 1300: Loss = 2.6066\n",
      "Epoch 2 Batch 1310: Loss = 2.5648\n",
      "Epoch 2 Batch 1320: Loss = 2.4196\n",
      "Epoch 2 Batch 1330: Loss = 2.3378\n",
      "Epoch 2 Batch 1340: Loss = 2.7110\n",
      "Epoch 2 Batch 1350: Loss = 2.2724\n",
      "Epoch 2 Batch 1360: Loss = 2.6807\n",
      "Epoch 2 Batch 1370: Loss = 2.3033\n",
      "Epoch 2 Batch 1380: Loss = 2.1054\n",
      "Epoch 2 Batch 1390: Loss = 2.0591\n",
      "Epoch 2 Batch 1400: Loss = 2.3645\n",
      "Epoch 2 Batch 1410: Loss = 2.2835\n",
      "Epoch 2 Batch 1420: Loss = 2.3539\n",
      "Epoch 2 Batch 1430: Loss = 2.3288\n",
      "Epoch 2 Batch 1440: Loss = 2.4273\n",
      "Epoch 2 Batch 1450: Loss = 2.4919\n",
      "Epoch 2 Batch 1460: Loss = 2.4575\n",
      "Epoch 2 Batch 1470: Loss = 2.3613\n",
      "Epoch 2 Batch 1480: Loss = 2.2719\n",
      "Epoch 2 Batch 1490: Loss = 2.3131\n",
      "Epoch 2 Batch 1500: Loss = 2.1666\n",
      "Epoch 2 Batch 1510: Loss = 2.4433\n",
      "Epoch 2 Batch 1520: Loss = 2.5186\n",
      "Epoch 2 Batch 1530: Loss = 2.5258\n",
      "Epoch 2 Batch 1540: Loss = 2.4576\n",
      "Epoch 2 Batch 1550: Loss = 2.3528\n",
      "Epoch 2 Batch 1560: Loss = 2.2889\n",
      "Epoch 2 Batch 1570: Loss = 2.2963\n",
      "Epoch 2 Batch 1580: Loss = 2.1737\n",
      "Epoch 2 Batch 1590: Loss = 2.0616\n",
      "Epoch 2 Batch 1600: Loss = 2.0371\n",
      "Epoch 2 Batch 1610: Loss = 2.5505\n",
      "Epoch 2 Batch 1620: Loss = 2.3928\n",
      "Epoch 2 Batch 1630: Loss = 2.5557\n",
      "Epoch 2 Batch 1640: Loss = 2.3443\n",
      "Epoch 2 Batch 1650: Loss = 2.2759\n",
      "Epoch 2 Batch 1660: Loss = 2.2247\n",
      "Epoch 2 Batch 1670: Loss = 2.3789\n",
      "Epoch 2 Batch 1680: Loss = 2.1754\n",
      "Epoch 2 Batch 1690: Loss = 2.3645\n",
      "Epoch 2 Batch 1700: Loss = 2.3725\n",
      "Epoch 2 Batch 1710: Loss = 2.3752\n",
      "Epoch 2 Batch 1720: Loss = 2.1874\n",
      "Epoch 2 Batch 1730: Loss = 2.1802\n",
      "Epoch 2 Batch 1740: Loss = 2.5120\n",
      "Epoch 2 Batch 1750: Loss = 2.3663\n",
      "Epoch 2 Batch 1760: Loss = 2.3013\n",
      "Epoch 2 Batch 1770: Loss = 2.1095\n",
      "Epoch 2 Batch 1780: Loss = 2.1693\n",
      "Epoch 2 Batch 1790: Loss = 2.2458\n",
      "Epoch 2 Batch 1800: Loss = 2.3273\n",
      "Epoch 2 Batch 1810: Loss = 2.3472\n",
      "Epoch 2 Batch 1820: Loss = 2.1518\n",
      "Epoch 2 Batch 1830: Loss = 2.2475\n",
      "Epoch 2 Batch 1840: Loss = 2.0452\n",
      "Epoch 2 Batch 1850: Loss = 2.3249\n",
      "Epoch 2 Batch 1860: Loss = 2.3833\n",
      "Epoch 2 Batch 1870: Loss = 2.2041\n",
      "Epoch 2 Batch 1880: Loss = 2.2672\n",
      "Epoch 2 Batch 1890: Loss = 2.1227\n",
      "Epoch 2 Batch 1900: Loss = 2.2692\n",
      "Epoch 2 Batch 1910: Loss = 2.2949\n",
      "Epoch 2 Batch 1920: Loss = 2.1861\n",
      "Epoch 2 Batch 1930: Loss = 2.2520\n",
      "Epoch 2 Batch 1940: Loss = 1.9892\n",
      "Epoch 2 Batch 1950: Loss = 1.9500\n",
      "Epoch 2 Batch 1960: Loss = 2.1542\n",
      "Epoch 2 Batch 1970: Loss = 2.3527\n",
      "Epoch 2 Batch 1980: Loss = 2.1841\n",
      "Epoch 2 Batch 1990: Loss = 2.2122\n",
      "Epoch 2 Batch 2000: Loss = 2.1553\n",
      "Epoch 2 Average Loss: 2.6062\n",
      "Epoch 3 Batch 0: Loss = 2.2578\n",
      "Epoch 3 Batch 10: Loss = 2.2086\n",
      "Epoch 3 Batch 20: Loss = 2.3493\n",
      "Epoch 3 Batch 30: Loss = 2.4049\n",
      "Epoch 3 Batch 40: Loss = 1.9621\n",
      "Epoch 3 Batch 50: Loss = 2.2342\n",
      "Epoch 3 Batch 60: Loss = 2.2536\n",
      "Epoch 3 Batch 70: Loss = 2.3236\n",
      "Epoch 3 Batch 80: Loss = 2.1072\n",
      "Epoch 3 Batch 90: Loss = 1.9212\n",
      "Epoch 3 Batch 100: Loss = 2.4506\n",
      "Epoch 3 Batch 110: Loss = 1.9462\n",
      "Epoch 3 Batch 120: Loss = 2.1135\n",
      "Epoch 3 Batch 130: Loss = 2.1186\n",
      "Epoch 3 Batch 140: Loss = 2.1360\n",
      "Epoch 3 Batch 150: Loss = 1.8926\n",
      "Epoch 3 Batch 160: Loss = 2.0704\n",
      "Epoch 3 Batch 170: Loss = 2.2362\n",
      "Epoch 3 Batch 180: Loss = 1.8000\n",
      "Epoch 3 Batch 190: Loss = 1.9433\n",
      "Epoch 3 Batch 200: Loss = 2.0113\n",
      "Epoch 3 Batch 210: Loss = 2.1274\n",
      "Epoch 3 Batch 220: Loss = 2.1682\n",
      "Epoch 3 Batch 230: Loss = 2.0896\n",
      "Epoch 3 Batch 240: Loss = 2.1241\n",
      "Epoch 3 Batch 250: Loss = 1.9942\n",
      "Epoch 3 Batch 260: Loss = 2.2284\n",
      "Epoch 3 Batch 270: Loss = 1.8479\n",
      "Epoch 3 Batch 280: Loss = 1.9491\n",
      "Epoch 3 Batch 290: Loss = 2.0932\n",
      "Epoch 3 Batch 300: Loss = 2.1580\n",
      "Epoch 3 Batch 310: Loss = 1.8421\n",
      "Epoch 3 Batch 320: Loss = 2.0390\n",
      "Epoch 3 Batch 330: Loss = 2.2072\n",
      "Epoch 3 Batch 340: Loss = 1.9071\n",
      "Epoch 3 Batch 350: Loss = 1.9610\n",
      "Epoch 3 Batch 360: Loss = 2.0666\n",
      "Epoch 3 Batch 370: Loss = 2.0107\n",
      "Epoch 3 Batch 380: Loss = 2.0277\n",
      "Epoch 3 Batch 390: Loss = 1.8368\n",
      "Epoch 3 Batch 400: Loss = 2.0286\n",
      "Epoch 3 Batch 410: Loss = 2.0043\n",
      "Epoch 3 Batch 420: Loss = 1.8442\n",
      "Epoch 3 Batch 430: Loss = 1.8954\n",
      "Epoch 3 Batch 440: Loss = 2.2173\n",
      "Epoch 3 Batch 450: Loss = 1.9444\n",
      "Epoch 3 Batch 460: Loss = 1.9775\n",
      "Epoch 3 Batch 470: Loss = 1.8493\n",
      "Epoch 3 Batch 480: Loss = 2.1207\n",
      "Epoch 3 Batch 490: Loss = 2.0604\n",
      "Epoch 3 Batch 500: Loss = 2.1609\n",
      "Epoch 3 Batch 510: Loss = 2.0243\n",
      "Epoch 3 Batch 520: Loss = 1.9823\n",
      "Epoch 3 Batch 530: Loss = 1.8170\n",
      "Epoch 3 Batch 540: Loss = 1.8588\n",
      "Epoch 3 Batch 550: Loss = 1.9507\n",
      "Epoch 3 Batch 560: Loss = 1.9551\n",
      "Epoch 3 Batch 570: Loss = 2.1277\n",
      "Epoch 3 Batch 580: Loss = 1.9743\n",
      "Epoch 3 Batch 590: Loss = 2.1474\n",
      "Epoch 3 Batch 600: Loss = 1.7001\n",
      "Epoch 3 Batch 610: Loss = 1.9506\n",
      "Epoch 3 Batch 620: Loss = 1.7877\n",
      "Epoch 3 Batch 630: Loss = 1.9900\n",
      "Epoch 3 Batch 640: Loss = 2.0939\n",
      "Epoch 3 Batch 650: Loss = 2.2259\n",
      "Epoch 3 Batch 660: Loss = 2.1435\n",
      "Epoch 3 Batch 670: Loss = 1.9753\n",
      "Epoch 3 Batch 680: Loss = 2.0114\n",
      "Epoch 3 Batch 690: Loss = 2.1856\n",
      "Epoch 3 Batch 700: Loss = 1.9162\n",
      "Epoch 3 Batch 710: Loss = 1.9050\n",
      "Epoch 3 Batch 720: Loss = 1.9649\n",
      "Epoch 3 Batch 730: Loss = 1.8539\n",
      "Epoch 3 Batch 740: Loss = 1.6484\n",
      "Epoch 3 Batch 750: Loss = 1.8693\n",
      "Epoch 3 Batch 760: Loss = 1.8786\n",
      "Epoch 3 Batch 770: Loss = 2.0559\n",
      "Epoch 3 Batch 780: Loss = 1.9970\n",
      "Epoch 3 Batch 790: Loss = 1.8580\n",
      "Epoch 3 Batch 800: Loss = 2.2258\n",
      "Epoch 3 Batch 810: Loss = 2.1867\n",
      "Epoch 3 Batch 820: Loss = 1.8922\n",
      "Epoch 3 Batch 830: Loss = 1.8743\n",
      "Epoch 3 Batch 840: Loss = 1.6124\n",
      "Epoch 3 Batch 850: Loss = 1.7421\n",
      "Epoch 3 Batch 860: Loss = 1.7292\n",
      "Epoch 3 Batch 870: Loss = 1.7884\n",
      "Epoch 3 Batch 880: Loss = 1.9889\n",
      "Epoch 3 Batch 890: Loss = 1.8018\n",
      "Epoch 3 Batch 900: Loss = 1.7580\n",
      "Epoch 3 Batch 910: Loss = 1.8130\n",
      "Epoch 3 Batch 920: Loss = 1.8063\n",
      "Epoch 3 Batch 930: Loss = 1.8375\n",
      "Epoch 3 Batch 940: Loss = 1.8508\n",
      "Epoch 3 Batch 950: Loss = 1.7008\n",
      "Epoch 3 Batch 960: Loss = 1.7568\n",
      "Epoch 3 Batch 970: Loss = 1.7075\n",
      "Epoch 3 Batch 980: Loss = 1.9123\n",
      "Epoch 3 Batch 990: Loss = 1.9742\n",
      "Epoch 3 Batch 1000: Loss = 1.8922\n",
      "Epoch 3 Batch 1010: Loss = 1.8261\n",
      "Epoch 3 Batch 1020: Loss = 2.2252\n",
      "Epoch 3 Batch 1030: Loss = 1.7906\n",
      "Epoch 3 Batch 1040: Loss = 1.9649\n",
      "Epoch 3 Batch 1050: Loss = 1.8940\n",
      "Epoch 3 Batch 1060: Loss = 1.9736\n",
      "Epoch 3 Batch 1070: Loss = 1.7613\n",
      "Epoch 3 Batch 1080: Loss = 1.9681\n",
      "Epoch 3 Batch 1090: Loss = 1.7365\n",
      "Epoch 3 Batch 1100: Loss = 1.7047\n",
      "Epoch 3 Batch 1110: Loss = 2.1570\n",
      "Epoch 3 Batch 1120: Loss = 1.8805\n",
      "Epoch 3 Batch 1130: Loss = 1.9059\n",
      "Epoch 3 Batch 1140: Loss = 1.8552\n",
      "Epoch 3 Batch 1150: Loss = 1.8837\n",
      "Epoch 3 Batch 1160: Loss = 2.0280\n",
      "Epoch 3 Batch 1170: Loss = 1.9749\n",
      "Epoch 3 Batch 1180: Loss = 1.9892\n",
      "Epoch 3 Batch 1190: Loss = 1.6997\n",
      "Epoch 3 Batch 1200: Loss = 1.8966\n",
      "Epoch 3 Batch 1210: Loss = 1.6853\n",
      "Epoch 3 Batch 1220: Loss = 2.0219\n",
      "Epoch 3 Batch 1230: Loss = 1.9747\n",
      "Epoch 3 Batch 1240: Loss = 2.0194\n",
      "Epoch 3 Batch 1250: Loss = 1.8378\n",
      "Epoch 3 Batch 1260: Loss = 1.9299\n",
      "Epoch 3 Batch 1270: Loss = 1.9621\n",
      "Epoch 3 Batch 1280: Loss = 1.9069\n",
      "Epoch 3 Batch 1290: Loss = 1.8508\n",
      "Epoch 3 Batch 1300: Loss = 2.2591\n",
      "Epoch 3 Batch 1310: Loss = 1.8512\n",
      "Epoch 3 Batch 1320: Loss = 1.7924\n",
      "Epoch 3 Batch 1330: Loss = 1.7844\n",
      "Epoch 3 Batch 1340: Loss = 1.7925\n",
      "Epoch 3 Batch 1350: Loss = 1.7340\n",
      "Epoch 3 Batch 1360: Loss = 1.8457\n",
      "Epoch 3 Batch 1370: Loss = 1.6327\n",
      "Epoch 3 Batch 1380: Loss = 1.7620\n",
      "Epoch 3 Batch 1390: Loss = 1.7130\n",
      "Epoch 3 Batch 1400: Loss = 1.5976\n",
      "Epoch 3 Batch 1410: Loss = 1.8335\n",
      "Epoch 3 Batch 1420: Loss = 1.7078\n",
      "Epoch 3 Batch 1430: Loss = 1.7707\n",
      "Epoch 3 Batch 1440: Loss = 1.5931\n",
      "Epoch 3 Batch 1450: Loss = 1.9649\n",
      "Epoch 3 Batch 1460: Loss = 1.9361\n",
      "Epoch 3 Batch 1470: Loss = 1.8024\n",
      "Epoch 3 Batch 1480: Loss = 1.9262\n",
      "Epoch 3 Batch 1490: Loss = 1.6938\n",
      "Epoch 3 Batch 1500: Loss = 2.0008\n",
      "Epoch 3 Batch 1510: Loss = 1.7478\n",
      "Epoch 3 Batch 1520: Loss = 1.7312\n",
      "Epoch 3 Batch 1530: Loss = 1.5547\n",
      "Epoch 3 Batch 1540: Loss = 1.6866\n",
      "Epoch 3 Batch 1550: Loss = 2.0078\n",
      "Epoch 3 Batch 1560: Loss = 1.9029\n",
      "Epoch 3 Batch 1570: Loss = 1.7641\n",
      "Epoch 3 Batch 1580: Loss = 1.7252\n",
      "Epoch 3 Batch 1590: Loss = 1.8094\n",
      "Epoch 3 Batch 1600: Loss = 1.6892\n",
      "Epoch 3 Batch 1610: Loss = 1.6395\n",
      "Epoch 3 Batch 1620: Loss = 1.5972\n",
      "Epoch 3 Batch 1630: Loss = 1.6411\n",
      "Epoch 3 Batch 1640: Loss = 1.7211\n",
      "Epoch 3 Batch 1650: Loss = 1.9411\n",
      "Epoch 3 Batch 1660: Loss = 1.7328\n",
      "Epoch 3 Batch 1670: Loss = 1.5436\n",
      "Epoch 3 Batch 1680: Loss = 1.7671\n",
      "Epoch 3 Batch 1690: Loss = 1.8013\n",
      "Epoch 3 Batch 1700: Loss = 1.7805\n",
      "Epoch 3 Batch 1710: Loss = 1.9056\n",
      "Epoch 3 Batch 1720: Loss = 2.0784\n",
      "Epoch 3 Batch 1730: Loss = 1.5765\n",
      "Epoch 3 Batch 1740: Loss = 1.7146\n",
      "Epoch 3 Batch 1750: Loss = 1.7065\n",
      "Epoch 3 Batch 1760: Loss = 1.8565\n",
      "Epoch 3 Batch 1770: Loss = 1.9834\n",
      "Epoch 3 Batch 1780: Loss = 1.8520\n",
      "Epoch 3 Batch 1790: Loss = 1.7903\n",
      "Epoch 3 Batch 1800: Loss = 1.8240\n",
      "Epoch 3 Batch 1810: Loss = 1.5558\n",
      "Epoch 3 Batch 1820: Loss = 1.6480\n",
      "Epoch 3 Batch 1830: Loss = 2.0052\n",
      "Epoch 3 Batch 1840: Loss = 1.6809\n",
      "Epoch 3 Batch 1850: Loss = 2.0981\n",
      "Epoch 3 Batch 1860: Loss = 1.6344\n",
      "Epoch 3 Batch 1870: Loss = 1.6986\n",
      "Epoch 3 Batch 1880: Loss = 1.7248\n",
      "Epoch 3 Batch 1890: Loss = 2.1488\n",
      "Epoch 3 Batch 1900: Loss = 1.7969\n",
      "Epoch 3 Batch 1910: Loss = 1.8593\n",
      "Epoch 3 Batch 1920: Loss = 1.6955\n",
      "Epoch 3 Batch 1930: Loss = 1.7384\n",
      "Epoch 3 Batch 1940: Loss = 1.7691\n",
      "Epoch 3 Batch 1950: Loss = 1.7712\n",
      "Epoch 3 Batch 1960: Loss = 1.7634\n",
      "Epoch 3 Batch 1970: Loss = 1.6094\n",
      "Epoch 3 Batch 1980: Loss = 1.6485\n",
      "Epoch 3 Batch 1990: Loss = 2.0671\n",
      "Epoch 3 Batch 2000: Loss = 1.8443\n",
      "Epoch 3 Average Loss: 1.9209\n",
      "Epoch 4 Batch 0: Loss = 1.4175\n",
      "Epoch 4 Batch 10: Loss = 1.5472\n",
      "Epoch 4 Batch 20: Loss = 1.6058\n",
      "Epoch 4 Batch 30: Loss = 1.6438\n",
      "Epoch 4 Batch 40: Loss = 1.4262\n",
      "Epoch 4 Batch 50: Loss = 1.5573\n",
      "Epoch 4 Batch 60: Loss = 1.7600\n",
      "Epoch 4 Batch 70: Loss = 1.8067\n",
      "Epoch 4 Batch 80: Loss = 1.4020\n",
      "Epoch 4 Batch 90: Loss = 1.4215\n",
      "Epoch 4 Batch 100: Loss = 1.5461\n",
      "Epoch 4 Batch 110: Loss = 1.6419\n",
      "Epoch 4 Batch 120: Loss = 1.6272\n",
      "Epoch 4 Batch 130: Loss = 1.8077\n",
      "Epoch 4 Batch 140: Loss = 1.8730\n",
      "Epoch 4 Batch 150: Loss = 1.8164\n",
      "Epoch 4 Batch 160: Loss = 1.5631\n",
      "Epoch 4 Batch 170: Loss = 1.8451\n",
      "Epoch 4 Batch 180: Loss = 1.6769\n",
      "Epoch 4 Batch 190: Loss = 1.5664\n",
      "Epoch 4 Batch 200: Loss = 1.7888\n",
      "Epoch 4 Batch 210: Loss = 1.5875\n",
      "Epoch 4 Batch 220: Loss = 1.7891\n",
      "Epoch 4 Batch 230: Loss = 1.5419\n",
      "Epoch 4 Batch 240: Loss = 1.6867\n",
      "Epoch 4 Batch 250: Loss = 1.8060\n",
      "Epoch 4 Batch 260: Loss = 1.9699\n",
      "Epoch 4 Batch 270: Loss = 1.5509\n",
      "Epoch 4 Batch 280: Loss = 1.7175\n",
      "Epoch 4 Batch 290: Loss = 1.5168\n",
      "Epoch 4 Batch 300: Loss = 1.6800\n",
      "Epoch 4 Batch 310: Loss = 1.6205\n",
      "Epoch 4 Batch 320: Loss = 1.8411\n",
      "Epoch 4 Batch 330: Loss = 1.7460\n",
      "Epoch 4 Batch 340: Loss = 1.5422\n",
      "Epoch 4 Batch 350: Loss = 1.5393\n",
      "Epoch 4 Batch 360: Loss = 1.4073\n",
      "Epoch 4 Batch 370: Loss = 1.8180\n",
      "Epoch 4 Batch 380: Loss = 1.7129\n",
      "Epoch 4 Batch 390: Loss = 1.5594\n",
      "Epoch 4 Batch 400: Loss = 1.6783\n",
      "Epoch 4 Batch 410: Loss = 1.8509\n",
      "Epoch 4 Batch 420: Loss = 1.6197\n",
      "Epoch 4 Batch 430: Loss = 1.7535\n",
      "Epoch 4 Batch 440: Loss = 1.5006\n",
      "Epoch 4 Batch 450: Loss = 1.6751\n",
      "Epoch 4 Batch 460: Loss = 1.4852\n",
      "Epoch 4 Batch 470: Loss = 1.7769\n",
      "Epoch 4 Batch 480: Loss = 1.7988\n",
      "Epoch 4 Batch 490: Loss = 1.6970\n",
      "Epoch 4 Batch 500: Loss = 1.2887\n",
      "Epoch 4 Batch 510: Loss = 1.7172\n",
      "Epoch 4 Batch 520: Loss = 1.5272\n",
      "Epoch 4 Batch 530: Loss = 1.6707\n",
      "Epoch 4 Batch 540: Loss = 1.7754\n",
      "Epoch 4 Batch 550: Loss = 1.6820\n",
      "Epoch 4 Batch 560: Loss = 1.6352\n",
      "Epoch 4 Batch 570: Loss = 1.4810\n",
      "Epoch 4 Batch 580: Loss = 1.5745\n",
      "Epoch 4 Batch 590: Loss = 1.4069\n",
      "Epoch 4 Batch 600: Loss = 1.8345\n",
      "Epoch 4 Batch 610: Loss = 1.3874\n",
      "Epoch 4 Batch 620: Loss = 1.6967\n",
      "Epoch 4 Batch 630: Loss = 1.7944\n",
      "Epoch 4 Batch 640: Loss = 1.4595\n",
      "Epoch 4 Batch 650: Loss = 1.7029\n",
      "Epoch 4 Batch 660: Loss = 1.6519\n",
      "Epoch 4 Batch 670: Loss = 1.9279\n",
      "Epoch 4 Batch 680: Loss = 1.4987\n",
      "Epoch 4 Batch 690: Loss = 1.6909\n",
      "Epoch 4 Batch 700: Loss = 1.6279\n",
      "Epoch 4 Batch 710: Loss = 1.5155\n",
      "Epoch 4 Batch 720: Loss = 1.6200\n",
      "Epoch 4 Batch 730: Loss = 1.7475\n",
      "Epoch 4 Batch 740: Loss = 1.6783\n",
      "Epoch 4 Batch 750: Loss = 1.8577\n",
      "Epoch 4 Batch 760: Loss = 1.8142\n",
      "Epoch 4 Batch 770: Loss = 1.4638\n",
      "Epoch 4 Batch 780: Loss = 1.5609\n",
      "Epoch 4 Batch 790: Loss = 1.8295\n",
      "Epoch 4 Batch 800: Loss = 1.6698\n",
      "Epoch 4 Batch 810: Loss = 1.4027\n",
      "Epoch 4 Batch 820: Loss = 1.5585\n",
      "Epoch 4 Batch 830: Loss = 1.6921\n",
      "Epoch 4 Batch 840: Loss = 1.7007\n",
      "Epoch 4 Batch 850: Loss = 1.6014\n",
      "Epoch 4 Batch 860: Loss = 1.6058\n",
      "Epoch 4 Batch 870: Loss = 1.5916\n",
      "Epoch 4 Batch 880: Loss = 1.5480\n",
      "Epoch 4 Batch 890: Loss = 1.6614\n",
      "Epoch 4 Batch 900: Loss = 1.5252\n",
      "Epoch 4 Batch 910: Loss = 1.5706\n",
      "Epoch 4 Batch 920: Loss = 1.8058\n",
      "Epoch 4 Batch 930: Loss = 1.4418\n",
      "Epoch 4 Batch 940: Loss = 1.5954\n",
      "Epoch 4 Batch 950: Loss = 1.4528\n",
      "Epoch 4 Batch 960: Loss = 1.4566\n",
      "Epoch 4 Batch 970: Loss = 1.5637\n",
      "Epoch 4 Batch 980: Loss = 1.5601\n",
      "Epoch 4 Batch 990: Loss = 1.6755\n",
      "Epoch 4 Batch 1000: Loss = 1.4665\n",
      "Epoch 4 Batch 1010: Loss = 1.5752\n",
      "Epoch 4 Batch 1020: Loss = 1.6568\n",
      "Epoch 4 Batch 1030: Loss = 1.6119\n",
      "Epoch 4 Batch 1040: Loss = 1.6461\n",
      "Epoch 4 Batch 1050: Loss = 1.4194\n",
      "Epoch 4 Batch 1060: Loss = 1.4067\n",
      "Epoch 4 Batch 1070: Loss = 1.5450\n",
      "Epoch 4 Batch 1080: Loss = 1.7397\n",
      "Epoch 4 Batch 1090: Loss = 1.9858\n",
      "Epoch 4 Batch 1100: Loss = 1.4832\n",
      "Epoch 4 Batch 1110: Loss = 1.4181\n",
      "Epoch 4 Batch 1120: Loss = 1.5483\n",
      "Epoch 4 Batch 1130: Loss = 1.5915\n",
      "Epoch 4 Batch 1140: Loss = 1.5913\n",
      "Epoch 4 Batch 1150: Loss = 1.5243\n",
      "Epoch 4 Batch 1160: Loss = 1.5444\n",
      "Epoch 4 Batch 1170: Loss = 1.5334\n",
      "Epoch 4 Batch 1180: Loss = 1.5929\n",
      "Epoch 4 Batch 1190: Loss = 1.6890\n",
      "Epoch 4 Batch 1200: Loss = 1.5916\n",
      "Epoch 4 Batch 1210: Loss = 1.5235\n",
      "Epoch 4 Batch 1220: Loss = 1.5797\n",
      "Epoch 4 Batch 1230: Loss = 1.5332\n",
      "Epoch 4 Batch 1240: Loss = 1.6763\n",
      "Epoch 4 Batch 1250: Loss = 1.4438\n",
      "Epoch 4 Batch 1260: Loss = 1.4300\n",
      "Epoch 4 Batch 1270: Loss = 1.4339\n",
      "Epoch 4 Batch 1280: Loss = 1.6455\n",
      "Epoch 4 Batch 1290: Loss = 1.3975\n",
      "Epoch 4 Batch 1300: Loss = 1.4654\n",
      "Epoch 4 Batch 1310: Loss = 1.6645\n",
      "Epoch 4 Batch 1320: Loss = 1.5215\n",
      "Epoch 4 Batch 1330: Loss = 1.7869\n",
      "Epoch 4 Batch 1340: Loss = 1.5593\n",
      "Epoch 4 Batch 1350: Loss = 1.5415\n",
      "Epoch 4 Batch 1360: Loss = 1.6944\n",
      "Epoch 4 Batch 1370: Loss = 1.5038\n",
      "Epoch 4 Batch 1380: Loss = 1.4593\n",
      "Epoch 4 Batch 1390: Loss = 1.5807\n",
      "Epoch 4 Batch 1400: Loss = 1.6232\n",
      "Epoch 4 Batch 1410: Loss = 1.3383\n",
      "Epoch 4 Batch 1420: Loss = 1.5751\n",
      "Epoch 4 Batch 1430: Loss = 1.5078\n",
      "Epoch 4 Batch 1440: Loss = 1.5584\n",
      "Epoch 4 Batch 1450: Loss = 1.4454\n",
      "Epoch 4 Batch 1460: Loss = 1.5422\n",
      "Epoch 4 Batch 1470: Loss = 1.6604\n",
      "Epoch 4 Batch 1480: Loss = 1.6207\n",
      "Epoch 4 Batch 1490: Loss = 1.6806\n",
      "Epoch 4 Batch 1500: Loss = 1.7004\n",
      "Epoch 4 Batch 1510: Loss = 1.7829\n",
      "Epoch 4 Batch 1520: Loss = 1.4067\n",
      "Epoch 4 Batch 1530: Loss = 1.4201\n",
      "Epoch 4 Batch 1540: Loss = 1.4954\n",
      "Epoch 4 Batch 1550: Loss = 1.3236\n",
      "Epoch 4 Batch 1560: Loss = 1.6854\n",
      "Epoch 4 Batch 1570: Loss = 1.4865\n",
      "Epoch 4 Batch 1580: Loss = 1.6010\n",
      "Epoch 4 Batch 1590: Loss = 1.5956\n",
      "Epoch 4 Batch 1600: Loss = 1.6139\n",
      "Epoch 4 Batch 1610: Loss = 1.4484\n",
      "Epoch 4 Batch 1620: Loss = 1.5221\n",
      "Epoch 4 Batch 1630: Loss = 1.4057\n",
      "Epoch 4 Batch 1640: Loss = 1.4412\n",
      "Epoch 4 Batch 1650: Loss = 1.3351\n",
      "Epoch 4 Batch 1660: Loss = 1.3582\n",
      "Epoch 4 Batch 1670: Loss = 1.5239\n",
      "Epoch 4 Batch 1680: Loss = 1.4367\n",
      "Epoch 4 Batch 1690: Loss = 1.5813\n",
      "Epoch 4 Batch 1700: Loss = 1.4093\n",
      "Epoch 4 Batch 1710: Loss = 1.4575\n",
      "Epoch 4 Batch 1720: Loss = 1.4568\n",
      "Epoch 4 Batch 1730: Loss = 1.3801\n",
      "Epoch 4 Batch 1740: Loss = 1.5220\n",
      "Epoch 4 Batch 1750: Loss = 1.5081\n",
      "Epoch 4 Batch 1760: Loss = 1.7177\n",
      "Epoch 4 Batch 1770: Loss = 1.4029\n",
      "Epoch 4 Batch 1780: Loss = 1.4840\n",
      "Epoch 4 Batch 1790: Loss = 1.5270\n",
      "Epoch 4 Batch 1800: Loss = 1.5776\n",
      "Epoch 4 Batch 1810: Loss = 1.5671\n",
      "Epoch 4 Batch 1820: Loss = 1.3122\n",
      "Epoch 4 Batch 1830: Loss = 1.4565\n",
      "Epoch 4 Batch 1840: Loss = 1.4191\n",
      "Epoch 4 Batch 1850: Loss = 1.3513\n",
      "Epoch 4 Batch 1860: Loss = 1.4159\n",
      "Epoch 4 Batch 1870: Loss = 1.5228\n",
      "Epoch 4 Batch 1880: Loss = 1.3421\n",
      "Epoch 4 Batch 1890: Loss = 1.5512\n",
      "Epoch 4 Batch 1900: Loss = 1.6993\n",
      "Epoch 4 Batch 1910: Loss = 1.4858\n",
      "Epoch 4 Batch 1920: Loss = 1.4773\n",
      "Epoch 4 Batch 1930: Loss = 1.5260\n",
      "Epoch 4 Batch 1940: Loss = 1.6468\n",
      "Epoch 4 Batch 1950: Loss = 1.4423\n",
      "Epoch 4 Batch 1960: Loss = 1.5108\n",
      "Epoch 4 Batch 1970: Loss = 1.6712\n",
      "Epoch 4 Batch 1980: Loss = 1.5481\n",
      "Epoch 4 Batch 1990: Loss = 1.4232\n",
      "Epoch 4 Batch 2000: Loss = 1.6354\n",
      "Epoch 4 Average Loss: 1.6026\n",
      "Epoch 5 Batch 0: Loss = 1.5608\n",
      "Epoch 5 Batch 10: Loss = 1.3146\n",
      "Epoch 5 Batch 20: Loss = 1.5298\n",
      "Epoch 5 Batch 30: Loss = 1.3879\n",
      "Epoch 5 Batch 40: Loss = 1.5979\n",
      "Epoch 5 Batch 50: Loss = 1.4402\n",
      "Epoch 5 Batch 60: Loss = 1.5419\n",
      "Epoch 5 Batch 70: Loss = 1.5889\n",
      "Epoch 5 Batch 80: Loss = 1.5302\n",
      "Epoch 5 Batch 90: Loss = 1.2548\n",
      "Epoch 5 Batch 100: Loss = 1.5013\n",
      "Epoch 5 Batch 110: Loss = 1.3288\n",
      "Epoch 5 Batch 120: Loss = 1.7051\n",
      "Epoch 5 Batch 130: Loss = 1.6525\n",
      "Epoch 5 Batch 140: Loss = 1.4560\n",
      "Epoch 5 Batch 150: Loss = 1.4731\n",
      "Epoch 5 Batch 160: Loss = 1.4784\n",
      "Epoch 5 Batch 170: Loss = 1.7739\n",
      "Epoch 5 Batch 180: Loss = 1.3784\n",
      "Epoch 5 Batch 190: Loss = 1.4235\n",
      "Epoch 5 Batch 200: Loss = 1.4633\n",
      "Epoch 5 Batch 210: Loss = 1.5882\n",
      "Epoch 5 Batch 220: Loss = 1.3677\n",
      "Epoch 5 Batch 230: Loss = 1.3459\n",
      "Epoch 5 Batch 240: Loss = 1.3120\n",
      "Epoch 5 Batch 250: Loss = 1.5284\n",
      "Epoch 5 Batch 260: Loss = 1.4957\n",
      "Epoch 5 Batch 270: Loss = 1.6436\n",
      "Epoch 5 Batch 280: Loss = 1.3170\n",
      "Epoch 5 Batch 290: Loss = 1.1380\n",
      "Epoch 5 Batch 300: Loss = 1.5603\n",
      "Epoch 5 Batch 310: Loss = 1.3614\n",
      "Epoch 5 Batch 320: Loss = 1.5621\n",
      "Epoch 5 Batch 330: Loss = 1.2660\n",
      "Epoch 5 Batch 340: Loss = 1.5101\n",
      "Epoch 5 Batch 350: Loss = 1.2440\n",
      "Epoch 5 Batch 360: Loss = 1.5241\n",
      "Epoch 5 Batch 370: Loss = 1.5266\n",
      "Epoch 5 Batch 380: Loss = 1.1629\n",
      "Epoch 5 Batch 390: Loss = 1.3617\n",
      "Epoch 5 Batch 400: Loss = 1.4687\n",
      "Epoch 5 Batch 410: Loss = 1.4400\n",
      "Epoch 5 Batch 420: Loss = 1.7355\n",
      "Epoch 5 Batch 430: Loss = 1.2552\n",
      "Epoch 5 Batch 440: Loss = 1.2585\n",
      "Epoch 5 Batch 450: Loss = 1.3688\n",
      "Epoch 5 Batch 460: Loss = 1.5723\n",
      "Epoch 5 Batch 470: Loss = 1.4600\n",
      "Epoch 5 Batch 480: Loss = 1.2472\n",
      "Epoch 5 Batch 490: Loss = 1.5674\n",
      "Epoch 5 Batch 500: Loss = 1.5718\n",
      "Epoch 5 Batch 510: Loss = 1.7738\n",
      "Epoch 5 Batch 520: Loss = 1.4406\n",
      "Epoch 5 Batch 530: Loss = 1.4589\n",
      "Epoch 5 Batch 540: Loss = 1.4350\n",
      "Epoch 5 Batch 550: Loss = 1.3351\n",
      "Epoch 5 Batch 560: Loss = 1.4073\n",
      "Epoch 5 Batch 570: Loss = 1.3799\n",
      "Epoch 5 Batch 580: Loss = 1.6128\n",
      "Epoch 5 Batch 590: Loss = 1.4219\n",
      "Epoch 5 Batch 600: Loss = 1.4630\n",
      "Epoch 5 Batch 610: Loss = 1.4099\n",
      "Epoch 5 Batch 620: Loss = 1.2149\n",
      "Epoch 5 Batch 630: Loss = 1.6388\n",
      "Epoch 5 Batch 640: Loss = 1.3115\n",
      "Epoch 5 Batch 650: Loss = 1.5010\n",
      "Epoch 5 Batch 660: Loss = 1.6367\n",
      "Epoch 5 Batch 670: Loss = 1.6062\n",
      "Epoch 5 Batch 680: Loss = 1.4573\n",
      "Epoch 5 Batch 690: Loss = 1.3276\n",
      "Epoch 5 Batch 700: Loss = 1.2432\n",
      "Epoch 5 Batch 710: Loss = 1.4708\n",
      "Epoch 5 Batch 720: Loss = 1.2435\n",
      "Epoch 5 Batch 730: Loss = 1.5130\n",
      "Epoch 5 Batch 740: Loss = 1.3416\n",
      "Epoch 5 Batch 750: Loss = 1.4452\n",
      "Epoch 5 Batch 760: Loss = 1.4638\n",
      "Epoch 5 Batch 770: Loss = 1.3089\n",
      "Epoch 5 Batch 780: Loss = 1.5335\n",
      "Epoch 5 Batch 790: Loss = 1.3895\n",
      "Epoch 5 Batch 800: Loss = 1.2906\n",
      "Epoch 5 Batch 810: Loss = 1.3970\n",
      "Epoch 5 Batch 820: Loss = 1.3728\n",
      "Epoch 5 Batch 830: Loss = 1.5348\n",
      "Epoch 5 Batch 840: Loss = 1.3291\n",
      "Epoch 5 Batch 850: Loss = 1.2744\n",
      "Epoch 5 Batch 860: Loss = 1.3200\n",
      "Epoch 5 Batch 870: Loss = 1.3410\n",
      "Epoch 5 Batch 880: Loss = 1.4875\n",
      "Epoch 5 Batch 890: Loss = 1.4187\n",
      "Epoch 5 Batch 900: Loss = 1.5687\n",
      "Epoch 5 Batch 910: Loss = 1.3644\n",
      "Epoch 5 Batch 920: Loss = 1.5175\n",
      "Epoch 5 Batch 930: Loss = 1.6365\n",
      "Epoch 5 Batch 940: Loss = 1.6524\n",
      "Epoch 5 Batch 950: Loss = 1.2404\n",
      "Epoch 5 Batch 960: Loss = 1.3683\n",
      "Epoch 5 Batch 970: Loss = 1.5599\n",
      "Epoch 5 Batch 980: Loss = 1.3295\n",
      "Epoch 5 Batch 990: Loss = 1.3812\n",
      "Epoch 5 Batch 1000: Loss = 1.3772\n",
      "Epoch 5 Batch 1010: Loss = 1.4453\n",
      "Epoch 5 Batch 1020: Loss = 1.2484\n",
      "Epoch 5 Batch 1030: Loss = 1.5481\n",
      "Epoch 5 Batch 1040: Loss = 1.4012\n",
      "Epoch 5 Batch 1050: Loss = 1.5773\n",
      "Epoch 5 Batch 1060: Loss = 1.2648\n",
      "Epoch 5 Batch 1070: Loss = 1.4658\n",
      "Epoch 5 Batch 1080: Loss = 1.3734\n",
      "Epoch 5 Batch 1090: Loss = 1.3922\n",
      "Epoch 5 Batch 1100: Loss = 1.5802\n",
      "Epoch 5 Batch 1110: Loss = 1.4122\n",
      "Epoch 5 Batch 1120: Loss = 1.4579\n",
      "Epoch 5 Batch 1130: Loss = 1.5317\n",
      "Epoch 5 Batch 1140: Loss = 1.5111\n",
      "Epoch 5 Batch 1150: Loss = 1.5022\n",
      "Epoch 5 Batch 1160: Loss = 1.3484\n",
      "Epoch 5 Batch 1170: Loss = 1.3524\n",
      "Epoch 5 Batch 1180: Loss = 1.3676\n",
      "Epoch 5 Batch 1190: Loss = 1.2668\n",
      "Epoch 5 Batch 1200: Loss = 1.2514\n",
      "Epoch 5 Batch 1210: Loss = 1.4374\n",
      "Epoch 5 Batch 1220: Loss = 1.2729\n",
      "Epoch 5 Batch 1230: Loss = 1.3999\n",
      "Epoch 5 Batch 1240: Loss = 1.3961\n",
      "Epoch 5 Batch 1250: Loss = 1.3708\n",
      "Epoch 5 Batch 1260: Loss = 1.2795\n",
      "Epoch 5 Batch 1270: Loss = 1.4586\n",
      "Epoch 5 Batch 1280: Loss = 1.4065\n",
      "Epoch 5 Batch 1290: Loss = 1.3276\n",
      "Epoch 5 Batch 1300: Loss = 1.5238\n",
      "Epoch 5 Batch 1310: Loss = 1.2226\n",
      "Epoch 5 Batch 1320: Loss = 1.4013\n",
      "Epoch 5 Batch 1330: Loss = 1.1716\n",
      "Epoch 5 Batch 1340: Loss = 1.3227\n",
      "Epoch 5 Batch 1350: Loss = 1.2232\n",
      "Epoch 5 Batch 1360: Loss = 1.5022\n",
      "Epoch 5 Batch 1370: Loss = 1.5969\n",
      "Epoch 5 Batch 1380: Loss = 1.5242\n",
      "Epoch 5 Batch 1390: Loss = 1.4408\n",
      "Epoch 5 Batch 1400: Loss = 1.3855\n",
      "Epoch 5 Batch 1410: Loss = 1.4668\n",
      "Epoch 5 Batch 1420: Loss = 1.4403\n",
      "Epoch 5 Batch 1430: Loss = 1.3568\n",
      "Epoch 5 Batch 1440: Loss = 1.3455\n",
      "Epoch 5 Batch 1450: Loss = 1.2970\n",
      "Epoch 5 Batch 1460: Loss = 1.3692\n",
      "Epoch 5 Batch 1470: Loss = 1.5713\n",
      "Epoch 5 Batch 1480: Loss = 1.3429\n",
      "Epoch 5 Batch 1490: Loss = 1.4884\n",
      "Epoch 5 Batch 1500: Loss = 1.6467\n",
      "Epoch 5 Batch 1510: Loss = 1.4111\n",
      "Epoch 5 Batch 1520: Loss = 1.6792\n",
      "Epoch 5 Batch 1530: Loss = 1.4483\n",
      "Epoch 5 Batch 1540: Loss = 1.2839\n",
      "Epoch 5 Batch 1550: Loss = 1.3503\n",
      "Epoch 5 Batch 1560: Loss = 1.2840\n",
      "Epoch 5 Batch 1570: Loss = 1.1488\n",
      "Epoch 5 Batch 1580: Loss = 1.6528\n",
      "Epoch 5 Batch 1590: Loss = 1.3548\n",
      "Epoch 5 Batch 1600: Loss = 1.3788\n",
      "Epoch 5 Batch 1610: Loss = 1.3409\n",
      "Epoch 5 Batch 1620: Loss = 1.4834\n",
      "Epoch 5 Batch 1630: Loss = 1.3175\n",
      "Epoch 5 Batch 1640: Loss = 1.4280\n",
      "Epoch 5 Batch 1650: Loss = 1.5104\n",
      "Epoch 5 Batch 1660: Loss = 1.0567\n",
      "Epoch 5 Batch 1670: Loss = 1.2423\n",
      "Epoch 5 Batch 1680: Loss = 1.2899\n",
      "Epoch 5 Batch 1690: Loss = 1.3108\n",
      "Epoch 5 Batch 1700: Loss = 1.3594\n",
      "Epoch 5 Batch 1710: Loss = 1.3780\n",
      "Epoch 5 Batch 1720: Loss = 1.4552\n",
      "Epoch 5 Batch 1730: Loss = 1.5222\n",
      "Epoch 5 Batch 1740: Loss = 1.1421\n",
      "Epoch 5 Batch 1750: Loss = 1.3900\n",
      "Epoch 5 Batch 1760: Loss = 1.4709\n",
      "Epoch 5 Batch 1770: Loss = 1.3957\n",
      "Epoch 5 Batch 1780: Loss = 1.3859\n",
      "Epoch 5 Batch 1790: Loss = 1.3933\n",
      "Epoch 5 Batch 1800: Loss = 1.3203\n",
      "Epoch 5 Batch 1810: Loss = 1.6606\n",
      "Epoch 5 Batch 1820: Loss = 1.1591\n",
      "Epoch 5 Batch 1830: Loss = 1.2698\n",
      "Epoch 5 Batch 1840: Loss = 1.3194\n",
      "Epoch 5 Batch 1850: Loss = 1.6924\n",
      "Epoch 5 Batch 1860: Loss = 1.4638\n",
      "Epoch 5 Batch 1870: Loss = 1.5623\n",
      "Epoch 5 Batch 1880: Loss = 1.4827\n",
      "Epoch 5 Batch 1890: Loss = 1.3909\n",
      "Epoch 5 Batch 1900: Loss = 1.5092\n",
      "Epoch 5 Batch 1910: Loss = 1.2721\n",
      "Epoch 5 Batch 1920: Loss = 1.3323\n",
      "Epoch 5 Batch 1930: Loss = 1.3476\n",
      "Epoch 5 Batch 1940: Loss = 1.3713\n",
      "Epoch 5 Batch 1950: Loss = 1.4993\n",
      "Epoch 5 Batch 1960: Loss = 1.1081\n",
      "Epoch 5 Batch 1970: Loss = 1.3316\n",
      "Epoch 5 Batch 1980: Loss = 1.2422\n",
      "Epoch 5 Batch 1990: Loss = 1.4022\n",
      "Epoch 5 Batch 2000: Loss = 1.3258\n",
      "Epoch 5 Average Loss: 1.4197\n",
      "Epoch 6 Batch 0: Loss = 1.3623\n",
      "Epoch 6 Batch 10: Loss = 1.2988\n",
      "Epoch 6 Batch 20: Loss = 1.2792\n",
      "Epoch 6 Batch 30: Loss = 1.3672\n",
      "Epoch 6 Batch 40: Loss = 1.2679\n",
      "Epoch 6 Batch 50: Loss = 1.4121\n",
      "Epoch 6 Batch 60: Loss = 1.3268\n",
      "Epoch 6 Batch 70: Loss = 1.3471\n",
      "Epoch 6 Batch 80: Loss = 1.4934\n",
      "Epoch 6 Batch 90: Loss = 1.3684\n",
      "Epoch 6 Batch 100: Loss = 1.5874\n",
      "Epoch 6 Batch 110: Loss = 1.2605\n",
      "Epoch 6 Batch 120: Loss = 1.3505\n",
      "Epoch 6 Batch 130: Loss = 1.2960\n",
      "Epoch 6 Batch 140: Loss = 1.3281\n",
      "Epoch 6 Batch 150: Loss = 1.4880\n",
      "Epoch 6 Batch 160: Loss = 1.3890\n",
      "Epoch 6 Batch 170: Loss = 1.2016\n",
      "Epoch 6 Batch 180: Loss = 1.1961\n",
      "Epoch 6 Batch 190: Loss = 1.3074\n",
      "Epoch 6 Batch 200: Loss = 1.1484\n",
      "Epoch 6 Batch 210: Loss = 1.3459\n",
      "Epoch 6 Batch 220: Loss = 1.4780\n",
      "Epoch 6 Batch 230: Loss = 1.3537\n",
      "Epoch 6 Batch 240: Loss = 1.2712\n",
      "Epoch 6 Batch 250: Loss = 1.3151\n",
      "Epoch 6 Batch 260: Loss = 1.2968\n",
      "Epoch 6 Batch 270: Loss = 1.2615\n",
      "Epoch 6 Batch 280: Loss = 1.3742\n",
      "Epoch 6 Batch 290: Loss = 1.1278\n",
      "Epoch 6 Batch 300: Loss = 1.2347\n",
      "Epoch 6 Batch 310: Loss = 1.3704\n",
      "Epoch 6 Batch 320: Loss = 1.2894\n",
      "Epoch 6 Batch 330: Loss = 1.1685\n",
      "Epoch 6 Batch 340: Loss = 1.3672\n",
      "Epoch 6 Batch 350: Loss = 1.3947\n",
      "Epoch 6 Batch 360: Loss = 1.3663\n",
      "Epoch 6 Batch 370: Loss = 1.2582\n",
      "Epoch 6 Batch 380: Loss = 1.2960\n",
      "Epoch 6 Batch 390: Loss = 1.2460\n",
      "Epoch 6 Batch 400: Loss = 1.1769\n",
      "Epoch 6 Batch 410: Loss = 1.2108\n",
      "Epoch 6 Batch 420: Loss = 1.3241\n",
      "Epoch 6 Batch 430: Loss = 1.2368\n",
      "Epoch 6 Batch 440: Loss = 1.1756\n",
      "Epoch 6 Batch 450: Loss = 1.3663\n",
      "Epoch 6 Batch 460: Loss = 1.3264\n",
      "Epoch 6 Batch 470: Loss = 1.2220\n",
      "Epoch 6 Batch 480: Loss = 1.1762\n",
      "Epoch 6 Batch 490: Loss = 1.1651\n",
      "Epoch 6 Batch 500: Loss = 1.3275\n",
      "Epoch 6 Batch 510: Loss = 1.2501\n",
      "Epoch 6 Batch 520: Loss = 1.2044\n",
      "Epoch 6 Batch 530: Loss = 1.3293\n",
      "Epoch 6 Batch 540: Loss = 1.2769\n",
      "Epoch 6 Batch 550: Loss = 1.1583\n",
      "Epoch 6 Batch 560: Loss = 1.4520\n",
      "Epoch 6 Batch 570: Loss = 1.2484\n",
      "Epoch 6 Batch 580: Loss = 1.2152\n",
      "Epoch 6 Batch 590: Loss = 1.4441\n",
      "Epoch 6 Batch 600: Loss = 1.3402\n",
      "Epoch 6 Batch 610: Loss = 1.2050\n",
      "Epoch 6 Batch 620: Loss = 1.2225\n",
      "Epoch 6 Batch 630: Loss = 1.2724\n",
      "Epoch 6 Batch 640: Loss = 1.3598\n",
      "Epoch 6 Batch 650: Loss = 1.3278\n",
      "Epoch 6 Batch 660: Loss = 1.3162\n",
      "Epoch 6 Batch 670: Loss = 1.2908\n",
      "Epoch 6 Batch 680: Loss = 1.0768\n",
      "Epoch 6 Batch 690: Loss = 1.3561\n",
      "Epoch 6 Batch 700: Loss = 1.3394\n",
      "Epoch 6 Batch 710: Loss = 1.3784\n",
      "Epoch 6 Batch 720: Loss = 1.2370\n",
      "Epoch 6 Batch 730: Loss = 1.2527\n",
      "Epoch 6 Batch 740: Loss = 1.4609\n",
      "Epoch 6 Batch 750: Loss = 1.1240\n",
      "Epoch 6 Batch 760: Loss = 1.3055\n",
      "Epoch 6 Batch 770: Loss = 1.4362\n",
      "Epoch 6 Batch 780: Loss = 1.3956\n",
      "Epoch 6 Batch 790: Loss = 1.1173\n",
      "Epoch 6 Batch 800: Loss = 1.2563\n",
      "Epoch 6 Batch 810: Loss = 1.2104\n",
      "Epoch 6 Batch 820: Loss = 1.4550\n",
      "Epoch 6 Batch 830: Loss = 1.3741\n",
      "Epoch 6 Batch 840: Loss = 1.2538\n",
      "Epoch 6 Batch 850: Loss = 1.1825\n",
      "Epoch 6 Batch 860: Loss = 1.3073\n",
      "Epoch 6 Batch 870: Loss = 1.5204\n",
      "Epoch 6 Batch 880: Loss = 1.4066\n",
      "Epoch 6 Batch 890: Loss = 1.1589\n",
      "Epoch 6 Batch 900: Loss = 1.3826\n",
      "Epoch 6 Batch 910: Loss = 1.3930\n",
      "Epoch 6 Batch 920: Loss = 1.3557\n",
      "Epoch 6 Batch 930: Loss = 1.2566\n",
      "Epoch 6 Batch 940: Loss = 1.2959\n",
      "Epoch 6 Batch 950: Loss = 1.2575\n",
      "Epoch 6 Batch 960: Loss = 1.2522\n",
      "Epoch 6 Batch 970: Loss = 1.2153\n",
      "Epoch 6 Batch 980: Loss = 1.3621\n",
      "Epoch 6 Batch 990: Loss = 1.3089\n",
      "Epoch 6 Batch 1000: Loss = 1.0724\n",
      "Epoch 6 Batch 1010: Loss = 1.3464\n",
      "Epoch 6 Batch 1020: Loss = 1.4464\n",
      "Epoch 6 Batch 1030: Loss = 1.2268\n",
      "Epoch 6 Batch 1040: Loss = 1.3600\n",
      "Epoch 6 Batch 1050: Loss = 1.2275\n",
      "Epoch 6 Batch 1060: Loss = 1.2124\n",
      "Epoch 6 Batch 1070: Loss = 1.3506\n",
      "Epoch 6 Batch 1080: Loss = 1.3547\n",
      "Epoch 6 Batch 1090: Loss = 1.3020\n",
      "Epoch 6 Batch 1100: Loss = 1.3731\n",
      "Epoch 6 Batch 1110: Loss = 1.2100\n",
      "Epoch 6 Batch 1120: Loss = 1.1265\n",
      "Epoch 6 Batch 1130: Loss = 1.1743\n",
      "Epoch 6 Batch 1140: Loss = 1.2380\n",
      "Epoch 6 Batch 1150: Loss = 1.5293\n",
      "Epoch 6 Batch 1160: Loss = 1.3534\n",
      "Epoch 6 Batch 1170: Loss = 1.3036\n",
      "Epoch 6 Batch 1180: Loss = 1.2272\n",
      "Epoch 6 Batch 1190: Loss = 1.1757\n",
      "Epoch 6 Batch 1200: Loss = 1.2342\n",
      "Epoch 6 Batch 1210: Loss = 1.0833\n",
      "Epoch 6 Batch 1220: Loss = 1.3116\n",
      "Epoch 6 Batch 1230: Loss = 1.1650\n",
      "Epoch 6 Batch 1240: Loss = 1.2810\n",
      "Epoch 6 Batch 1250: Loss = 1.3109\n",
      "Epoch 6 Batch 1260: Loss = 1.3101\n",
      "Epoch 6 Batch 1270: Loss = 1.3341\n",
      "Epoch 6 Batch 1280: Loss = 1.3693\n",
      "Epoch 6 Batch 1290: Loss = 1.4600\n",
      "Epoch 6 Batch 1300: Loss = 1.3876\n",
      "Epoch 6 Batch 1310: Loss = 1.2318\n",
      "Epoch 6 Batch 1320: Loss = 1.2265\n",
      "Epoch 6 Batch 1330: Loss = 1.3712\n",
      "Epoch 6 Batch 1340: Loss = 1.4285\n",
      "Epoch 6 Batch 1350: Loss = 1.2852\n",
      "Epoch 6 Batch 1360: Loss = 1.1759\n",
      "Epoch 6 Batch 1370: Loss = 1.5891\n",
      "Epoch 6 Batch 1380: Loss = 1.4267\n",
      "Epoch 6 Batch 1390: Loss = 1.2582\n",
      "Epoch 6 Batch 1400: Loss = 1.3150\n",
      "Epoch 6 Batch 1410: Loss = 1.6232\n",
      "Epoch 6 Batch 1420: Loss = 1.5547\n",
      "Epoch 6 Batch 1430: Loss = 1.3280\n",
      "Epoch 6 Batch 1440: Loss = 1.3645\n",
      "Epoch 6 Batch 1450: Loss = 1.3860\n",
      "Epoch 6 Batch 1460: Loss = 1.0900\n",
      "Epoch 6 Batch 1470: Loss = 1.3327\n",
      "Epoch 6 Batch 1480: Loss = 1.0798\n",
      "Epoch 6 Batch 1490: Loss = 1.3157\n",
      "Epoch 6 Batch 1500: Loss = 1.4148\n",
      "Epoch 6 Batch 1510: Loss = 1.2860\n",
      "Epoch 6 Batch 1520: Loss = 1.2590\n",
      "Epoch 6 Batch 1530: Loss = 1.3368\n",
      "Epoch 6 Batch 1540: Loss = 1.4294\n",
      "Epoch 6 Batch 1550: Loss = 1.2079\n",
      "Epoch 6 Batch 1560: Loss = 1.1024\n",
      "Epoch 6 Batch 1570: Loss = 1.2472\n",
      "Epoch 6 Batch 1580: Loss = 1.5239\n",
      "Epoch 6 Batch 1590: Loss = 1.3813\n",
      "Epoch 6 Batch 1600: Loss = 1.3110\n",
      "Epoch 6 Batch 1610: Loss = 1.3958\n",
      "Epoch 6 Batch 1620: Loss = 1.2876\n",
      "Epoch 6 Batch 1630: Loss = 1.3772\n",
      "Epoch 6 Batch 1640: Loss = 1.2292\n",
      "Epoch 6 Batch 1650: Loss = 1.3632\n",
      "Epoch 6 Batch 1660: Loss = 1.2041\n",
      "Epoch 6 Batch 1670: Loss = 1.2116\n",
      "Epoch 6 Batch 1680: Loss = 1.4015\n",
      "Epoch 6 Batch 1690: Loss = 1.2247\n",
      "Epoch 6 Batch 1700: Loss = 1.2760\n",
      "Epoch 6 Batch 1710: Loss = 1.3428\n",
      "Epoch 6 Batch 1720: Loss = 1.3328\n",
      "Epoch 6 Batch 1730: Loss = 1.2872\n",
      "Epoch 6 Batch 1740: Loss = 1.1696\n",
      "Epoch 6 Batch 1750: Loss = 1.2670\n",
      "Epoch 6 Batch 1760: Loss = 1.1156\n",
      "Epoch 6 Batch 1770: Loss = 1.3516\n",
      "Epoch 6 Batch 1780: Loss = 1.4357\n",
      "Epoch 6 Batch 1790: Loss = 1.3006\n",
      "Epoch 6 Batch 1800: Loss = 1.2734\n",
      "Epoch 6 Batch 1810: Loss = 1.1583\n",
      "Epoch 6 Batch 1820: Loss = 1.1527\n",
      "Epoch 6 Batch 1830: Loss = 1.3858\n",
      "Epoch 6 Batch 1840: Loss = 1.3239\n",
      "Epoch 6 Batch 1850: Loss = 1.2449\n",
      "Epoch 6 Batch 1860: Loss = 1.2819\n",
      "Epoch 6 Batch 1870: Loss = 1.2832\n",
      "Epoch 6 Batch 1880: Loss = 1.3634\n",
      "Epoch 6 Batch 1890: Loss = 1.4729\n",
      "Epoch 6 Batch 1900: Loss = 1.4608\n",
      "Epoch 6 Batch 1910: Loss = 1.2668\n",
      "Epoch 6 Batch 1920: Loss = 1.3307\n",
      "Epoch 6 Batch 1930: Loss = 1.2002\n",
      "Epoch 6 Batch 1940: Loss = 1.4003\n",
      "Epoch 6 Batch 1950: Loss = 1.4393\n",
      "Epoch 6 Batch 1960: Loss = 1.1644\n",
      "Epoch 6 Batch 1970: Loss = 1.3348\n",
      "Epoch 6 Batch 1980: Loss = 1.1763\n",
      "Epoch 6 Batch 1990: Loss = 1.3245\n",
      "Epoch 6 Batch 2000: Loss = 1.3402\n",
      "Epoch 6 Average Loss: 1.3058\n",
      "Epoch 7 Batch 0: Loss = 1.4126\n",
      "Epoch 7 Batch 10: Loss = 1.0957\n",
      "Epoch 7 Batch 20: Loss = 1.1564\n",
      "Epoch 7 Batch 30: Loss = 1.1607\n",
      "Epoch 7 Batch 40: Loss = 1.2717\n",
      "Epoch 7 Batch 50: Loss = 1.3455\n",
      "Epoch 7 Batch 60: Loss = 1.2015\n",
      "Epoch 7 Batch 70: Loss = 1.2701\n",
      "Epoch 7 Batch 80: Loss = 1.1972\n",
      "Epoch 7 Batch 90: Loss = 1.4460\n",
      "Epoch 7 Batch 100: Loss = 1.2909\n",
      "Epoch 7 Batch 110: Loss = 1.3314\n",
      "Epoch 7 Batch 120: Loss = 1.2671\n",
      "Epoch 7 Batch 130: Loss = 1.1222\n",
      "Epoch 7 Batch 140: Loss = 1.1027\n",
      "Epoch 7 Batch 150: Loss = 1.1244\n",
      "Epoch 7 Batch 160: Loss = 1.1665\n",
      "Epoch 7 Batch 170: Loss = 1.0983\n",
      "Epoch 7 Batch 180: Loss = 1.1813\n",
      "Epoch 7 Batch 190: Loss = 1.1670\n",
      "Epoch 7 Batch 200: Loss = 1.3531\n",
      "Epoch 7 Batch 210: Loss = 1.3883\n",
      "Epoch 7 Batch 220: Loss = 1.1400\n",
      "Epoch 7 Batch 230: Loss = 1.1640\n",
      "Epoch 7 Batch 240: Loss = 1.2635\n",
      "Epoch 7 Batch 250: Loss = 1.2182\n",
      "Epoch 7 Batch 260: Loss = 1.1280\n",
      "Epoch 7 Batch 270: Loss = 1.0960\n",
      "Epoch 7 Batch 280: Loss = 1.2156\n",
      "Epoch 7 Batch 290: Loss = 1.1215\n",
      "Epoch 7 Batch 300: Loss = 1.1641\n",
      "Epoch 7 Batch 310: Loss = 1.0298\n",
      "Epoch 7 Batch 320: Loss = 1.2508\n",
      "Epoch 7 Batch 330: Loss = 1.3181\n",
      "Epoch 7 Batch 340: Loss = 1.3404\n",
      "Epoch 7 Batch 350: Loss = 1.2291\n",
      "Epoch 7 Batch 360: Loss = 1.0916\n",
      "Epoch 7 Batch 370: Loss = 1.3114\n",
      "Epoch 7 Batch 380: Loss = 1.1473\n",
      "Epoch 7 Batch 390: Loss = 1.2435\n",
      "Epoch 7 Batch 400: Loss = 1.3166\n",
      "Epoch 7 Batch 410: Loss = 1.3377\n",
      "Epoch 7 Batch 420: Loss = 1.0659\n",
      "Epoch 7 Batch 430: Loss = 1.1158\n",
      "Epoch 7 Batch 440: Loss = 1.3697\n",
      "Epoch 7 Batch 450: Loss = 1.2220\n",
      "Epoch 7 Batch 460: Loss = 1.1466\n",
      "Epoch 7 Batch 470: Loss = 1.1517\n",
      "Epoch 7 Batch 480: Loss = 1.4008\n",
      "Epoch 7 Batch 490: Loss = 1.1920\n",
      "Epoch 7 Batch 500: Loss = 1.2173\n",
      "Epoch 7 Batch 510: Loss = 1.2918\n",
      "Epoch 7 Batch 520: Loss = 1.1832\n",
      "Epoch 7 Batch 530: Loss = 1.3843\n",
      "Epoch 7 Batch 540: Loss = 1.3795\n",
      "Epoch 7 Batch 550: Loss = 1.1483\n",
      "Epoch 7 Batch 560: Loss = 1.2210\n",
      "Epoch 7 Batch 570: Loss = 1.1404\n",
      "Epoch 7 Batch 580: Loss = 1.2235\n",
      "Epoch 7 Batch 590: Loss = 1.1850\n",
      "Epoch 7 Batch 600: Loss = 1.1435\n",
      "Epoch 7 Batch 610: Loss = 1.0777\n",
      "Epoch 7 Batch 620: Loss = 1.2706\n",
      "Epoch 7 Batch 630: Loss = 1.2760\n",
      "Epoch 7 Batch 640: Loss = 1.2153\n",
      "Epoch 7 Batch 650: Loss = 1.2260\n",
      "Epoch 7 Batch 660: Loss = 1.3700\n",
      "Epoch 7 Batch 670: Loss = 1.2743\n",
      "Epoch 7 Batch 680: Loss = 1.1785\n",
      "Epoch 7 Batch 690: Loss = 1.2805\n",
      "Epoch 7 Batch 700: Loss = 1.2303\n",
      "Epoch 7 Batch 710: Loss = 1.2279\n",
      "Epoch 7 Batch 720: Loss = 1.4029\n",
      "Epoch 7 Batch 730: Loss = 1.2337\n",
      "Epoch 7 Batch 740: Loss = 1.1578\n",
      "Epoch 7 Batch 750: Loss = 0.9764\n",
      "Epoch 7 Batch 760: Loss = 1.3480\n",
      "Epoch 7 Batch 770: Loss = 1.0818\n",
      "Epoch 7 Batch 780: Loss = 1.0926\n",
      "Epoch 7 Batch 790: Loss = 1.1555\n",
      "Epoch 7 Batch 800: Loss = 1.1339\n",
      "Epoch 7 Batch 810: Loss = 1.1176\n",
      "Epoch 7 Batch 820: Loss = 1.1485\n",
      "Epoch 7 Batch 830: Loss = 1.2261\n",
      "Epoch 7 Batch 840: Loss = 1.2075\n",
      "Epoch 7 Batch 850: Loss = 1.2238\n",
      "Epoch 7 Batch 860: Loss = 1.2872\n",
      "Epoch 7 Batch 870: Loss = 1.3229\n",
      "Epoch 7 Batch 880: Loss = 1.3057\n",
      "Epoch 7 Batch 890: Loss = 1.3285\n",
      "Epoch 7 Batch 900: Loss = 1.1711\n",
      "Epoch 7 Batch 910: Loss = 1.3221\n",
      "Epoch 7 Batch 920: Loss = 1.1733\n",
      "Epoch 7 Batch 930: Loss = 1.2442\n",
      "Epoch 7 Batch 940: Loss = 1.2463\n",
      "Epoch 7 Batch 950: Loss = 1.2540\n",
      "Epoch 7 Batch 960: Loss = 1.1848\n",
      "Epoch 7 Batch 970: Loss = 1.2317\n",
      "Epoch 7 Batch 980: Loss = 1.2237\n",
      "Epoch 7 Batch 990: Loss = 1.2439\n",
      "Epoch 7 Batch 1000: Loss = 1.0839\n",
      "Epoch 7 Batch 1010: Loss = 1.2710\n",
      "Epoch 7 Batch 1020: Loss = 1.0684\n",
      "Epoch 7 Batch 1030: Loss = 1.0834\n",
      "Epoch 7 Batch 1040: Loss = 1.2507\n",
      "Epoch 7 Batch 1050: Loss = 1.4077\n",
      "Epoch 7 Batch 1060: Loss = 1.1853\n",
      "Epoch 7 Batch 1070: Loss = 1.4063\n",
      "Epoch 7 Batch 1080: Loss = 1.3359\n",
      "Epoch 7 Batch 1090: Loss = 1.3780\n",
      "Epoch 7 Batch 1100: Loss = 1.1550\n",
      "Epoch 7 Batch 1110: Loss = 1.1321\n",
      "Epoch 7 Batch 1120: Loss = 1.1959\n",
      "Epoch 7 Batch 1130: Loss = 1.3516\n",
      "Epoch 7 Batch 1140: Loss = 1.2110\n",
      "Epoch 7 Batch 1150: Loss = 1.4463\n",
      "Epoch 7 Batch 1160: Loss = 1.2191\n",
      "Epoch 7 Batch 1170: Loss = 1.3071\n",
      "Epoch 7 Batch 1180: Loss = 1.2937\n",
      "Epoch 7 Batch 1190: Loss = 1.2611\n",
      "Epoch 7 Batch 1200: Loss = 1.5301\n",
      "Epoch 7 Batch 1210: Loss = 1.2121\n",
      "Epoch 7 Batch 1220: Loss = 1.1058\n",
      "Epoch 7 Batch 1230: Loss = 1.2325\n",
      "Epoch 7 Batch 1240: Loss = 1.0460\n",
      "Epoch 7 Batch 1250: Loss = 1.2231\n",
      "Epoch 7 Batch 1260: Loss = 1.2491\n",
      "Epoch 7 Batch 1270: Loss = 1.0786\n",
      "Epoch 7 Batch 1280: Loss = 1.1832\n",
      "Epoch 7 Batch 1290: Loss = 1.2777\n",
      "Epoch 7 Batch 1300: Loss = 1.3149\n",
      "Epoch 7 Batch 1310: Loss = 1.4194\n",
      "Epoch 7 Batch 1320: Loss = 1.2473\n",
      "Epoch 7 Batch 1330: Loss = 1.2329\n",
      "Epoch 7 Batch 1340: Loss = 1.1043\n",
      "Epoch 7 Batch 1350: Loss = 1.1772\n",
      "Epoch 7 Batch 1360: Loss = 1.3494\n",
      "Epoch 7 Batch 1370: Loss = 1.0765\n",
      "Epoch 7 Batch 1380: Loss = 1.3866\n",
      "Epoch 7 Batch 1390: Loss = 1.1399\n",
      "Epoch 7 Batch 1400: Loss = 1.2355\n",
      "Epoch 7 Batch 1410: Loss = 1.0594\n",
      "Epoch 7 Batch 1420: Loss = 1.0355\n",
      "Epoch 7 Batch 1430: Loss = 1.3445\n",
      "Epoch 7 Batch 1440: Loss = 1.1865\n",
      "Epoch 7 Batch 1450: Loss = 1.2572\n",
      "Epoch 7 Batch 1460: Loss = 1.0736\n",
      "Epoch 7 Batch 1470: Loss = 1.1109\n",
      "Epoch 7 Batch 1480: Loss = 1.2911\n",
      "Epoch 7 Batch 1490: Loss = 1.1891\n",
      "Epoch 7 Batch 1500: Loss = 1.2454\n",
      "Epoch 7 Batch 1510: Loss = 1.1877\n",
      "Epoch 7 Batch 1520: Loss = 1.2884\n",
      "Epoch 7 Batch 1530: Loss = 1.4228\n",
      "Epoch 7 Batch 1540: Loss = 1.1176\n",
      "Epoch 7 Batch 1550: Loss = 1.2332\n",
      "Epoch 7 Batch 1560: Loss = 1.2510\n",
      "Epoch 7 Batch 1570: Loss = 1.2870\n",
      "Epoch 7 Batch 1580: Loss = 1.0875\n",
      "Epoch 7 Batch 1590: Loss = 1.3293\n",
      "Epoch 7 Batch 1600: Loss = 1.3879\n",
      "Epoch 7 Batch 1610: Loss = 1.0678\n",
      "Epoch 7 Batch 1620: Loss = 1.1010\n",
      "Epoch 7 Batch 1630: Loss = 1.1302\n",
      "Epoch 7 Batch 1640: Loss = 1.0641\n",
      "Epoch 7 Batch 1650: Loss = 1.3536\n",
      "Epoch 7 Batch 1660: Loss = 1.2859\n",
      "Epoch 7 Batch 1670: Loss = 1.1794\n",
      "Epoch 7 Batch 1680: Loss = 1.0471\n",
      "Epoch 7 Batch 1690: Loss = 1.0751\n",
      "Epoch 7 Batch 1700: Loss = 1.2482\n",
      "Epoch 7 Batch 1710: Loss = 1.0959\n",
      "Epoch 7 Batch 1720: Loss = 1.0686\n",
      "Epoch 7 Batch 1730: Loss = 1.1959\n",
      "Epoch 7 Batch 1740: Loss = 1.1701\n",
      "Epoch 7 Batch 1750: Loss = 1.0662\n",
      "Epoch 7 Batch 1760: Loss = 1.0835\n",
      "Epoch 7 Batch 1770: Loss = 1.0318\n",
      "Epoch 7 Batch 1780: Loss = 1.1848\n",
      "Epoch 7 Batch 1790: Loss = 1.2113\n",
      "Epoch 7 Batch 1800: Loss = 1.1879\n",
      "Epoch 7 Batch 1810: Loss = 1.4095\n",
      "Epoch 7 Batch 1820: Loss = 1.0259\n",
      "Epoch 7 Batch 1830: Loss = 1.1836\n",
      "Epoch 7 Batch 1840: Loss = 1.2849\n",
      "Epoch 7 Batch 1850: Loss = 1.1766\n",
      "Epoch 7 Batch 1860: Loss = 1.0724\n",
      "Epoch 7 Batch 1870: Loss = 1.2409\n",
      "Epoch 7 Batch 1880: Loss = 1.4931\n",
      "Epoch 7 Batch 1890: Loss = 1.2294\n",
      "Epoch 7 Batch 1900: Loss = 1.2318\n",
      "Epoch 7 Batch 1910: Loss = 1.2920\n",
      "Epoch 7 Batch 1920: Loss = 1.4271\n",
      "Epoch 7 Batch 1930: Loss = 1.1450\n",
      "Epoch 7 Batch 1940: Loss = 1.3720\n",
      "Epoch 7 Batch 1950: Loss = 1.1114\n",
      "Epoch 7 Batch 1960: Loss = 1.1237\n",
      "Epoch 7 Batch 1970: Loss = 1.1255\n",
      "Epoch 7 Batch 1980: Loss = 1.2172\n",
      "Epoch 7 Batch 1990: Loss = 1.1381\n",
      "Epoch 7 Batch 2000: Loss = 1.1280\n",
      "Epoch 7 Average Loss: 1.2279\n",
      "Epoch 8 Batch 0: Loss = 1.2198\n",
      "Epoch 8 Batch 10: Loss = 1.0525\n",
      "Epoch 8 Batch 20: Loss = 1.0644\n",
      "Epoch 8 Batch 30: Loss = 1.1197\n",
      "Epoch 8 Batch 40: Loss = 1.1713\n",
      "Epoch 8 Batch 50: Loss = 1.1616\n",
      "Epoch 8 Batch 60: Loss = 1.2383\n",
      "Epoch 8 Batch 70: Loss = 1.0842\n",
      "Epoch 8 Batch 80: Loss = 1.2861\n",
      "Epoch 8 Batch 90: Loss = 1.2504\n",
      "Epoch 8 Batch 100: Loss = 1.1378\n",
      "Epoch 8 Batch 110: Loss = 1.3017\n",
      "Epoch 8 Batch 120: Loss = 1.2730\n",
      "Epoch 8 Batch 130: Loss = 1.1633\n",
      "Epoch 8 Batch 140: Loss = 1.1375\n",
      "Epoch 8 Batch 150: Loss = 1.2992\n",
      "Epoch 8 Batch 160: Loss = 1.0951\n",
      "Epoch 8 Batch 170: Loss = 1.2049\n",
      "Epoch 8 Batch 180: Loss = 1.0455\n",
      "Epoch 8 Batch 190: Loss = 1.2158\n",
      "Epoch 8 Batch 200: Loss = 1.1024\n",
      "Epoch 8 Batch 210: Loss = 1.1095\n",
      "Epoch 8 Batch 220: Loss = 1.2132\n",
      "Epoch 8 Batch 230: Loss = 1.1154\n",
      "Epoch 8 Batch 240: Loss = 1.5039\n",
      "Epoch 8 Batch 250: Loss = 1.0606\n",
      "Epoch 8 Batch 260: Loss = 1.1180\n",
      "Epoch 8 Batch 270: Loss = 0.9888\n",
      "Epoch 8 Batch 280: Loss = 1.1822\n",
      "Epoch 8 Batch 290: Loss = 1.1264\n",
      "Epoch 8 Batch 300: Loss = 1.2573\n",
      "Epoch 8 Batch 310: Loss = 1.2311\n",
      "Epoch 8 Batch 320: Loss = 1.1434\n",
      "Epoch 8 Batch 330: Loss = 1.2255\n",
      "Epoch 8 Batch 340: Loss = 1.0202\n",
      "Epoch 8 Batch 350: Loss = 1.1885\n",
      "Epoch 8 Batch 360: Loss = 1.1279\n",
      "Epoch 8 Batch 370: Loss = 1.1889\n",
      "Epoch 8 Batch 380: Loss = 1.1131\n",
      "Epoch 8 Batch 390: Loss = 0.9831\n",
      "Epoch 8 Batch 400: Loss = 1.3231\n",
      "Epoch 8 Batch 410: Loss = 1.1067\n",
      "Epoch 8 Batch 420: Loss = 1.1178\n",
      "Epoch 8 Batch 430: Loss = 1.0805\n",
      "Epoch 8 Batch 440: Loss = 1.2449\n",
      "Epoch 8 Batch 450: Loss = 1.1026\n",
      "Epoch 8 Batch 460: Loss = 1.2904\n",
      "Epoch 8 Batch 470: Loss = 1.3532\n",
      "Epoch 8 Batch 480: Loss = 1.1783\n",
      "Epoch 8 Batch 490: Loss = 1.2273\n",
      "Epoch 8 Batch 500: Loss = 1.1488\n",
      "Epoch 8 Batch 510: Loss = 0.9619\n",
      "Epoch 8 Batch 520: Loss = 1.1014\n",
      "Epoch 8 Batch 530: Loss = 0.9795\n",
      "Epoch 8 Batch 540: Loss = 1.1873\n",
      "Epoch 8 Batch 550: Loss = 1.2044\n",
      "Epoch 8 Batch 560: Loss = 1.2488\n",
      "Epoch 8 Batch 570: Loss = 1.2142\n",
      "Epoch 8 Batch 580: Loss = 1.0509\n",
      "Epoch 8 Batch 590: Loss = 1.1717\n",
      "Epoch 8 Batch 600: Loss = 1.1382\n",
      "Epoch 8 Batch 610: Loss = 1.2471\n",
      "Epoch 8 Batch 620: Loss = 1.1616\n",
      "Epoch 8 Batch 630: Loss = 1.0275\n",
      "Epoch 8 Batch 640: Loss = 1.2050\n",
      "Epoch 8 Batch 650: Loss = 1.2454\n",
      "Epoch 8 Batch 660: Loss = 1.2319\n",
      "Epoch 8 Batch 670: Loss = 1.0087\n",
      "Epoch 8 Batch 680: Loss = 1.1151\n",
      "Epoch 8 Batch 690: Loss = 1.1129\n",
      "Epoch 8 Batch 700: Loss = 1.2799\n",
      "Epoch 8 Batch 710: Loss = 1.3498\n",
      "Epoch 8 Batch 720: Loss = 1.1934\n",
      "Epoch 8 Batch 730: Loss = 0.9560\n",
      "Epoch 8 Batch 740: Loss = 1.2613\n",
      "Epoch 8 Batch 750: Loss = 1.0582\n",
      "Epoch 8 Batch 760: Loss = 1.3239\n",
      "Epoch 8 Batch 770: Loss = 1.2236\n",
      "Epoch 8 Batch 780: Loss = 1.1187\n",
      "Epoch 8 Batch 790: Loss = 1.0802\n",
      "Epoch 8 Batch 800: Loss = 1.0611\n",
      "Epoch 8 Batch 810: Loss = 1.1842\n",
      "Epoch 8 Batch 820: Loss = 1.3030\n",
      "Epoch 8 Batch 830: Loss = 1.1183\n",
      "Epoch 8 Batch 840: Loss = 1.1739\n",
      "Epoch 8 Batch 850: Loss = 1.2828\n",
      "Epoch 8 Batch 860: Loss = 1.2030\n",
      "Epoch 8 Batch 870: Loss = 1.0652\n",
      "Epoch 8 Batch 880: Loss = 1.1911\n",
      "Epoch 8 Batch 890: Loss = 1.0798\n",
      "Epoch 8 Batch 900: Loss = 1.4226\n",
      "Epoch 8 Batch 910: Loss = 1.4208\n",
      "Epoch 8 Batch 920: Loss = 1.0224\n",
      "Epoch 8 Batch 930: Loss = 1.1836\n",
      "Epoch 8 Batch 940: Loss = 1.1072\n",
      "Epoch 8 Batch 950: Loss = 1.2566\n",
      "Epoch 8 Batch 960: Loss = 1.1800\n",
      "Epoch 8 Batch 970: Loss = 1.0794\n",
      "Epoch 8 Batch 980: Loss = 1.0081\n",
      "Epoch 8 Batch 990: Loss = 0.8944\n",
      "Epoch 8 Batch 1000: Loss = 1.1366\n",
      "Epoch 8 Batch 1010: Loss = 1.2508\n",
      "Epoch 8 Batch 1020: Loss = 1.0831\n",
      "Epoch 8 Batch 1030: Loss = 1.1431\n",
      "Epoch 8 Batch 1040: Loss = 1.0958\n",
      "Epoch 8 Batch 1050: Loss = 1.0991\n",
      "Epoch 8 Batch 1060: Loss = 1.3388\n",
      "Epoch 8 Batch 1070: Loss = 1.1892\n",
      "Epoch 8 Batch 1080: Loss = 1.0512\n",
      "Epoch 8 Batch 1090: Loss = 1.0115\n",
      "Epoch 8 Batch 1100: Loss = 1.2584\n",
      "Epoch 8 Batch 1110: Loss = 1.2269\n",
      "Epoch 8 Batch 1120: Loss = 1.3056\n",
      "Epoch 8 Batch 1130: Loss = 1.0788\n",
      "Epoch 8 Batch 1140: Loss = 1.1156\n",
      "Epoch 8 Batch 1150: Loss = 1.0190\n",
      "Epoch 8 Batch 1160: Loss = 1.0512\n",
      "Epoch 8 Batch 1170: Loss = 1.0253\n",
      "Epoch 8 Batch 1180: Loss = 1.2279\n",
      "Epoch 8 Batch 1190: Loss = 1.1887\n",
      "Epoch 8 Batch 1200: Loss = 1.2254\n",
      "Epoch 8 Batch 1210: Loss = 1.2081\n",
      "Epoch 8 Batch 1220: Loss = 1.2165\n",
      "Epoch 8 Batch 1230: Loss = 1.0639\n",
      "Epoch 8 Batch 1240: Loss = 1.1693\n",
      "Epoch 8 Batch 1250: Loss = 1.1985\n",
      "Epoch 8 Batch 1260: Loss = 1.0986\n",
      "Epoch 8 Batch 1270: Loss = 1.2456\n",
      "Epoch 8 Batch 1280: Loss = 1.1528\n",
      "Epoch 8 Batch 1290: Loss = 1.2423\n",
      "Epoch 8 Batch 1300: Loss = 0.9970\n",
      "Epoch 8 Batch 1310: Loss = 0.9879\n",
      "Epoch 8 Batch 1320: Loss = 1.0464\n",
      "Epoch 8 Batch 1330: Loss = 1.1341\n",
      "Epoch 8 Batch 1340: Loss = 1.1718\n",
      "Epoch 8 Batch 1350: Loss = 1.2203\n",
      "Epoch 8 Batch 1360: Loss = 1.2260\n",
      "Epoch 8 Batch 1370: Loss = 1.0805\n",
      "Epoch 8 Batch 1380: Loss = 1.2248\n",
      "Epoch 8 Batch 1390: Loss = 1.2506\n",
      "Epoch 8 Batch 1400: Loss = 1.1685\n",
      "Epoch 8 Batch 1410: Loss = 1.0928\n",
      "Epoch 8 Batch 1420: Loss = 1.1646\n",
      "Epoch 8 Batch 1430: Loss = 1.1243\n",
      "Epoch 8 Batch 1440: Loss = 1.0274\n",
      "Epoch 8 Batch 1450: Loss = 1.0565\n",
      "Epoch 8 Batch 1460: Loss = 1.1246\n",
      "Epoch 8 Batch 1470: Loss = 1.0771\n",
      "Epoch 8 Batch 1480: Loss = 1.0538\n",
      "Epoch 8 Batch 1490: Loss = 1.1700\n",
      "Epoch 8 Batch 1500: Loss = 1.1798\n",
      "Epoch 8 Batch 1510: Loss = 1.1922\n",
      "Epoch 8 Batch 1520: Loss = 1.1930\n",
      "Epoch 8 Batch 1530: Loss = 1.2603\n",
      "Epoch 8 Batch 1540: Loss = 1.1171\n",
      "Epoch 8 Batch 1550: Loss = 1.2962\n",
      "Epoch 8 Batch 1560: Loss = 1.0914\n",
      "Epoch 8 Batch 1570: Loss = 1.3737\n",
      "Epoch 8 Batch 1580: Loss = 1.1177\n",
      "Epoch 8 Batch 1590: Loss = 1.0744\n",
      "Epoch 8 Batch 1600: Loss = 1.3312\n",
      "Epoch 8 Batch 1610: Loss = 1.2052\n",
      "Epoch 8 Batch 1620: Loss = 1.1088\n",
      "Epoch 8 Batch 1630: Loss = 1.3104\n",
      "Epoch 8 Batch 1640: Loss = 1.1040\n",
      "Epoch 8 Batch 1650: Loss = 1.1319\n",
      "Epoch 8 Batch 1660: Loss = 1.2114\n",
      "Epoch 8 Batch 1670: Loss = 1.1921\n",
      "Epoch 8 Batch 1680: Loss = 0.9809\n",
      "Epoch 8 Batch 1690: Loss = 1.0584\n",
      "Epoch 8 Batch 1700: Loss = 1.2829\n",
      "Epoch 8 Batch 1710: Loss = 0.9930\n",
      "Epoch 8 Batch 1720: Loss = 1.3163\n",
      "Epoch 8 Batch 1730: Loss = 1.1038\n",
      "Epoch 8 Batch 1740: Loss = 1.2274\n",
      "Epoch 8 Batch 1750: Loss = 1.2862\n",
      "Epoch 8 Batch 1760: Loss = 1.2097\n",
      "Epoch 8 Batch 1770: Loss = 1.3942\n",
      "Epoch 8 Batch 1780: Loss = 1.1538\n",
      "Epoch 8 Batch 1790: Loss = 1.0745\n",
      "Epoch 8 Batch 1800: Loss = 1.1698\n",
      "Epoch 8 Batch 1810: Loss = 1.1715\n",
      "Epoch 8 Batch 1820: Loss = 1.2072\n",
      "Epoch 8 Batch 1830: Loss = 1.1171\n",
      "Epoch 8 Batch 1840: Loss = 1.1064\n",
      "Epoch 8 Batch 1850: Loss = 1.1209\n",
      "Epoch 8 Batch 1860: Loss = 1.3044\n",
      "Epoch 8 Batch 1870: Loss = 1.2095\n",
      "Epoch 8 Batch 1880: Loss = 1.1329\n",
      "Epoch 8 Batch 1890: Loss = 1.1267\n",
      "Epoch 8 Batch 1900: Loss = 1.3713\n",
      "Epoch 8 Batch 1910: Loss = 1.0820\n",
      "Epoch 8 Batch 1920: Loss = 1.1876\n",
      "Epoch 8 Batch 1930: Loss = 1.1806\n",
      "Epoch 8 Batch 1940: Loss = 1.2271\n",
      "Epoch 8 Batch 1950: Loss = 1.2504\n",
      "Epoch 8 Batch 1960: Loss = 1.2728\n",
      "Epoch 8 Batch 1970: Loss = 1.1002\n",
      "Epoch 8 Batch 1980: Loss = 1.0627\n",
      "Epoch 8 Batch 1990: Loss = 1.1756\n",
      "Epoch 8 Batch 2000: Loss = 1.2421\n",
      "Epoch 8 Average Loss: 1.1657\n",
      "Epoch 9 Batch 0: Loss = 1.1827\n",
      "Epoch 9 Batch 10: Loss = 0.9883\n",
      "Epoch 9 Batch 20: Loss = 1.0384\n",
      "Epoch 9 Batch 30: Loss = 1.2681\n",
      "Epoch 9 Batch 40: Loss = 1.0695\n",
      "Epoch 9 Batch 50: Loss = 1.1175\n",
      "Epoch 9 Batch 60: Loss = 1.1396\n",
      "Epoch 9 Batch 70: Loss = 1.1502\n",
      "Epoch 9 Batch 80: Loss = 1.2722\n",
      "Epoch 9 Batch 90: Loss = 1.0631\n",
      "Epoch 9 Batch 100: Loss = 1.2111\n",
      "Epoch 9 Batch 110: Loss = 1.0916\n",
      "Epoch 9 Batch 120: Loss = 1.2654\n",
      "Epoch 9 Batch 130: Loss = 1.1746\n",
      "Epoch 9 Batch 140: Loss = 1.2980\n",
      "Epoch 9 Batch 150: Loss = 1.1053\n",
      "Epoch 9 Batch 160: Loss = 1.2395\n",
      "Epoch 9 Batch 170: Loss = 1.2355\n",
      "Epoch 9 Batch 180: Loss = 1.1860\n",
      "Epoch 9 Batch 190: Loss = 1.0093\n",
      "Epoch 9 Batch 200: Loss = 1.2140\n",
      "Epoch 9 Batch 210: Loss = 1.0009\n",
      "Epoch 9 Batch 220: Loss = 1.0784\n",
      "Epoch 9 Batch 230: Loss = 1.2425\n",
      "Epoch 9 Batch 240: Loss = 1.1477\n",
      "Epoch 9 Batch 250: Loss = 1.2130\n",
      "Epoch 9 Batch 260: Loss = 1.0540\n",
      "Epoch 9 Batch 270: Loss = 1.1045\n",
      "Epoch 9 Batch 280: Loss = 1.0587\n",
      "Epoch 9 Batch 290: Loss = 0.9888\n",
      "Epoch 9 Batch 300: Loss = 1.2886\n",
      "Epoch 9 Batch 310: Loss = 1.1261\n",
      "Epoch 9 Batch 320: Loss = 1.2854\n",
      "Epoch 9 Batch 330: Loss = 1.2925\n",
      "Epoch 9 Batch 340: Loss = 1.1518\n",
      "Epoch 9 Batch 350: Loss = 1.0692\n",
      "Epoch 9 Batch 360: Loss = 1.1270\n",
      "Epoch 9 Batch 370: Loss = 0.9713\n",
      "Epoch 9 Batch 380: Loss = 1.2378\n",
      "Epoch 9 Batch 390: Loss = 1.0293\n",
      "Epoch 9 Batch 400: Loss = 1.2335\n",
      "Epoch 9 Batch 410: Loss = 1.1162\n",
      "Epoch 9 Batch 420: Loss = 1.1958\n",
      "Epoch 9 Batch 430: Loss = 0.9932\n",
      "Epoch 9 Batch 440: Loss = 1.0502\n",
      "Epoch 9 Batch 450: Loss = 1.2678\n",
      "Epoch 9 Batch 460: Loss = 1.1712\n",
      "Epoch 9 Batch 470: Loss = 1.1534\n",
      "Epoch 9 Batch 480: Loss = 1.0155\n",
      "Epoch 9 Batch 490: Loss = 1.2303\n",
      "Epoch 9 Batch 500: Loss = 1.2073\n",
      "Epoch 9 Batch 510: Loss = 0.9804\n",
      "Epoch 9 Batch 520: Loss = 1.1540\n",
      "Epoch 9 Batch 530: Loss = 1.1245\n",
      "Epoch 9 Batch 540: Loss = 1.1396\n",
      "Epoch 9 Batch 550: Loss = 1.1718\n",
      "Epoch 9 Batch 560: Loss = 1.2615\n",
      "Epoch 9 Batch 570: Loss = 1.0896\n",
      "Epoch 9 Batch 580: Loss = 1.0350\n",
      "Epoch 9 Batch 590: Loss = 1.1275\n",
      "Epoch 9 Batch 600: Loss = 1.2939\n",
      "Epoch 9 Batch 610: Loss = 1.1600\n",
      "Epoch 9 Batch 620: Loss = 1.1695\n",
      "Epoch 9 Batch 630: Loss = 1.1726\n",
      "Epoch 9 Batch 640: Loss = 1.2166\n",
      "Epoch 9 Batch 650: Loss = 0.9430\n",
      "Epoch 9 Batch 660: Loss = 1.0433\n",
      "Epoch 9 Batch 670: Loss = 1.0326\n",
      "Epoch 9 Batch 680: Loss = 0.9896\n",
      "Epoch 9 Batch 690: Loss = 1.0256\n",
      "Epoch 9 Batch 700: Loss = 1.0586\n",
      "Epoch 9 Batch 710: Loss = 1.1001\n",
      "Epoch 9 Batch 720: Loss = 1.2106\n",
      "Epoch 9 Batch 730: Loss = 1.1633\n",
      "Epoch 9 Batch 740: Loss = 1.1718\n",
      "Epoch 9 Batch 750: Loss = 1.2605\n",
      "Epoch 9 Batch 760: Loss = 1.0210\n",
      "Epoch 9 Batch 770: Loss = 1.0984\n",
      "Epoch 9 Batch 780: Loss = 1.1804\n",
      "Epoch 9 Batch 790: Loss = 1.1662\n",
      "Epoch 9 Batch 800: Loss = 1.1961\n",
      "Epoch 9 Batch 810: Loss = 1.1994\n",
      "Epoch 9 Batch 820: Loss = 0.9481\n",
      "Epoch 9 Batch 830: Loss = 1.2462\n",
      "Epoch 9 Batch 840: Loss = 1.0753\n",
      "Epoch 9 Batch 850: Loss = 1.0111\n",
      "Epoch 9 Batch 860: Loss = 1.1761\n",
      "Epoch 9 Batch 870: Loss = 1.0947\n",
      "Epoch 9 Batch 880: Loss = 1.2061\n",
      "Epoch 9 Batch 890: Loss = 1.1379\n",
      "Epoch 9 Batch 900: Loss = 1.0483\n",
      "Epoch 9 Batch 910: Loss = 1.1892\n",
      "Epoch 9 Batch 920: Loss = 0.9756\n",
      "Epoch 9 Batch 930: Loss = 0.9902\n",
      "Epoch 9 Batch 940: Loss = 1.0816\n",
      "Epoch 9 Batch 950: Loss = 1.2687\n",
      "Epoch 9 Batch 960: Loss = 1.1343\n",
      "Epoch 9 Batch 970: Loss = 1.2159\n",
      "Epoch 9 Batch 980: Loss = 1.1451\n",
      "Epoch 9 Batch 990: Loss = 1.1504\n",
      "Epoch 9 Batch 1000: Loss = 1.0570\n",
      "Epoch 9 Batch 1010: Loss = 1.0618\n",
      "Epoch 9 Batch 1020: Loss = 1.1774\n",
      "Epoch 9 Batch 1030: Loss = 1.1360\n",
      "Epoch 9 Batch 1040: Loss = 1.1449\n",
      "Epoch 9 Batch 1050: Loss = 1.1582\n",
      "Epoch 9 Batch 1060: Loss = 1.1949\n",
      "Epoch 9 Batch 1070: Loss = 1.1216\n",
      "Epoch 9 Batch 1080: Loss = 1.1269\n",
      "Epoch 9 Batch 1090: Loss = 1.0326\n",
      "Epoch 9 Batch 1100: Loss = 1.1553\n",
      "Epoch 9 Batch 1110: Loss = 1.0187\n",
      "Epoch 9 Batch 1120: Loss = 1.1812\n",
      "Epoch 9 Batch 1130: Loss = 1.2930\n",
      "Epoch 9 Batch 1140: Loss = 1.1087\n",
      "Epoch 9 Batch 1150: Loss = 1.2905\n",
      "Epoch 9 Batch 1160: Loss = 1.1992\n",
      "Epoch 9 Batch 1170: Loss = 1.2766\n",
      "Epoch 9 Batch 1180: Loss = 1.1592\n",
      "Epoch 9 Batch 1190: Loss = 1.1960\n",
      "Epoch 9 Batch 1200: Loss = 1.0690\n",
      "Epoch 9 Batch 1210: Loss = 1.0024\n",
      "Epoch 9 Batch 1220: Loss = 1.1479\n",
      "Epoch 9 Batch 1230: Loss = 1.2851\n",
      "Epoch 9 Batch 1240: Loss = 1.0377\n",
      "Epoch 9 Batch 1250: Loss = 1.2318\n",
      "Epoch 9 Batch 1260: Loss = 1.1291\n",
      "Epoch 9 Batch 1270: Loss = 1.0994\n",
      "Epoch 9 Batch 1280: Loss = 1.2018\n",
      "Epoch 9 Batch 1290: Loss = 0.9344\n",
      "Epoch 9 Batch 1300: Loss = 1.0182\n",
      "Epoch 9 Batch 1310: Loss = 0.9644\n",
      "Epoch 9 Batch 1320: Loss = 1.1081\n",
      "Epoch 9 Batch 1330: Loss = 1.0643\n",
      "Epoch 9 Batch 1340: Loss = 1.0568\n",
      "Epoch 9 Batch 1350: Loss = 1.2391\n",
      "Epoch 9 Batch 1360: Loss = 1.1408\n",
      "Epoch 9 Batch 1370: Loss = 1.1670\n",
      "Epoch 9 Batch 1380: Loss = 0.9891\n",
      "Epoch 9 Batch 1390: Loss = 1.2359\n",
      "Epoch 9 Batch 1400: Loss = 1.0638\n",
      "Epoch 9 Batch 1410: Loss = 1.3138\n",
      "Epoch 9 Batch 1420: Loss = 1.1747\n",
      "Epoch 9 Batch 1430: Loss = 1.1870\n",
      "Epoch 9 Batch 1440: Loss = 1.1566\n",
      "Epoch 9 Batch 1450: Loss = 1.0752\n",
      "Epoch 9 Batch 1460: Loss = 1.1369\n",
      "Epoch 9 Batch 1470: Loss = 1.1841\n",
      "Epoch 9 Batch 1480: Loss = 1.1793\n",
      "Epoch 9 Batch 1490: Loss = 1.1075\n",
      "Epoch 9 Batch 1500: Loss = 1.0317\n",
      "Epoch 9 Batch 1510: Loss = 1.1512\n",
      "Epoch 9 Batch 1520: Loss = 1.1954\n",
      "Epoch 9 Batch 1530: Loss = 1.1275\n",
      "Epoch 9 Batch 1540: Loss = 1.2765\n",
      "Epoch 9 Batch 1550: Loss = 1.1504\n",
      "Epoch 9 Batch 1560: Loss = 0.9631\n",
      "Epoch 9 Batch 1570: Loss = 1.0974\n",
      "Epoch 9 Batch 1580: Loss = 1.0883\n",
      "Epoch 9 Batch 1590: Loss = 1.1906\n",
      "Epoch 9 Batch 1600: Loss = 1.2171\n",
      "Epoch 9 Batch 1610: Loss = 1.0848\n",
      "Epoch 9 Batch 1620: Loss = 1.1087\n",
      "Epoch 9 Batch 1630: Loss = 1.3281\n",
      "Epoch 9 Batch 1640: Loss = 1.1401\n",
      "Epoch 9 Batch 1650: Loss = 1.3090\n",
      "Epoch 9 Batch 1660: Loss = 1.0454\n",
      "Epoch 9 Batch 1670: Loss = 1.1221\n",
      "Epoch 9 Batch 1680: Loss = 1.0606\n",
      "Epoch 9 Batch 1690: Loss = 1.2090\n",
      "Epoch 9 Batch 1700: Loss = 0.9410\n",
      "Epoch 9 Batch 1710: Loss = 1.2868\n",
      "Epoch 9 Batch 1720: Loss = 0.9347\n",
      "Epoch 9 Batch 1730: Loss = 1.0387\n",
      "Epoch 9 Batch 1740: Loss = 0.9769\n",
      "Epoch 9 Batch 1750: Loss = 1.2161\n",
      "Epoch 9 Batch 1760: Loss = 1.2143\n",
      "Epoch 9 Batch 1770: Loss = 1.1041\n",
      "Epoch 9 Batch 1780: Loss = 1.2616\n",
      "Epoch 9 Batch 1790: Loss = 1.2380\n",
      "Epoch 9 Batch 1800: Loss = 1.0765\n",
      "Epoch 9 Batch 1810: Loss = 1.0584\n",
      "Epoch 9 Batch 1820: Loss = 0.9725\n",
      "Epoch 9 Batch 1830: Loss = 1.0489\n",
      "Epoch 9 Batch 1840: Loss = 1.0047\n",
      "Epoch 9 Batch 1850: Loss = 1.0334\n",
      "Epoch 9 Batch 1860: Loss = 1.1120\n",
      "Epoch 9 Batch 1870: Loss = 1.0513\n",
      "Epoch 9 Batch 1880: Loss = 1.2298\n",
      "Epoch 9 Batch 1890: Loss = 1.0765\n",
      "Epoch 9 Batch 1900: Loss = 1.2649\n",
      "Epoch 9 Batch 1910: Loss = 1.2579\n",
      "Epoch 9 Batch 1920: Loss = 1.0291\n",
      "Epoch 9 Batch 1930: Loss = 0.9602\n",
      "Epoch 9 Batch 1940: Loss = 1.0452\n",
      "Epoch 9 Batch 1950: Loss = 1.1976\n",
      "Epoch 9 Batch 1960: Loss = 1.1136\n",
      "Epoch 9 Batch 1970: Loss = 1.1743\n",
      "Epoch 9 Batch 1980: Loss = 1.1571\n",
      "Epoch 9 Batch 1990: Loss = 1.1599\n",
      "Epoch 9 Batch 2000: Loss = 1.1948\n",
      "Epoch 9 Average Loss: 1.1286\n",
      "Epoch 10 Batch 0: Loss = 1.1152\n",
      "Epoch 10 Batch 10: Loss = 1.1749\n",
      "Epoch 10 Batch 20: Loss = 1.1218\n",
      "Epoch 10 Batch 30: Loss = 1.1457\n",
      "Epoch 10 Batch 40: Loss = 1.0018\n",
      "Epoch 10 Batch 50: Loss = 1.1529\n",
      "Epoch 10 Batch 60: Loss = 0.9960\n",
      "Epoch 10 Batch 70: Loss = 1.1333\n",
      "Epoch 10 Batch 80: Loss = 1.0426\n",
      "Epoch 10 Batch 90: Loss = 1.0483\n",
      "Epoch 10 Batch 100: Loss = 1.0930\n",
      "Epoch 10 Batch 110: Loss = 1.1112\n",
      "Epoch 10 Batch 120: Loss = 1.0464\n",
      "Epoch 10 Batch 130: Loss = 1.1295\n",
      "Epoch 10 Batch 140: Loss = 1.1108\n",
      "Epoch 10 Batch 150: Loss = 1.2441\n",
      "Epoch 10 Batch 160: Loss = 1.1087\n",
      "Epoch 10 Batch 170: Loss = 1.1209\n",
      "Epoch 10 Batch 180: Loss = 1.3003\n",
      "Epoch 10 Batch 190: Loss = 1.0162\n",
      "Epoch 10 Batch 200: Loss = 1.0602\n",
      "Epoch 10 Batch 210: Loss = 1.2490\n",
      "Epoch 10 Batch 220: Loss = 1.0086\n",
      "Epoch 10 Batch 230: Loss = 1.1030\n",
      "Epoch 10 Batch 240: Loss = 1.1404\n",
      "Epoch 10 Batch 250: Loss = 1.2538\n",
      "Epoch 10 Batch 260: Loss = 1.0794\n",
      "Epoch 10 Batch 270: Loss = 0.9407\n",
      "Epoch 10 Batch 280: Loss = 1.0492\n",
      "Epoch 10 Batch 290: Loss = 1.0307\n",
      "Epoch 10 Batch 300: Loss = 1.1696\n",
      "Epoch 10 Batch 310: Loss = 1.0862\n",
      "Epoch 10 Batch 320: Loss = 1.0309\n",
      "Epoch 10 Batch 330: Loss = 0.9262\n",
      "Epoch 10 Batch 340: Loss = 1.1639\n",
      "Epoch 10 Batch 350: Loss = 0.9886\n",
      "Epoch 10 Batch 360: Loss = 1.0994\n",
      "Epoch 10 Batch 370: Loss = 1.1079\n",
      "Epoch 10 Batch 380: Loss = 1.1319\n",
      "Epoch 10 Batch 390: Loss = 1.0065\n",
      "Epoch 10 Batch 400: Loss = 1.0692\n",
      "Epoch 10 Batch 410: Loss = 1.0852\n",
      "Epoch 10 Batch 420: Loss = 1.0700\n",
      "Epoch 10 Batch 430: Loss = 1.3036\n",
      "Epoch 10 Batch 440: Loss = 1.2195\n",
      "Epoch 10 Batch 450: Loss = 1.3089\n",
      "Epoch 10 Batch 460: Loss = 0.8721\n",
      "Epoch 10 Batch 470: Loss = 1.2465\n",
      "Epoch 10 Batch 480: Loss = 1.2560\n",
      "Epoch 10 Batch 490: Loss = 0.8698\n",
      "Epoch 10 Batch 500: Loss = 1.0657\n",
      "Epoch 10 Batch 510: Loss = 1.1122\n",
      "Epoch 10 Batch 520: Loss = 1.0341\n",
      "Epoch 10 Batch 530: Loss = 1.1181\n",
      "Epoch 10 Batch 540: Loss = 1.1474\n",
      "Epoch 10 Batch 550: Loss = 1.2544\n",
      "Epoch 10 Batch 560: Loss = 1.2121\n",
      "Epoch 10 Batch 570: Loss = 1.2605\n",
      "Epoch 10 Batch 580: Loss = 1.0140\n",
      "Epoch 10 Batch 590: Loss = 1.0018\n",
      "Epoch 10 Batch 600: Loss = 1.1273\n",
      "Epoch 10 Batch 610: Loss = 1.0467\n",
      "Epoch 10 Batch 620: Loss = 0.9348\n",
      "Epoch 10 Batch 630: Loss = 1.0265\n",
      "Epoch 10 Batch 640: Loss = 1.1328\n",
      "Epoch 10 Batch 650: Loss = 1.0413\n",
      "Epoch 10 Batch 660: Loss = 1.1340\n",
      "Epoch 10 Batch 670: Loss = 1.2667\n",
      "Epoch 10 Batch 680: Loss = 1.0952\n",
      "Epoch 10 Batch 690: Loss = 0.9619\n",
      "Epoch 10 Batch 700: Loss = 1.0829\n",
      "Epoch 10 Batch 710: Loss = 1.0824\n",
      "Epoch 10 Batch 720: Loss = 1.0212\n",
      "Epoch 10 Batch 730: Loss = 0.9940\n",
      "Epoch 10 Batch 740: Loss = 1.1089\n",
      "Epoch 10 Batch 750: Loss = 1.0150\n",
      "Epoch 10 Batch 760: Loss = 1.1315\n",
      "Epoch 10 Batch 770: Loss = 1.1836\n",
      "Epoch 10 Batch 780: Loss = 1.0466\n",
      "Epoch 10 Batch 790: Loss = 1.3061\n",
      "Epoch 10 Batch 800: Loss = 1.1049\n",
      "Epoch 10 Batch 810: Loss = 1.0131\n",
      "Epoch 10 Batch 820: Loss = 0.9750\n",
      "Epoch 10 Batch 830: Loss = 1.2111\n",
      "Epoch 10 Batch 840: Loss = 1.2591\n",
      "Epoch 10 Batch 850: Loss = 1.1632\n",
      "Epoch 10 Batch 860: Loss = 1.0742\n",
      "Epoch 10 Batch 870: Loss = 1.0945\n",
      "Epoch 10 Batch 880: Loss = 1.2269\n",
      "Epoch 10 Batch 890: Loss = 1.1561\n",
      "Epoch 10 Batch 900: Loss = 1.0455\n",
      "Epoch 10 Batch 910: Loss = 0.9814\n",
      "Epoch 10 Batch 920: Loss = 0.8984\n",
      "Epoch 10 Batch 930: Loss = 1.2189\n",
      "Epoch 10 Batch 940: Loss = 1.2061\n",
      "Epoch 10 Batch 950: Loss = 0.9029\n",
      "Epoch 10 Batch 960: Loss = 1.1554\n",
      "Epoch 10 Batch 970: Loss = 0.9921\n",
      "Epoch 10 Batch 980: Loss = 1.0902\n",
      "Epoch 10 Batch 990: Loss = 1.2084\n",
      "Epoch 10 Batch 1000: Loss = 0.9766\n",
      "Epoch 10 Batch 1010: Loss = 1.0055\n",
      "Epoch 10 Batch 1020: Loss = 1.1480\n",
      "Epoch 10 Batch 1030: Loss = 1.1559\n",
      "Epoch 10 Batch 1040: Loss = 1.2886\n",
      "Epoch 10 Batch 1050: Loss = 1.1864\n",
      "Epoch 10 Batch 1060: Loss = 1.0935\n",
      "Epoch 10 Batch 1070: Loss = 1.1035\n",
      "Epoch 10 Batch 1080: Loss = 1.2103\n",
      "Epoch 10 Batch 1090: Loss = 1.2242\n",
      "Epoch 10 Batch 1100: Loss = 1.1667\n",
      "Epoch 10 Batch 1110: Loss = 1.0183\n",
      "Epoch 10 Batch 1120: Loss = 1.1191\n",
      "Epoch 10 Batch 1130: Loss = 1.0323\n",
      "Epoch 10 Batch 1140: Loss = 1.1816\n",
      "Epoch 10 Batch 1150: Loss = 1.0971\n",
      "Epoch 10 Batch 1160: Loss = 1.0251\n",
      "Epoch 10 Batch 1170: Loss = 1.0871\n",
      "Epoch 10 Batch 1180: Loss = 1.2094\n",
      "Epoch 10 Batch 1190: Loss = 1.0306\n",
      "Epoch 10 Batch 1200: Loss = 1.1379\n",
      "Epoch 10 Batch 1210: Loss = 1.1698\n",
      "Epoch 10 Batch 1220: Loss = 1.1786\n",
      "Epoch 10 Batch 1230: Loss = 0.9455\n",
      "Epoch 10 Batch 1240: Loss = 1.1982\n",
      "Epoch 10 Batch 1250: Loss = 1.0799\n",
      "Epoch 10 Batch 1260: Loss = 1.0994\n",
      "Epoch 10 Batch 1270: Loss = 1.0944\n",
      "Epoch 10 Batch 1280: Loss = 1.0055\n",
      "Epoch 10 Batch 1290: Loss = 1.0074\n",
      "Epoch 10 Batch 1300: Loss = 1.0542\n",
      "Epoch 10 Batch 1310: Loss = 1.2637\n",
      "Epoch 10 Batch 1320: Loss = 1.0621\n",
      "Epoch 10 Batch 1330: Loss = 1.1803\n",
      "Epoch 10 Batch 1340: Loss = 1.0524\n",
      "Epoch 10 Batch 1350: Loss = 1.1669\n",
      "Epoch 10 Batch 1360: Loss = 0.9517\n",
      "Epoch 10 Batch 1370: Loss = 1.2218\n",
      "Epoch 10 Batch 1380: Loss = 1.0660\n",
      "Epoch 10 Batch 1390: Loss = 1.0085\n",
      "Epoch 10 Batch 1400: Loss = 1.1993\n",
      "Epoch 10 Batch 1410: Loss = 1.0251\n",
      "Epoch 10 Batch 1420: Loss = 1.3281\n",
      "Epoch 10 Batch 1430: Loss = 1.0145\n",
      "Epoch 10 Batch 1440: Loss = 1.1039\n",
      "Epoch 10 Batch 1450: Loss = 1.0422\n",
      "Epoch 10 Batch 1460: Loss = 1.1285\n",
      "Epoch 10 Batch 1470: Loss = 1.0730\n",
      "Epoch 10 Batch 1480: Loss = 1.0331\n",
      "Epoch 10 Batch 1490: Loss = 1.0570\n",
      "Epoch 10 Batch 1500: Loss = 1.1485\n",
      "Epoch 10 Batch 1510: Loss = 0.9601\n",
      "Epoch 10 Batch 1520: Loss = 1.1248\n",
      "Epoch 10 Batch 1530: Loss = 1.1021\n",
      "Epoch 10 Batch 1540: Loss = 1.0109\n",
      "Epoch 10 Batch 1550: Loss = 1.1199\n",
      "Epoch 10 Batch 1560: Loss = 1.0263\n",
      "Epoch 10 Batch 1570: Loss = 1.1374\n",
      "Epoch 10 Batch 1580: Loss = 1.1328\n",
      "Epoch 10 Batch 1590: Loss = 1.0174\n",
      "Epoch 10 Batch 1600: Loss = 1.1593\n",
      "Epoch 10 Batch 1610: Loss = 1.1020\n",
      "Epoch 10 Batch 1620: Loss = 1.0374\n",
      "Epoch 10 Batch 1630: Loss = 1.1478\n",
      "Epoch 10 Batch 1640: Loss = 1.2390\n",
      "Epoch 10 Batch 1650: Loss = 1.0707\n",
      "Epoch 10 Batch 1660: Loss = 1.0854\n",
      "Epoch 10 Batch 1670: Loss = 1.0787\n",
      "Epoch 10 Batch 1680: Loss = 1.1279\n",
      "Epoch 10 Batch 1690: Loss = 1.1838\n",
      "Epoch 10 Batch 1700: Loss = 1.1087\n",
      "Epoch 10 Batch 1710: Loss = 1.1116\n",
      "Epoch 10 Batch 1720: Loss = 1.0432\n",
      "Epoch 10 Batch 1730: Loss = 0.9835\n",
      "Epoch 10 Batch 1740: Loss = 1.1899\n",
      "Epoch 10 Batch 1750: Loss = 1.2274\n",
      "Epoch 10 Batch 1760: Loss = 1.2533\n",
      "Epoch 10 Batch 1770: Loss = 1.1200\n",
      "Epoch 10 Batch 1780: Loss = 1.2400\n",
      "Epoch 10 Batch 1790: Loss = 1.0027\n",
      "Epoch 10 Batch 1800: Loss = 1.0705\n",
      "Epoch 10 Batch 1810: Loss = 1.0763\n",
      "Epoch 10 Batch 1820: Loss = 1.1841\n",
      "Epoch 10 Batch 1830: Loss = 1.1006\n",
      "Epoch 10 Batch 1840: Loss = 1.0769\n",
      "Epoch 10 Batch 1850: Loss = 1.0989\n",
      "Epoch 10 Batch 1860: Loss = 1.1100\n",
      "Epoch 10 Batch 1870: Loss = 1.2049\n",
      "Epoch 10 Batch 1880: Loss = 1.0077\n",
      "Epoch 10 Batch 1890: Loss = 1.0290\n",
      "Epoch 10 Batch 1900: Loss = 1.2157\n",
      "Epoch 10 Batch 1910: Loss = 1.1583\n",
      "Epoch 10 Batch 1920: Loss = 1.1325\n",
      "Epoch 10 Batch 1930: Loss = 1.0009\n",
      "Epoch 10 Batch 1940: Loss = 1.0486\n",
      "Epoch 10 Batch 1950: Loss = 1.1317\n",
      "Epoch 10 Batch 1960: Loss = 1.0681\n",
      "Epoch 10 Batch 1970: Loss = 1.2668\n",
      "Epoch 10 Batch 1980: Loss = 1.1870\n",
      "Epoch 10 Batch 1990: Loss = 1.0884\n",
      "Epoch 10 Batch 2000: Loss = 1.0337\n",
      "Epoch 10 Average Loss: 1.1012\n",
      "\n",
      "Generated Text:\n",
      "In a village <EOS> crouch Ting busts triphenylstibine Shibuya Goddin propagates tricolor Pitambari Bustamante senility 720 Sakshi Rameau exasperated Nuon Jesy elifi frittatas straightened ist kitsunebi Perimeter Perimeter Perimeter Perimeter Bysshe Jewish Jewish Jewish Jewish Jewish Jewish Jewish Jewish Jewish Jewish Jewish Jewish Jewish Jewish Jewish Jewish Jewish Shigure Shigure Shigure Dijkstra clades clades\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Main Execution\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Load a generic text corpus from Hugging Face.\n",
    "    # Here we use the WikiText-2 raw dataset.\n",
    "    dataset_hf = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "    # Concatenate all text entries into one large corpus.\n",
    "    corpus = \"\\n\".join(dataset_hf[\"text\"])\n",
    "    print(\"Corpus loaded. Corpus length:\", len(corpus))\n",
    "\n",
    "    # Initialize the tokenizer and dataset.\n",
    "    tokenizer = SimpleTokenizer(corpus)\n",
    "    dataset = TextDataset(corpus, tokenizer, seq_length=128, mask_prob=0.15)\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    # Define device.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Instantiate model.\n",
    "    vocab_size = len(tokenizer.vocab)\n",
    "    model = GPT2Model(vocab_size, d_model=255, n_heads=5, n_layers=4, max_seq_length=128, dropout=0.1)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Define optimizer.\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    # Train the model for 10 epochs.\n",
    "    train(model, dataloader, optimizer, device, epochs=10)\n",
    "    \n",
    "    # Generate sample text.\n",
    "    prompt = \"In a village\"\n",
    "    sample = generate_text(model, tokenizer, prompt, max_length=50, temperature=1.0, device=device)\n",
    "    print(\"\\nGenerated Text:\")\n",
    "    print(sample)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

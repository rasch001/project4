{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report: SwitchHead MOE for Transformers – Architecture and Methodology\n",
    "\n",
    "This report summarizes the SwitchHead method as described in the paper *\"SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention\"* by Róbert Csordás, Piotr Piękos, Kazuki Irie, and Jürgen Schmidhuber (Accepted to NeurIPS 2024, [arXiv:2312.07987](https://doi.org/10.48550/arXiv.2312.07987)). The method introduces an efficient Mixture-of-Experts (MoE) mechanism for the self-attention layer, aiming to reduce both compute and memory requirements while matching the performance of standard Transformers.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Overview\n",
    "\n",
    "Standard Transformer models compute self-attention for every head, leading to a large number of attention matrices and significant computational cost. While several recent works have applied MoE techniques to feedforward layers, applying MoE to the self-attention layer has proven challenging. **SwitchHead** is a novel approach that successfully applies an MoE mechanism to self-attention, reducing the number of attention computations by up to 8 times compared to the standard approach. This results in significant wall-clock speedups and reduced memory usage, without sacrificing language modeling performance.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Motivation and Background\n",
    "\n",
    "In conventional self-attention, for an input sequence $x \\in \\mathbb{R}^{L \\times d}$, each head computes:\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V,\n",
    "$$\n",
    "where the queries, keys, and values are derived as:\n",
    "$$\n",
    "Q = xW^Q,\\quad K = xW^K,\\quad V = xW^V,\n",
    "$$\n",
    "with $W^Q, W^K, W^V \\in \\mathbb{R}^{d \\times d_k}$ and $d_k = d / H$ for $H$ heads.\n",
    "\n",
    "The computational cost grows with the number of heads $H$ and the sequence length $L$. Previous MoE approaches for attention attempted to split the computation across experts but failed to achieve parity with a parameter-matched baseline.\n",
    "\n",
    "SwitchHead addresses these issues by **routing tokens selectively to fewer attention computations**, thereby reducing the number of matrices computed without degrading performance.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. SwitchHead Mechanism\n",
    "\n",
    "### 3.1 Standard Self-Attention Recap\n",
    "\n",
    "For a Transformer with $H$ attention heads, the model computes $H$ sets of attention matrices. For each head $h$, the output is:\n",
    "$$\n",
    "\\text{head}_h(x) = \\text{Attention}(xW_h^Q, xW_h^K, xW_h^V),\n",
    "$$\n",
    "and the concatenated output is projected:\n",
    "$$\n",
    "\\text{MultiHead}(x) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_H)W^O.\n",
    "$$\n",
    "\n",
    "### 3.2 The SwitchHead Approach\n",
    "\n",
    "SwitchHead modifies the attention computation by introducing a gating mechanism that dynamically routes tokens to a subset of attention experts. Instead of computing an attention matrix for every head independently, SwitchHead computes **fewer attention matrices** based on the routing decisions.\n",
    "\n",
    "1. **Gating Function:**  \n",
    "   For each token representation $x_i$, a gating network computes a score over a set of experts:\n",
    "   $$\n",
    "   g(x_i) = \\text{softmax}(W_g x_i),\n",
    "   $$\n",
    "   where $W_g \\in \\mathbb{R}^{d \\times E}$ and $E$ is the number of experts.\n",
    "\n",
    "2. **Expert Selection:**  \n",
    "   For each token, the expert with the highest gate score is selected:\n",
    "   $$\n",
    "   e_i = \\arg\\max g(x_i).\n",
    "   $$\n",
    "   This selection means that tokens sharing similar properties may be processed together by the same expert.\n",
    "\n",
    "3. **Reduced Attention Computations:**  \n",
    "   Instead of computing $H$ full attention heads, SwitchHead **groups tokens** and computes only a subset of attention matrices. Let $M$ be the number of distinct attention matrices computed (with $M \\ll H$). For each group (or expert), the standard attention computation is performed:\n",
    "   $$\n",
    "   \\text{Attention}_{\\text{expert}}(Q_e, K_e, V_e) = \\text{softmax}\\left(\\frac{Q_e K_e^\\top}{\\sqrt{d_k}}\\right)V_e,\n",
    "   $$\n",
    "   where $Q_e, K_e, V_e$ are the queries, keys, and values for tokens routed to expert $e$.\n",
    "\n",
    "4. **Aggregation:**  \n",
    "   The outputs from each expert are then combined to form the final multi-head attention output. This aggregation ensures that despite fewer computations, the model still captures diverse interactions across tokens.\n",
    "\n",
    "By routing tokens and computing fewer attention matrices, SwitchHead achieves significant reductions in computation and memory usage while maintaining model performance.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Efficiency Gains: Compute and Memory Reduction\n",
    "\n",
    "The SwitchHead method is reported to compute up to 8 times fewer attention matrices compared to the standard Transformer. This is achieved by:\n",
    "- **Selective Routing:** Only computing the attention matrix for tokens that are grouped under the same expert.\n",
    "- **Dynamic Allocation:** Adjusting the computation based on the input, so that many tokens can share an expert's attention computation.\n",
    "\n",
    "Mathematically, if a standard Transformer computes:\n",
    "$$\n",
    "\\text{TotalAttentionCost} \\propto H \\times L^2,\n",
    "$$\n",
    "SwitchHead reduces this cost approximately to:\n",
    "$$\n",
    "\\text{TotalAttentionCost}_{\\text{SwitchHead}} \\propto M \\times L^2,\n",
    "$$\n",
    "with $M \\ll H$. The paper reports that for a model with 262M parameters, SwitchHead can achieve comparable perplexity to a standard model using only 44% of the compute and 27% of the memory.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Integration with MoE Feedforward Layers: \"SwitchAll\"\n",
    "\n",
    "SwitchHead can be further extended by combining it with MoE feedforward layers. This fully-MoE Transformer, sometimes referred to as **SwitchAll**, applies mixture-of-experts mechanisms to both the self-attention and feedforward components. The combined approach offers additional efficiency gains while preserving—or even improving—the performance on language modeling and downstream tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Summary\n",
    "\n",
    "SwitchHead represents a significant step forward in making Transformer models more resource-efficient. By applying a Mixture-of-Experts approach to the self-attention layer, it:\n",
    "- **Reduces compute and memory usage:** Up to 8 times fewer attention matrix computations.\n",
    "- **Maintains high performance:** Matches the perplexity of standard Transformers while using less compute.\n",
    "- **Offers flexibility:** Can be combined with MoE feedforward layers to build fully-MoE Transformers (\"SwitchAll\").\n",
    "\n",
    "The SwitchHead mechanism is mathematically grounded in the use of gating functions and selective computation, ensuring that only the most relevant attention matrices are computed. This approach not only accelerates the Transformer but also opens new avenues for scalable and efficient language models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus loaded. Corpus length: 10929707\n",
      "Epoch 1 Batch 0: Loss = 11.4639\n",
      "Epoch 1 Batch 10: Loss = 11.1988\n",
      "Epoch 1 Batch 20: Loss = 10.7967\n",
      "Epoch 1 Batch 30: Loss = 10.1381\n",
      "Epoch 1 Batch 40: Loss = 9.8889\n",
      "Epoch 1 Batch 50: Loss = 9.5045\n",
      "Epoch 1 Batch 60: Loss = 9.2401\n",
      "Epoch 1 Batch 70: Loss = 8.9139\n",
      "Epoch 1 Batch 80: Loss = 8.6633\n",
      "Epoch 1 Batch 90: Loss = 8.2594\n",
      "Epoch 1 Batch 100: Loss = 8.1619\n",
      "Epoch 1 Batch 110: Loss = 8.1151\n",
      "Epoch 1 Batch 120: Loss = 7.5400\n",
      "Epoch 1 Batch 130: Loss = 7.6229\n",
      "Epoch 1 Batch 140: Loss = 7.4033\n",
      "Epoch 1 Batch 150: Loss = 7.1092\n",
      "Epoch 1 Batch 160: Loss = 6.9897\n",
      "Epoch 1 Batch 170: Loss = 6.8470\n",
      "Epoch 1 Batch 180: Loss = 6.9177\n",
      "Epoch 1 Batch 190: Loss = 6.7889\n",
      "Epoch 1 Batch 200: Loss = 6.7306\n",
      "Epoch 1 Batch 210: Loss = 6.6031\n",
      "Epoch 1 Batch 220: Loss = 6.4492\n",
      "Epoch 1 Batch 230: Loss = 6.5301\n",
      "Epoch 1 Batch 240: Loss = 6.2601\n",
      "Epoch 1 Batch 250: Loss = 6.4749\n",
      "Epoch 1 Batch 260: Loss = 6.1158\n",
      "Epoch 1 Batch 270: Loss = 6.2580\n",
      "Epoch 1 Batch 280: Loss = 6.0104\n",
      "Epoch 1 Batch 290: Loss = 6.0601\n",
      "Epoch 1 Batch 300: Loss = 5.9279\n",
      "Epoch 1 Batch 310: Loss = 5.7886\n",
      "Epoch 1 Batch 320: Loss = 5.9689\n",
      "Epoch 1 Batch 330: Loss = 5.7269\n",
      "Epoch 1 Batch 340: Loss = 6.0866\n",
      "Epoch 1 Batch 350: Loss = 6.0328\n",
      "Epoch 1 Batch 360: Loss = 5.8369\n",
      "Epoch 1 Batch 370: Loss = 6.0424\n",
      "Epoch 1 Batch 380: Loss = 5.8021\n",
      "Epoch 1 Batch 390: Loss = 5.6813\n",
      "Epoch 1 Batch 400: Loss = 5.6652\n",
      "Epoch 1 Batch 410: Loss = 5.8392\n",
      "Epoch 1 Batch 420: Loss = 5.6225\n",
      "Epoch 1 Batch 430: Loss = 5.4964\n",
      "Epoch 1 Batch 440: Loss = 5.6476\n",
      "Epoch 1 Batch 450: Loss = 5.6377\n",
      "Epoch 1 Batch 460: Loss = 5.5295\n",
      "Epoch 1 Batch 470: Loss = 5.6498\n",
      "Epoch 1 Batch 480: Loss = 5.6453\n",
      "Epoch 1 Batch 490: Loss = 5.3639\n",
      "Epoch 1 Batch 500: Loss = 5.4934\n",
      "Epoch 1 Batch 510: Loss = 5.1999\n",
      "Epoch 1 Batch 520: Loss = 5.2870\n",
      "Epoch 1 Batch 530: Loss = 5.4141\n",
      "Epoch 1 Batch 540: Loss = 5.2495\n",
      "Epoch 1 Batch 550: Loss = 5.1468\n",
      "Epoch 1 Batch 560: Loss = 5.4141\n",
      "Epoch 1 Batch 570: Loss = 5.3548\n",
      "Epoch 1 Batch 580: Loss = 5.1358\n",
      "Epoch 1 Batch 590: Loss = 5.0700\n",
      "Epoch 1 Batch 600: Loss = 5.0940\n",
      "Epoch 1 Batch 610: Loss = 5.1840\n",
      "Epoch 1 Batch 620: Loss = 4.8633\n",
      "Epoch 1 Batch 630: Loss = 5.0877\n",
      "Epoch 1 Batch 640: Loss = 4.9329\n",
      "Epoch 1 Batch 650: Loss = 5.1832\n",
      "Epoch 1 Batch 660: Loss = 5.0820\n",
      "Epoch 1 Batch 670: Loss = 5.0330\n",
      "Epoch 1 Batch 680: Loss = 5.0732\n",
      "Epoch 1 Batch 690: Loss = 5.1561\n",
      "Epoch 1 Batch 700: Loss = 4.8105\n",
      "Epoch 1 Batch 710: Loss = 5.0532\n",
      "Epoch 1 Batch 720: Loss = 5.1494\n",
      "Epoch 1 Batch 730: Loss = 5.1768\n",
      "Epoch 1 Batch 740: Loss = 4.6875\n",
      "Epoch 1 Batch 750: Loss = 4.6853\n",
      "Epoch 1 Batch 760: Loss = 4.7271\n",
      "Epoch 1 Batch 770: Loss = 5.1986\n",
      "Epoch 1 Batch 780: Loss = 4.6233\n",
      "Epoch 1 Batch 790: Loss = 4.4641\n",
      "Epoch 1 Batch 800: Loss = 4.9290\n",
      "Epoch 1 Batch 810: Loss = 4.7530\n",
      "Epoch 1 Batch 820: Loss = 4.6219\n",
      "Epoch 1 Batch 830: Loss = 4.8128\n",
      "Epoch 1 Batch 840: Loss = 4.7379\n",
      "Epoch 1 Batch 850: Loss = 4.5172\n",
      "Epoch 1 Batch 860: Loss = 4.4246\n",
      "Epoch 1 Batch 870: Loss = 4.4297\n",
      "Epoch 1 Batch 880: Loss = 4.5341\n",
      "Epoch 1 Batch 890: Loss = 4.5885\n",
      "Epoch 1 Batch 900: Loss = 4.6987\n",
      "Epoch 1 Batch 910: Loss = 4.5888\n",
      "Epoch 1 Batch 920: Loss = 4.7641\n",
      "Epoch 1 Batch 930: Loss = 4.6664\n",
      "Epoch 1 Batch 940: Loss = 4.4845\n",
      "Epoch 1 Batch 950: Loss = 4.3987\n",
      "Epoch 1 Batch 960: Loss = 4.2871\n",
      "Epoch 1 Batch 970: Loss = 4.4975\n",
      "Epoch 1 Batch 980: Loss = 4.5242\n",
      "Epoch 1 Batch 990: Loss = 4.2470\n",
      "Epoch 1 Batch 1000: Loss = 4.5718\n",
      "Epoch 1 Batch 1010: Loss = 4.3221\n",
      "Epoch 1 Batch 1020: Loss = 4.5738\n",
      "Epoch 1 Batch 1030: Loss = 4.4901\n",
      "Epoch 1 Batch 1040: Loss = 4.6738\n",
      "Epoch 1 Batch 1050: Loss = 4.3639\n",
      "Epoch 1 Batch 1060: Loss = 4.2922\n",
      "Epoch 1 Batch 1070: Loss = 4.4494\n",
      "Epoch 1 Batch 1080: Loss = 4.4080\n",
      "Epoch 1 Batch 1090: Loss = 4.2372\n",
      "Epoch 1 Batch 1100: Loss = 4.4316\n",
      "Epoch 1 Batch 1110: Loss = 4.6100\n",
      "Epoch 1 Batch 1120: Loss = 4.1793\n",
      "Epoch 1 Batch 1130: Loss = 4.0500\n",
      "Epoch 1 Batch 1140: Loss = 4.1790\n",
      "Epoch 1 Batch 1150: Loss = 4.1883\n",
      "Epoch 1 Batch 1160: Loss = 3.8483\n",
      "Epoch 1 Batch 1170: Loss = 4.2176\n",
      "Epoch 1 Batch 1180: Loss = 4.3092\n",
      "Epoch 1 Batch 1190: Loss = 4.0229\n",
      "Epoch 1 Batch 1200: Loss = 4.5319\n",
      "Epoch 1 Batch 1210: Loss = 4.1124\n",
      "Epoch 1 Batch 1220: Loss = 4.1137\n",
      "Epoch 1 Batch 1230: Loss = 4.0429\n",
      "Epoch 1 Batch 1240: Loss = 3.9218\n",
      "Epoch 1 Batch 1250: Loss = 3.9114\n",
      "Epoch 1 Batch 1260: Loss = 3.8768\n",
      "Epoch 1 Batch 1270: Loss = 4.3219\n",
      "Epoch 1 Batch 1280: Loss = 4.1240\n",
      "Epoch 1 Batch 1290: Loss = 3.9267\n",
      "Epoch 1 Batch 1300: Loss = 4.0744\n",
      "Epoch 1 Batch 1310: Loss = 3.9478\n",
      "Epoch 1 Batch 1320: Loss = 3.8826\n",
      "Epoch 1 Batch 1330: Loss = 4.0718\n",
      "Epoch 1 Batch 1340: Loss = 3.9128\n",
      "Epoch 1 Batch 1350: Loss = 4.0709\n",
      "Epoch 1 Batch 1360: Loss = 4.1574\n",
      "Epoch 1 Batch 1370: Loss = 3.6810\n",
      "Epoch 1 Batch 1380: Loss = 4.0276\n",
      "Epoch 1 Batch 1390: Loss = 3.8964\n",
      "Epoch 1 Batch 1400: Loss = 3.9078\n",
      "Epoch 1 Batch 1410: Loss = 3.9933\n",
      "Epoch 1 Batch 1420: Loss = 3.7480\n",
      "Epoch 1 Batch 1430: Loss = 4.1477\n",
      "Epoch 1 Batch 1440: Loss = 3.7947\n",
      "Epoch 1 Batch 1450: Loss = 3.7692\n",
      "Epoch 1 Batch 1460: Loss = 3.9288\n",
      "Epoch 1 Batch 1470: Loss = 3.5482\n",
      "Epoch 1 Batch 1480: Loss = 3.6786\n",
      "Epoch 1 Batch 1490: Loss = 3.9812\n",
      "Epoch 1 Batch 1500: Loss = 3.9137\n",
      "Epoch 1 Batch 1510: Loss = 3.5962\n",
      "Epoch 1 Batch 1520: Loss = 3.6804\n",
      "Epoch 1 Batch 1530: Loss = 3.8934\n",
      "Epoch 1 Batch 1540: Loss = 3.6106\n",
      "Epoch 1 Batch 1550: Loss = 3.7065\n",
      "Epoch 1 Batch 1560: Loss = 3.7551\n",
      "Epoch 1 Batch 1570: Loss = 3.4253\n",
      "Epoch 1 Batch 1580: Loss = 3.2985\n",
      "Epoch 1 Batch 1590: Loss = 3.8436\n",
      "Epoch 1 Batch 1600: Loss = 3.6902\n",
      "Epoch 1 Batch 1610: Loss = 3.9977\n",
      "Epoch 1 Batch 1620: Loss = 3.6973\n",
      "Epoch 1 Batch 1630: Loss = 3.6662\n",
      "Epoch 1 Batch 1640: Loss = 3.6104\n",
      "Epoch 1 Batch 1650: Loss = 3.5095\n",
      "Epoch 1 Batch 1660: Loss = 3.5796\n",
      "Epoch 1 Batch 1670: Loss = 3.5876\n",
      "Epoch 1 Batch 1680: Loss = 3.5716\n",
      "Epoch 1 Batch 1690: Loss = 3.5367\n",
      "Epoch 1 Batch 1700: Loss = 3.4555\n",
      "Epoch 1 Batch 1710: Loss = 3.3533\n",
      "Epoch 1 Batch 1720: Loss = 3.7379\n",
      "Epoch 1 Batch 1730: Loss = 3.7634\n",
      "Epoch 1 Batch 1740: Loss = 3.7820\n",
      "Epoch 1 Batch 1750: Loss = 3.5170\n",
      "Epoch 1 Batch 1760: Loss = 3.7837\n",
      "Epoch 1 Batch 1770: Loss = 3.4818\n",
      "Epoch 1 Batch 1780: Loss = 3.3809\n",
      "Epoch 1 Batch 1790: Loss = 3.5969\n",
      "Epoch 1 Batch 1800: Loss = 3.4282\n",
      "Epoch 1 Batch 1810: Loss = 3.4074\n",
      "Epoch 1 Batch 1820: Loss = 3.3244\n",
      "Epoch 1 Batch 1830: Loss = 3.4871\n",
      "Epoch 1 Batch 1840: Loss = 3.6105\n",
      "Epoch 1 Batch 1850: Loss = 3.2140\n",
      "Epoch 1 Batch 1860: Loss = 3.3608\n",
      "Epoch 1 Batch 1870: Loss = 3.2013\n",
      "Epoch 1 Batch 1880: Loss = 3.1926\n",
      "Epoch 1 Batch 1890: Loss = 3.3552\n",
      "Epoch 1 Batch 1900: Loss = 3.4425\n",
      "Epoch 1 Batch 1910: Loss = 3.2186\n",
      "Epoch 1 Batch 1920: Loss = 3.6382\n",
      "Epoch 1 Batch 1930: Loss = 3.4019\n",
      "Epoch 1 Batch 1940: Loss = 2.9477\n",
      "Epoch 1 Batch 1950: Loss = 3.8602\n",
      "Epoch 1 Batch 1960: Loss = 3.3153\n",
      "Epoch 1 Batch 1970: Loss = 3.5902\n",
      "Epoch 1 Batch 1980: Loss = 3.1387\n",
      "Epoch 1 Batch 1990: Loss = 2.9757\n",
      "Epoch 1 Batch 2000: Loss = 3.2310\n",
      "Epoch 1 Average Loss: 4.8655\n",
      "Epoch 2 Batch 0: Loss = 3.3948\n",
      "Epoch 2 Batch 10: Loss = 3.1987\n",
      "Epoch 2 Batch 20: Loss = 3.4648\n",
      "Epoch 2 Batch 30: Loss = 3.1951\n",
      "Epoch 2 Batch 40: Loss = 3.2409\n",
      "Epoch 2 Batch 50: Loss = 3.2891\n",
      "Epoch 2 Batch 60: Loss = 2.9447\n",
      "Epoch 2 Batch 70: Loss = 2.9292\n",
      "Epoch 2 Batch 80: Loss = 3.2583\n",
      "Epoch 2 Batch 90: Loss = 3.1076\n",
      "Epoch 2 Batch 100: Loss = 3.4590\n",
      "Epoch 2 Batch 110: Loss = 3.0160\n",
      "Epoch 2 Batch 120: Loss = 3.0458\n",
      "Epoch 2 Batch 130: Loss = 3.1199\n",
      "Epoch 2 Batch 140: Loss = 3.3780\n",
      "Epoch 2 Batch 150: Loss = 3.1689\n",
      "Epoch 2 Batch 160: Loss = 3.2740\n",
      "Epoch 2 Batch 170: Loss = 3.2181\n",
      "Epoch 2 Batch 180: Loss = 3.0475\n",
      "Epoch 2 Batch 190: Loss = 3.4227\n",
      "Epoch 2 Batch 200: Loss = 3.2752\n",
      "Epoch 2 Batch 210: Loss = 3.0229\n",
      "Epoch 2 Batch 220: Loss = 3.1830\n",
      "Epoch 2 Batch 230: Loss = 3.1513\n",
      "Epoch 2 Batch 240: Loss = 3.1125\n",
      "Epoch 2 Batch 250: Loss = 3.4382\n",
      "Epoch 2 Batch 260: Loss = 2.9967\n",
      "Epoch 2 Batch 270: Loss = 3.3010\n",
      "Epoch 2 Batch 280: Loss = 3.3842\n",
      "Epoch 2 Batch 290: Loss = 3.1726\n",
      "Epoch 2 Batch 300: Loss = 2.7717\n",
      "Epoch 2 Batch 310: Loss = 2.9783\n",
      "Epoch 2 Batch 320: Loss = 2.8274\n",
      "Epoch 2 Batch 330: Loss = 2.8453\n",
      "Epoch 2 Batch 340: Loss = 2.7669\n",
      "Epoch 2 Batch 350: Loss = 3.0246\n",
      "Epoch 2 Batch 360: Loss = 2.8333\n",
      "Epoch 2 Batch 370: Loss = 2.9751\n",
      "Epoch 2 Batch 380: Loss = 2.9331\n",
      "Epoch 2 Batch 390: Loss = 2.8477\n",
      "Epoch 2 Batch 400: Loss = 2.8798\n",
      "Epoch 2 Batch 410: Loss = 2.5793\n",
      "Epoch 2 Batch 420: Loss = 3.1430\n",
      "Epoch 2 Batch 430: Loss = 2.8376\n",
      "Epoch 2 Batch 440: Loss = 2.5865\n",
      "Epoch 2 Batch 450: Loss = 2.7628\n",
      "Epoch 2 Batch 460: Loss = 2.9457\n",
      "Epoch 2 Batch 470: Loss = 3.0059\n",
      "Epoch 2 Batch 480: Loss = 2.7755\n",
      "Epoch 2 Batch 490: Loss = 3.0273\n",
      "Epoch 2 Batch 500: Loss = 3.3057\n",
      "Epoch 2 Batch 510: Loss = 3.0774\n",
      "Epoch 2 Batch 520: Loss = 2.8343\n",
      "Epoch 2 Batch 530: Loss = 2.8971\n",
      "Epoch 2 Batch 540: Loss = 2.7730\n",
      "Epoch 2 Batch 550: Loss = 2.7378\n",
      "Epoch 2 Batch 560: Loss = 2.6386\n",
      "Epoch 2 Batch 570: Loss = 2.9728\n",
      "Epoch 2 Batch 580: Loss = 2.6505\n",
      "Epoch 2 Batch 590: Loss = 2.9117\n",
      "Epoch 2 Batch 600: Loss = 2.5468\n",
      "Epoch 2 Batch 610: Loss = 2.8588\n",
      "Epoch 2 Batch 620: Loss = 2.7084\n",
      "Epoch 2 Batch 630: Loss = 2.5104\n",
      "Epoch 2 Batch 640: Loss = 2.9989\n",
      "Epoch 2 Batch 650: Loss = 3.0918\n",
      "Epoch 2 Batch 660: Loss = 2.7409\n",
      "Epoch 2 Batch 670: Loss = 3.0705\n",
      "Epoch 2 Batch 680: Loss = 2.9868\n",
      "Epoch 2 Batch 690: Loss = 2.9092\n",
      "Epoch 2 Batch 700: Loss = 2.5290\n",
      "Epoch 2 Batch 710: Loss = 2.6524\n",
      "Epoch 2 Batch 720: Loss = 2.5305\n",
      "Epoch 2 Batch 730: Loss = 2.7416\n",
      "Epoch 2 Batch 740: Loss = 2.7688\n",
      "Epoch 2 Batch 750: Loss = 2.8531\n",
      "Epoch 2 Batch 760: Loss = 2.8267\n",
      "Epoch 2 Batch 770: Loss = 2.6674\n",
      "Epoch 2 Batch 780: Loss = 2.6555\n",
      "Epoch 2 Batch 790: Loss = 2.7738\n",
      "Epoch 2 Batch 800: Loss = 2.8300\n",
      "Epoch 2 Batch 810: Loss = 2.8758\n",
      "Epoch 2 Batch 820: Loss = 2.7977\n",
      "Epoch 2 Batch 830: Loss = 3.0859\n",
      "Epoch 2 Batch 840: Loss = 2.4693\n",
      "Epoch 2 Batch 850: Loss = 2.5395\n",
      "Epoch 2 Batch 860: Loss = 2.8132\n",
      "Epoch 2 Batch 870: Loss = 2.3276\n",
      "Epoch 2 Batch 880: Loss = 2.6246\n",
      "Epoch 2 Batch 890: Loss = 2.6371\n",
      "Epoch 2 Batch 900: Loss = 2.6672\n",
      "Epoch 2 Batch 910: Loss = 3.0542\n",
      "Epoch 2 Batch 920: Loss = 2.7031\n",
      "Epoch 2 Batch 930: Loss = 2.7594\n",
      "Epoch 2 Batch 940: Loss = 2.7120\n",
      "Epoch 2 Batch 950: Loss = 2.5084\n",
      "Epoch 2 Batch 960: Loss = 2.7274\n",
      "Epoch 2 Batch 970: Loss = 2.3531\n",
      "Epoch 2 Batch 980: Loss = 3.1049\n",
      "Epoch 2 Batch 990: Loss = 2.4205\n",
      "Epoch 2 Batch 1000: Loss = 2.5167\n",
      "Epoch 2 Batch 1010: Loss = 2.6449\n",
      "Epoch 2 Batch 1020: Loss = 2.5231\n",
      "Epoch 2 Batch 1030: Loss = 2.5598\n",
      "Epoch 2 Batch 1040: Loss = 2.5272\n",
      "Epoch 2 Batch 1050: Loss = 2.7852\n",
      "Epoch 2 Batch 1060: Loss = 2.6359\n",
      "Epoch 2 Batch 1070: Loss = 2.5887\n",
      "Epoch 2 Batch 1080: Loss = 2.4807\n",
      "Epoch 2 Batch 1090: Loss = 2.6161\n",
      "Epoch 2 Batch 1100: Loss = 2.5575\n",
      "Epoch 2 Batch 1110: Loss = 2.3064\n",
      "Epoch 2 Batch 1120: Loss = 2.8873\n",
      "Epoch 2 Batch 1130: Loss = 2.5602\n",
      "Epoch 2 Batch 1140: Loss = 2.4420\n",
      "Epoch 2 Batch 1150: Loss = 2.4550\n",
      "Epoch 2 Batch 1160: Loss = 2.8759\n",
      "Epoch 2 Batch 1170: Loss = 2.5054\n",
      "Epoch 2 Batch 1180: Loss = 2.4137\n",
      "Epoch 2 Batch 1190: Loss = 2.3559\n",
      "Epoch 2 Batch 1200: Loss = 2.2552\n",
      "Epoch 2 Batch 1210: Loss = 2.4007\n",
      "Epoch 2 Batch 1220: Loss = 2.4012\n",
      "Epoch 2 Batch 1230: Loss = 2.7331\n",
      "Epoch 2 Batch 1240: Loss = 2.5299\n",
      "Epoch 2 Batch 1250: Loss = 2.7403\n",
      "Epoch 2 Batch 1260: Loss = 2.7671\n",
      "Epoch 2 Batch 1270: Loss = 2.5142\n",
      "Epoch 2 Batch 1280: Loss = 2.5217\n",
      "Epoch 2 Batch 1290: Loss = 2.5789\n",
      "Epoch 2 Batch 1300: Loss = 2.3461\n",
      "Epoch 2 Batch 1310: Loss = 2.3101\n",
      "Epoch 2 Batch 1320: Loss = 2.3098\n",
      "Epoch 2 Batch 1330: Loss = 2.3773\n",
      "Epoch 2 Batch 1340: Loss = 2.3080\n",
      "Epoch 2 Batch 1350: Loss = 2.3408\n",
      "Epoch 2 Batch 1360: Loss = 2.6053\n",
      "Epoch 2 Batch 1370: Loss = 2.3096\n",
      "Epoch 2 Batch 1380: Loss = 2.6012\n",
      "Epoch 2 Batch 1390: Loss = 2.2811\n",
      "Epoch 2 Batch 1400: Loss = 2.2793\n",
      "Epoch 2 Batch 1410: Loss = 2.4904\n",
      "Epoch 2 Batch 1420: Loss = 2.4218\n",
      "Epoch 2 Batch 1430: Loss = 2.1314\n",
      "Epoch 2 Batch 1440: Loss = 2.1290\n",
      "Epoch 2 Batch 1450: Loss = 2.3093\n",
      "Epoch 2 Batch 1460: Loss = 2.2075\n",
      "Epoch 2 Batch 1470: Loss = 2.7486\n",
      "Epoch 2 Batch 1480: Loss = 2.1962\n",
      "Epoch 2 Batch 1490: Loss = 2.4912\n",
      "Epoch 2 Batch 1500: Loss = 2.3406\n",
      "Epoch 2 Batch 1510: Loss = 2.4650\n",
      "Epoch 2 Batch 1520: Loss = 2.3474\n",
      "Epoch 2 Batch 1530: Loss = 2.2780\n",
      "Epoch 2 Batch 1540: Loss = 2.1962\n",
      "Epoch 2 Batch 1550: Loss = 2.3326\n",
      "Epoch 2 Batch 1560: Loss = 2.1044\n",
      "Epoch 2 Batch 1570: Loss = 2.4022\n",
      "Epoch 2 Batch 1580: Loss = 2.3037\n",
      "Epoch 2 Batch 1590: Loss = 2.4489\n",
      "Epoch 2 Batch 1600: Loss = 2.2464\n",
      "Epoch 2 Batch 1610: Loss = 2.2327\n",
      "Epoch 2 Batch 1620: Loss = 2.4565\n",
      "Epoch 2 Batch 1630: Loss = 2.2723\n",
      "Epoch 2 Batch 1640: Loss = 2.3175\n",
      "Epoch 2 Batch 1650: Loss = 2.5172\n",
      "Epoch 2 Batch 1660: Loss = 2.4659\n",
      "Epoch 2 Batch 1670: Loss = 2.3686\n",
      "Epoch 2 Batch 1680: Loss = 2.4269\n",
      "Epoch 2 Batch 1690: Loss = 2.1924\n",
      "Epoch 2 Batch 1700: Loss = 2.4613\n",
      "Epoch 2 Batch 1710: Loss = 2.2988\n",
      "Epoch 2 Batch 1720: Loss = 2.1782\n",
      "Epoch 2 Batch 1730: Loss = 2.4267\n",
      "Epoch 2 Batch 1740: Loss = 2.4300\n",
      "Epoch 2 Batch 1750: Loss = 2.2784\n",
      "Epoch 2 Batch 1760: Loss = 2.1521\n",
      "Epoch 2 Batch 1770: Loss = 2.1857\n",
      "Epoch 2 Batch 1780: Loss = 2.1748\n",
      "Epoch 2 Batch 1790: Loss = 2.1778\n",
      "Epoch 2 Batch 1800: Loss = 2.0580\n",
      "Epoch 2 Batch 1810: Loss = 2.2652\n",
      "Epoch 2 Batch 1820: Loss = 2.1255\n",
      "Epoch 2 Batch 1830: Loss = 2.3310\n",
      "Epoch 2 Batch 1840: Loss = 2.0831\n",
      "Epoch 2 Batch 1850: Loss = 2.1723\n",
      "Epoch 2 Batch 1860: Loss = 2.3651\n",
      "Epoch 2 Batch 1870: Loss = 2.3578\n",
      "Epoch 2 Batch 1880: Loss = 2.1701\n",
      "Epoch 2 Batch 1890: Loss = 2.2062\n",
      "Epoch 2 Batch 1900: Loss = 2.1295\n",
      "Epoch 2 Batch 1910: Loss = 2.1387\n",
      "Epoch 2 Batch 1920: Loss = 2.2502\n",
      "Epoch 2 Batch 1930: Loss = 2.2683\n",
      "Epoch 2 Batch 1940: Loss = 1.9877\n",
      "Epoch 2 Batch 1950: Loss = 2.7479\n",
      "Epoch 2 Batch 1960: Loss = 2.7522\n",
      "Epoch 2 Batch 1970: Loss = 2.1754\n",
      "Epoch 2 Batch 1980: Loss = 2.0253\n",
      "Epoch 2 Batch 1990: Loss = 2.2054\n",
      "Epoch 2 Batch 2000: Loss = 2.0785\n",
      "Epoch 2 Average Loss: 2.6285\n",
      "Epoch 3 Batch 0: Loss = 1.9736\n",
      "Epoch 3 Batch 10: Loss = 2.3035\n",
      "Epoch 3 Batch 20: Loss = 1.9952\n",
      "Epoch 3 Batch 30: Loss = 2.3164\n",
      "Epoch 3 Batch 40: Loss = 2.2058\n",
      "Epoch 3 Batch 50: Loss = 2.1101\n",
      "Epoch 3 Batch 60: Loss = 2.3594\n",
      "Epoch 3 Batch 70: Loss = 2.3663\n",
      "Epoch 3 Batch 80: Loss = 1.9791\n",
      "Epoch 3 Batch 90: Loss = 1.9268\n",
      "Epoch 3 Batch 100: Loss = 2.4357\n",
      "Epoch 3 Batch 110: Loss = 2.3526\n",
      "Epoch 3 Batch 120: Loss = 2.2156\n",
      "Epoch 3 Batch 130: Loss = 2.2097\n",
      "Epoch 3 Batch 140: Loss = 2.1904\n",
      "Epoch 3 Batch 150: Loss = 2.1225\n",
      "Epoch 3 Batch 160: Loss = 2.0093\n",
      "Epoch 3 Batch 170: Loss = 2.1650\n",
      "Epoch 3 Batch 180: Loss = 2.2388\n",
      "Epoch 3 Batch 190: Loss = 2.1396\n",
      "Epoch 3 Batch 200: Loss = 2.2699\n",
      "Epoch 3 Batch 210: Loss = 1.9999\n",
      "Epoch 3 Batch 220: Loss = 1.8514\n",
      "Epoch 3 Batch 230: Loss = 2.0770\n",
      "Epoch 3 Batch 240: Loss = 2.2728\n",
      "Epoch 3 Batch 250: Loss = 2.2424\n",
      "Epoch 3 Batch 260: Loss = 2.3030\n",
      "Epoch 3 Batch 270: Loss = 2.1311\n",
      "Epoch 3 Batch 280: Loss = 2.2622\n",
      "Epoch 3 Batch 290: Loss = 2.1495\n",
      "Epoch 3 Batch 300: Loss = 2.0026\n",
      "Epoch 3 Batch 310: Loss = 2.2085\n",
      "Epoch 3 Batch 320: Loss = 1.7908\n",
      "Epoch 3 Batch 330: Loss = 2.2607\n",
      "Epoch 3 Batch 340: Loss = 2.0523\n",
      "Epoch 3 Batch 350: Loss = 1.9474\n",
      "Epoch 3 Batch 360: Loss = 2.1634\n",
      "Epoch 3 Batch 370: Loss = 1.8360\n",
      "Epoch 3 Batch 380: Loss = 1.9269\n",
      "Epoch 3 Batch 390: Loss = 1.8901\n",
      "Epoch 3 Batch 400: Loss = 1.9487\n",
      "Epoch 3 Batch 410: Loss = 2.1674\n",
      "Epoch 3 Batch 420: Loss = 1.7771\n",
      "Epoch 3 Batch 430: Loss = 2.1455\n",
      "Epoch 3 Batch 440: Loss = 2.1088\n",
      "Epoch 3 Batch 450: Loss = 2.0390\n",
      "Epoch 3 Batch 460: Loss = 2.2740\n",
      "Epoch 3 Batch 470: Loss = 2.2237\n",
      "Epoch 3 Batch 480: Loss = 2.4095\n",
      "Epoch 3 Batch 490: Loss = 1.8333\n",
      "Epoch 3 Batch 500: Loss = 1.9458\n",
      "Epoch 3 Batch 510: Loss = 1.8395\n",
      "Epoch 3 Batch 520: Loss = 2.0384\n",
      "Epoch 3 Batch 530: Loss = 1.9817\n",
      "Epoch 3 Batch 540: Loss = 2.0514\n",
      "Epoch 3 Batch 550: Loss = 1.9788\n",
      "Epoch 3 Batch 560: Loss = 2.0280\n",
      "Epoch 3 Batch 570: Loss = 1.8564\n",
      "Epoch 3 Batch 580: Loss = 2.2578\n",
      "Epoch 3 Batch 590: Loss = 2.1445\n",
      "Epoch 3 Batch 600: Loss = 2.1306\n",
      "Epoch 3 Batch 610: Loss = 2.3330\n",
      "Epoch 3 Batch 620: Loss = 1.9071\n",
      "Epoch 3 Batch 630: Loss = 2.0611\n",
      "Epoch 3 Batch 640: Loss = 1.8129\n",
      "Epoch 3 Batch 650: Loss = 1.8990\n",
      "Epoch 3 Batch 660: Loss = 2.0113\n",
      "Epoch 3 Batch 670: Loss = 2.4105\n",
      "Epoch 3 Batch 680: Loss = 1.9347\n",
      "Epoch 3 Batch 690: Loss = 2.0959\n",
      "Epoch 3 Batch 700: Loss = 1.9878\n",
      "Epoch 3 Batch 710: Loss = 1.8169\n",
      "Epoch 3 Batch 720: Loss = 1.9809\n",
      "Epoch 3 Batch 730: Loss = 2.0555\n",
      "Epoch 3 Batch 740: Loss = 1.8614\n",
      "Epoch 3 Batch 750: Loss = 1.9598\n",
      "Epoch 3 Batch 760: Loss = 2.1112\n",
      "Epoch 3 Batch 770: Loss = 1.9938\n",
      "Epoch 3 Batch 780: Loss = 1.9697\n",
      "Epoch 3 Batch 790: Loss = 1.7833\n",
      "Epoch 3 Batch 800: Loss = 1.8838\n",
      "Epoch 3 Batch 810: Loss = 1.5678\n",
      "Epoch 3 Batch 820: Loss = 2.1595\n",
      "Epoch 3 Batch 830: Loss = 2.0481\n",
      "Epoch 3 Batch 840: Loss = 1.6774\n",
      "Epoch 3 Batch 850: Loss = 2.1682\n",
      "Epoch 3 Batch 860: Loss = 1.9301\n",
      "Epoch 3 Batch 870: Loss = 1.4062\n",
      "Epoch 3 Batch 880: Loss = 2.0527\n",
      "Epoch 3 Batch 890: Loss = 1.7808\n",
      "Epoch 3 Batch 900: Loss = 1.8130\n",
      "Epoch 3 Batch 910: Loss = 1.9982\n",
      "Epoch 3 Batch 920: Loss = 1.7599\n",
      "Epoch 3 Batch 930: Loss = 1.9946\n",
      "Epoch 3 Batch 940: Loss = 1.7229\n",
      "Epoch 3 Batch 950: Loss = 1.8752\n",
      "Epoch 3 Batch 960: Loss = 1.8122\n",
      "Epoch 3 Batch 970: Loss = 2.0228\n",
      "Epoch 3 Batch 980: Loss = 1.8137\n",
      "Epoch 3 Batch 990: Loss = 1.9157\n",
      "Epoch 3 Batch 1000: Loss = 1.8880\n",
      "Epoch 3 Batch 1010: Loss = 1.7369\n",
      "Epoch 3 Batch 1020: Loss = 1.8940\n",
      "Epoch 3 Batch 1030: Loss = 2.0139\n",
      "Epoch 3 Batch 1040: Loss = 1.6735\n",
      "Epoch 3 Batch 1050: Loss = 1.8930\n",
      "Epoch 3 Batch 1060: Loss = 1.8299\n",
      "Epoch 3 Batch 1070: Loss = 1.7193\n",
      "Epoch 3 Batch 1080: Loss = 2.0144\n",
      "Epoch 3 Batch 1090: Loss = 2.1725\n",
      "Epoch 3 Batch 1100: Loss = 1.9133\n",
      "Epoch 3 Batch 1110: Loss = 2.0266\n",
      "Epoch 3 Batch 1120: Loss = 1.8367\n",
      "Epoch 3 Batch 1130: Loss = 1.6951\n",
      "Epoch 3 Batch 1140: Loss = 1.9891\n",
      "Epoch 3 Batch 1150: Loss = 1.8093\n",
      "Epoch 3 Batch 1160: Loss = 1.9534\n",
      "Epoch 3 Batch 1170: Loss = 1.5464\n",
      "Epoch 3 Batch 1180: Loss = 1.8298\n",
      "Epoch 3 Batch 1190: Loss = 1.9970\n",
      "Epoch 3 Batch 1200: Loss = 2.0204\n",
      "Epoch 3 Batch 1210: Loss = 1.8912\n",
      "Epoch 3 Batch 1220: Loss = 1.7640\n",
      "Epoch 3 Batch 1230: Loss = 1.8762\n",
      "Epoch 3 Batch 1240: Loss = 1.6646\n",
      "Epoch 3 Batch 1250: Loss = 2.0511\n",
      "Epoch 3 Batch 1260: Loss = 1.8913\n",
      "Epoch 3 Batch 1270: Loss = 1.9527\n",
      "Epoch 3 Batch 1280: Loss = 1.7896\n",
      "Epoch 3 Batch 1290: Loss = 1.9829\n",
      "Epoch 3 Batch 1300: Loss = 1.7533\n",
      "Epoch 3 Batch 1310: Loss = 1.7728\n",
      "Epoch 3 Batch 1320: Loss = 2.1467\n",
      "Epoch 3 Batch 1330: Loss = 1.6015\n",
      "Epoch 3 Batch 1340: Loss = 1.5325\n",
      "Epoch 3 Batch 1350: Loss = 1.8049\n",
      "Epoch 3 Batch 1360: Loss = 1.8151\n",
      "Epoch 3 Batch 1370: Loss = 2.0684\n",
      "Epoch 3 Batch 1380: Loss = 1.6298\n",
      "Epoch 3 Batch 1390: Loss = 1.7172\n",
      "Epoch 3 Batch 1400: Loss = 1.9872\n",
      "Epoch 3 Batch 1410: Loss = 1.9405\n",
      "Epoch 3 Batch 1420: Loss = 1.7807\n",
      "Epoch 3 Batch 1430: Loss = 2.0000\n",
      "Epoch 3 Batch 1440: Loss = 1.8975\n",
      "Epoch 3 Batch 1450: Loss = 2.0023\n",
      "Epoch 3 Batch 1460: Loss = 1.9374\n",
      "Epoch 3 Batch 1470: Loss = 1.8790\n",
      "Epoch 3 Batch 1480: Loss = 1.8742\n",
      "Epoch 3 Batch 1490: Loss = 1.6856\n",
      "Epoch 3 Batch 1500: Loss = 1.8605\n",
      "Epoch 3 Batch 1510: Loss = 1.9037\n",
      "Epoch 3 Batch 1520: Loss = 1.8696\n",
      "Epoch 3 Batch 1530: Loss = 1.7421\n",
      "Epoch 3 Batch 1540: Loss = 2.0529\n",
      "Epoch 3 Batch 1550: Loss = 1.7143\n",
      "Epoch 3 Batch 1560: Loss = 1.8441\n",
      "Epoch 3 Batch 1570: Loss = 1.9051\n",
      "Epoch 3 Batch 1580: Loss = 2.3800\n",
      "Epoch 3 Batch 1590: Loss = 2.0316\n",
      "Epoch 3 Batch 1600: Loss = 1.9751\n",
      "Epoch 3 Batch 1610: Loss = 1.7848\n",
      "Epoch 3 Batch 1620: Loss = 1.8952\n",
      "Epoch 3 Batch 1630: Loss = 1.6808\n",
      "Epoch 3 Batch 1640: Loss = 1.9630\n",
      "Epoch 3 Batch 1650: Loss = 1.7848\n",
      "Epoch 3 Batch 1660: Loss = 1.8681\n",
      "Epoch 3 Batch 1670: Loss = 1.7909\n",
      "Epoch 3 Batch 1680: Loss = 1.9188\n",
      "Epoch 3 Batch 1690: Loss = 1.6172\n",
      "Epoch 3 Batch 1700: Loss = 1.8553\n",
      "Epoch 3 Batch 1710: Loss = 1.8202\n",
      "Epoch 3 Batch 1720: Loss = 1.6307\n",
      "Epoch 3 Batch 1730: Loss = 1.6123\n",
      "Epoch 3 Batch 1740: Loss = 1.7490\n",
      "Epoch 3 Batch 1750: Loss = 1.6250\n",
      "Epoch 3 Batch 1760: Loss = 1.9896\n",
      "Epoch 3 Batch 1770: Loss = 1.9506\n",
      "Epoch 3 Batch 1780: Loss = 1.6085\n",
      "Epoch 3 Batch 1790: Loss = 2.1168\n",
      "Epoch 3 Batch 1800: Loss = 1.7200\n",
      "Epoch 3 Batch 1810: Loss = 1.7629\n",
      "Epoch 3 Batch 1820: Loss = 1.6780\n",
      "Epoch 3 Batch 1830: Loss = 1.9681\n",
      "Epoch 3 Batch 1840: Loss = 1.6918\n",
      "Epoch 3 Batch 1850: Loss = 1.7358\n",
      "Epoch 3 Batch 1860: Loss = 1.7366\n",
      "Epoch 3 Batch 1870: Loss = 1.7474\n",
      "Epoch 3 Batch 1880: Loss = 1.7451\n",
      "Epoch 3 Batch 1890: Loss = 1.7724\n",
      "Epoch 3 Batch 1900: Loss = 1.6254\n",
      "Epoch 3 Batch 1910: Loss = 1.6121\n",
      "Epoch 3 Batch 1920: Loss = 1.6115\n",
      "Epoch 3 Batch 1930: Loss = 1.4306\n",
      "Epoch 3 Batch 1940: Loss = 1.6511\n",
      "Epoch 3 Batch 1950: Loss = 1.7312\n",
      "Epoch 3 Batch 1960: Loss = 1.8573\n",
      "Epoch 3 Batch 1970: Loss = 1.7654\n",
      "Epoch 3 Batch 1980: Loss = 1.7435\n",
      "Epoch 3 Batch 1990: Loss = 1.9852\n",
      "Epoch 3 Batch 2000: Loss = 1.7974\n",
      "Epoch 3 Average Loss: 1.9323\n",
      "\n",
      "Generated Text:\n",
      "In a village <EOS> Frink listing grounds grounds MAOSEP UN malls diameters near near near near near hormones Edlund useful useful Zeb swoops Atlas Grapes horseman boasted roundtrips xenon inspired Chobits militia survived capitalising captivating eliminating migratory wat conjugated escapes possessing surging rooster melodious abolition Oyebanjo ambushed tightly matrix Clambake ejecting researchers researchers caucus\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "# ----------------------------\n",
    "# Simple Tokenizer Definition\n",
    "# ----------------------------\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, corpus):\n",
    "        # Build vocabulary by splitting on whitespace.\n",
    "        tokens = corpus.split()\n",
    "        unique_tokens = set(tokens)\n",
    "        # Reserve indices for special tokens.\n",
    "        self.vocab = {\"<PAD>\": 0, \"<UNK>\": 1, \"<MASK>\": 2, \"<EOS>\": 3}\n",
    "        for i, token in enumerate(unique_tokens):\n",
    "            self.vocab[token] = i + 4\n",
    "        self.inv_vocab = {i: token for token, i in self.vocab.items()}\n",
    "        self.mask_token_id = self.vocab[\"<MASK>\"]\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = text.split()\n",
    "        token_ids = [self.vocab.get(token, self.vocab[\"<UNK>\"]) for token in tokens] + [self.vocab[\"<EOS>\"]]\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        tokens = [self.inv_vocab.get(i, \"<UNK>\") for i in token_ids]\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "# ----------------------------\n",
    "# Switch FeedForward (Mixture-of-Experts)\n",
    "# ----------------------------\n",
    "class SwitchFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, num_experts=4, hidden_layers=10):\n",
    "        \"\"\"\n",
    "        d_model: model embedding dimension.\n",
    "        num_experts: number of feedforward experts.\n",
    "        hidden_layers: number of (Linear+ReLU) layers per expert.\n",
    "        \"\"\"\n",
    "        super(SwitchFeedForward, self).__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.experts = nn.ModuleList()\n",
    "        for _ in range(num_experts):\n",
    "            layers = []\n",
    "            for _ in range(hidden_layers):\n",
    "                layers.append(nn.Linear(d_model, d_model))\n",
    "                layers.append(nn.ReLU())\n",
    "            self.experts.append(nn.Sequential(*layers))\n",
    "        # The gating network: projects each token's representation to a distribution over experts.\n",
    "        self.gate = nn.Linear(d_model, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (seq_length, batch_size, d_model)\n",
    "        gate_logits = self.gate(x)  # shape: (seq_length, batch_size, num_experts)\n",
    "        # For each token, select the expert with the highest logit.\n",
    "        selected_expert = gate_logits.argmax(dim=-1)  # shape: (seq_length, batch_size)\n",
    "\n",
    "        # Prepare output tensor.\n",
    "        output = torch.zeros_like(x)\n",
    "        seq_length, batch_size, d_model = x.shape\n",
    "\n",
    "        # Route tokens to the appropriate expert.\n",
    "        for expert_idx, expert in enumerate(self.experts):\n",
    "            # Create a mask for tokens routed to this expert.\n",
    "            mask = (selected_expert == expert_idx)  # shape: (seq_length, batch_size)\n",
    "            if mask.sum() == 0:\n",
    "                continue\n",
    "            # Extract tokens for this expert.\n",
    "            # x_expert shape: (num_tokens, d_model)\n",
    "            x_expert = x[mask].view(-1, d_model)\n",
    "            # Process tokens through the expert.\n",
    "            expert_output = expert(x_expert)\n",
    "            # Place the expert outputs back into the output tensor.\n",
    "            output[mask] = expert_output\n",
    "        return output\n",
    "\n",
    "# ----------------------------\n",
    "# Transformer Block with MOE Option\n",
    "# ----------------------------\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1, use_moe=False, num_experts=4, hidden_layers=10):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=n_heads, dropout=dropout)\n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        if use_moe:\n",
    "            self.feedforward = SwitchFeedForward(d_model, num_experts, hidden_layers)\n",
    "        else:\n",
    "            # Standard feedforward network: 10 layers of Linear+ReLU.\n",
    "            ff_layers = []\n",
    "            for _ in range(hidden_layers):\n",
    "                ff_layers.append(nn.Linear(d_model, d_model))\n",
    "                ff_layers.append(nn.ReLU())\n",
    "            self.feedforward = nn.Sequential(*ff_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (seq_length, batch_size, d_model)\n",
    "        attn_output, _ = self.attn(x, x, x)\n",
    "        x = self.layernorm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feedforward(x)\n",
    "        x = self.layernorm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "# ----------------------------\n",
    "# GPT-2–like Model Definition with MOE Option\n",
    "# ----------------------------\n",
    "class GPT2Model(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=255, n_heads=5, n_layers=4, max_seq_length=128, dropout=0.1, use_moe=False):\n",
    "        super(GPT2Model, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_embedding = nn.Embedding(max_seq_length, d_model)\n",
    "        \n",
    "        # Create a stack of transformer blocks.\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, dropout, use_moe=use_moe) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # input_ids: (batch_size, seq_length)\n",
    "        batch_size, seq_length = input_ids.size()\n",
    "        positions = torch.arange(0, seq_length, device=input_ids.device).unsqueeze(0).expand(batch_size, seq_length)\n",
    "        x = self.token_embedding(input_ids) + self.positional_embedding(positions)\n",
    "        x = x.transpose(0, 1)  # (seq_length, batch_size, d_model)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.ln_f(x)\n",
    "        x = x.transpose(0, 1)  # back to (batch_size, seq_length, d_model)\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "# ----------------------------\n",
    "# Dataset Definition\n",
    "# ----------------------------\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, seq_length=128, mask_prob=0.15):\n",
    "        self.tokenizer = tokenizer\n",
    "        tokens = tokenizer.encode(text)\n",
    "        self.sequences = []\n",
    "        for i in range(0, len(tokens) - seq_length, seq_length):\n",
    "            self.sequences.append(tokens[i:i+seq_length])\n",
    "        self.seq_length = seq_length\n",
    "        self.mask_prob = mask_prob\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        input_seq = []\n",
    "        target_seq = []\n",
    "        for token in seq:\n",
    "            if random.random() < self.mask_prob:\n",
    "                input_seq.append(self.tokenizer.mask_token_id)\n",
    "            else:\n",
    "                input_seq.append(token)\n",
    "            target_seq.append(token)\n",
    "        return (torch.tensor(input_seq, dtype=torch.long),\n",
    "                torch.tensor(target_seq, dtype=torch.long))\n",
    "\n",
    "# ----------------------------\n",
    "# Training Setup\n",
    "# ----------------------------\n",
    "def train(model, dataloader, optimizer, device, epochs=3):\n",
    "    model.train()\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch_idx, (input_seq, target_seq) in enumerate(dataloader):\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_seq)  # (batch_size, seq_length, vocab_size)\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), target_seq.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1} Batch {batch_idx}: Loss = {loss.item():.4f}\")\n",
    "        print(f\"Epoch {epoch+1} Average Loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Text Generation Function\n",
    "# ----------------------------\n",
    "def generate_text(model, tokenizer, prompt, max_length=50, temperature=1.0, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    input_ids = torch.tensor(tokenizer.encode(prompt), dtype=torch.long).unsqueeze(0).to(device)\n",
    "    generated = input_ids.tolist()[0]\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            input_seq = input_ids[:, -model.max_seq_length:]\n",
    "            logits = model(input_seq)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probabilities = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "            generated.append(next_token.item())\n",
    "            if next_token.item() == tokenizer.vocab[\"<EOS>\"]:\n",
    "                break\n",
    "    return tokenizer.decode(generated)\n",
    "\n",
    "# ----------------------------\n",
    "# Main Execution with MOE\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the WikiText-2 raw training corpus from Hugging Face.\n",
    "    dataset_hf = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "    corpus = \"\\n\".join(dataset_hf[\"text\"])\n",
    "    print(\"Corpus loaded. Corpus length:\", len(corpus))\n",
    "\n",
    "    tokenizer = SimpleTokenizer(corpus)\n",
    "    dataset_obj = TextDataset(corpus, tokenizer, seq_length=128, mask_prob=0.15)\n",
    "    dataloader = DataLoader(dataset_obj, batch_size=8, shuffle=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    vocab_size = len(tokenizer.vocab)\n",
    "    # Set use_moe=True to enable the Switch Head Mixture-of-Experts in the transformer blocks.\n",
    "    model = GPT2Model(vocab_size, d_model=255, n_heads=5, n_layers=4, max_seq_length=128, dropout=0.1, use_moe=True)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # Train the model.\n",
    "    train(model, dataloader, optimizer, device, epochs=3)\n",
    "\n",
    "    # Generate sample text.\n",
    "    prompt = \"In a village\"\n",
    "    sample = generate_text(model, tokenizer, prompt, max_length=50, temperature=1.0, device=device)\n",
    "    print(\"\\nGenerated Text:\")\n",
    "    print(sample)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

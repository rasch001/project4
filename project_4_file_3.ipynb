{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report: Adversarial Training of a GPT-2 MOE Model with a GAN\n",
    "\n",
    "This report describes the design and operation of a codebase that implements adversarial training for a GPT-2 style generator enhanced with a Mixture-of-Experts (MOE) architecture (SwitchHead) using a Generative Adversarial Network (GAN) framework. The goal is to train a generator to produce realistic text while a discriminator is trained to distinguish between real text and generated text. The adversarial feedback from the discriminator is used to improve the generator.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Overview\n",
    "\n",
    "The code integrates two main components:\n",
    "\n",
    "1. **Generator:**  \n",
    "   A GPT-2–like language model that uses a SwitchHead MOE mechanism in its feedforward layers. It is designed to generate text by predicting the next token given a prompt.\n",
    "\n",
    "2. **Discriminator:**  \n",
    "   A binary classifier (implemented as a bidirectional LSTM) that distinguishes between real text from a dataset and text generated by the generator.\n",
    "\n",
    "The training procedure follows a GAN paradigm where:\n",
    "- The **discriminator** is trained to assign a high score (label 1) to real text and a low score (label 0) to generated (fake) text.\n",
    "- The **generator** is updated with a REINFORCE-style gradient, using the discriminator’s feedback as a reward signal to encourage the production of text that the discriminator considers real.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Components\n",
    "\n",
    "### 2.1 Generator (GPT-2 MOE Model)\n",
    "\n",
    "The generator is a GPT-2 style model that employs a Transformer architecture with the following characteristics:\n",
    "\n",
    "- **Token and Positional Embeddings:**  \n",
    "  Each input token is mapped to a dense vector, and positional information is added to provide context on the token order.\n",
    "\n",
    "- **Transformer Blocks with MOE (SwitchHead):**  \n",
    "  Each Transformer block contains:\n",
    "  - **Multi-Head Self-Attention:**  \n",
    "    Computes attention using multiple heads to capture diverse relationships between tokens.\n",
    "  - **Feedforward Network with Mixture-of-Experts:**  \n",
    "    Instead of a standard feedforward network, a SwitchHead MOE module is used. It employs several experts (each a deep feedforward network) and a gating mechanism to dynamically route tokens to different experts. This reduces the number of attention matrix computations and can improve efficiency.\n",
    "\n",
    "- **Autoregressive Generation:**  \n",
    "  The generator has a `generate` method that takes an initial prompt (a sequence of token IDs) and generates text one token at a time. At each time step, the model computes a probability distribution over the vocabulary for the next token, samples a token (with temperature scaling), and appends it to the sequence. Generation stops when the `<EOS>` token is produced or after a predefined maximum length.\n",
    "\n",
    "### 2.2 Discriminator\n",
    "\n",
    "The discriminator is implemented as a simple LSTM-based binary classifier:\n",
    "- **Embedding Layer:**  \n",
    "  Converts token IDs into dense vectors.\n",
    "- **Bidirectional LSTM:**  \n",
    "  Processes the sequence to capture contextual information from both forward and backward directions.\n",
    "- **Fully Connected Layer:**  \n",
    "  Aggregates the final hidden states from the LSTM and produces a single logit. This logit is used to classify the sequence as real (1) or generated/fake (0).\n",
    "\n",
    "### 2.3 Data Handling\n",
    "\n",
    "- **Tokenizer:**  \n",
    "  A simple whitespace tokenizer builds a vocabulary from the training corpus and supports encoding/decoding between text and token IDs.\n",
    "- **Dataset:**  \n",
    "  The real text dataset is constructed by loading text (e.g., from WikiText-2) and splitting it into fixed-length sequences. This dataset is used to provide real samples for training the discriminator.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. GAN Training Procedure\n",
    "\n",
    "The training loop follows a two-step update process in each iteration:\n",
    "\n",
    "### 3.1 Discriminator Training\n",
    "\n",
    "1. **Real Data Processing:**  \n",
    "   - Real text sequences from the dataset are passed through the discriminator.\n",
    "   - The discriminator computes logits and the binary cross-entropy (BCE) loss is calculated against the target label 1.\n",
    "\n",
    "2. **Fake Data Generation and Processing:**  \n",
    "   - For each batch, the generator produces fake sequences given a fixed prompt (e.g., \"In a village\").\n",
    "   - For each generated sequence, a simplified computation of the log probability is performed.\n",
    "   - The fake sequences are passed through the discriminator, and BCE loss is computed against the target label 0.\n",
    "\n",
    "3. **Total Discriminator Loss:**  \n",
    "   The losses from the real and fake data are summed and used to update the discriminator parameters.\n",
    "\n",
    "### 3.2 Generator Training\n",
    "\n",
    "1. **Reward Signal from Discriminator:**  \n",
    "   - The discriminator’s output (after applying a sigmoid function) is interpreted as a reward signal. A higher value indicates that the discriminator considers the generated sequence as more real.\n",
    "   \n",
    "2. **REINFORCE-Style Update:**  \n",
    "   - The generator’s loss is computed as the negative product of the log probability of the generated sequence and the reward. This encourages the generator to adjust its parameters to produce text that yields a higher reward from the discriminator.\n",
    "   - The loss is averaged over the batch and used to update the generator’s parameters.\n",
    "\n",
    "### 3.3 Alternating Updates\n",
    "\n",
    "- The training loop alternates between updating the discriminator and the generator.\n",
    "- Discriminator updates aim to improve the ability to distinguish real and fake text.\n",
    "- Generator updates aim to generate text that fools the discriminator.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Mathematical Formulation\n",
    "\n",
    "### Discriminator Loss\n",
    "\n",
    "For a batch of size $B$, let:\n",
    "- $x^{(i)}_{\\text{real}}$ be real sequences with target label $y=1$.\n",
    "- $x^{(i)}_{\\text{fake}}$ be generated sequences with target label $y=0$.\n",
    "\n",
    "The binary cross-entropy loss for the discriminator is:\n",
    "$$\n",
    "\\mathcal{L}_D = -\\frac{1}{B} \\sum_{i=1}^{B} \\left[ \\log D(x^{(i)}_{\\text{real}}) + \\log (1 - D(x^{(i)}_{\\text{fake}})) \\right]\n",
    "$$\n",
    "\n",
    "### Generator Loss\n",
    "\n",
    "For the generator, using REINFORCE, let $\\log p(x^{(i)}_{\\text{fake}})$ be the log probability of the generated sequence, and let $r^{(i)} = D(x^{(i)}_{\\text{fake}})$ be the reward from the discriminator. The generator loss is:\n",
    "$$\n",
    "\\mathcal{L}_G = -\\frac{1}{B} \\sum_{i=1}^{B} r^{(i)} \\cdot \\log p(x^{(i)}_{\\text{fake}})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Summary\n",
    "\n",
    "The provided code implements a GAN for text generation where:\n",
    "- The **generator** is a GPT-2 MOE model that leverages a SwitchHead mechanism to efficiently compute self-attention and generate text.\n",
    "- The **discriminator** is an LSTM-based classifier that differentiates between real and generated text.\n",
    "- **Adversarial training** is conducted by alternately updating the discriminator (to improve classification accuracy) and the generator (using a REINFORCE-style gradient based on discriminator rewards).\n",
    "\n",
    "This setup aims to refine the generator’s capability to produce text that closely resembles real text by providing a discriminative signal, thereby improving the quality and realism of the generated language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus loaded. Corpus length: 10929707\n",
      "Encoded prompt: [58660, 39094, 23037, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [86,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [80,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 276\u001b[0m\n\u001b[1;32m    273\u001b[0m disc_optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(discriminator\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;66;03m# Train the GAN.\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m \u001b[43mtrain_gan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisc_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# Generate sample text from the generator after GAN training.\u001b[39;00m\n\u001b[1;32m    279\u001b[0m prompt_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn a village\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[1], line 217\u001b[0m, in \u001b[0;36mtrain_gan\u001b[0;34m(generator, discriminator, real_dataloader, gen_optimizer, disc_optimizer, tokenizer, device, num_epochs)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# Compute log probabilities for generated sequence (simplified)\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 217\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen_seq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# Gather log probabilities for each token in the sequence\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     log_probs_seq \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[1], line 115\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m    113\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (seq_length, batch_size, d_model)\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 115\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f(x)\n\u001b[1;32m    117\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (batch_size, seq_length, d_model)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[1], line 89\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     87\u001b[0m attn_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(x, x, x)\n\u001b[1;32m     88\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attn_output))\n\u001b[0;32m---> 89\u001b[0m ff_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeedforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(ff_output))\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[1], line 62\u001b[0m, in \u001b[0;36mSwitchFeedForward.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m expert_idx, expert \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperts):\n\u001b[1;32m     61\u001b[0m     mask \u001b[38;5;241m=\u001b[39m (selected_expert \u001b[38;5;241m==\u001b[39m expert_idx)  \u001b[38;5;66;03m# (seq_length, batch_size)\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     x_expert \u001b[38;5;241m=\u001b[39m x[mask]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, d_model)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "# ----------------------------\n",
    "# Simple Tokenizer Definition\n",
    "# ----------------------------\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, corpus):\n",
    "        # Build vocabulary by splitting on whitespace.\n",
    "        tokens = corpus.split()\n",
    "        unique_tokens = set(tokens)\n",
    "        # Reserve indices for special tokens.\n",
    "        self.vocab = {\"<PAD>\": 0, \"<UNK>\": 1, \"<MASK>\": 2, \"<EOS>\": 3}\n",
    "        for i, token in enumerate(unique_tokens):\n",
    "            self.vocab[token] = i + 4\n",
    "        self.inv_vocab = {i: token for token, i in self.vocab.items()}\n",
    "        self.mask_token_id = self.vocab[\"<MASK>\"]\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = text.split()\n",
    "        token_ids = [self.vocab.get(token, self.vocab[\"<UNK>\"]) for token in tokens] + [self.vocab[\"<EOS>\"]]\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        tokens = [self.inv_vocab.get(i, \"<UNK>\") for i in token_ids]\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "# ----------------------------\n",
    "# SwitchFeedForward (MOE) and Transformer Block\n",
    "# ----------------------------\n",
    "class SwitchFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, num_experts=4, hidden_layers=10):\n",
    "        \"\"\"\n",
    "        d_model: model embedding dimension.\n",
    "        num_experts: number of feedforward experts.\n",
    "        hidden_layers: number of (Linear+ReLU) layers per expert.\n",
    "        \"\"\"\n",
    "        super(SwitchFeedForward, self).__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.experts = nn.ModuleList()\n",
    "        for _ in range(num_experts):\n",
    "            layers = []\n",
    "            for _ in range(hidden_layers):\n",
    "                layers.append(nn.Linear(d_model, d_model))\n",
    "                layers.append(nn.ReLU())\n",
    "            self.experts.append(nn.Sequential(*layers))\n",
    "        self.gate = nn.Linear(d_model, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (seq_length, batch_size, d_model)\n",
    "        gate_logits = self.gate(x)  # (seq_length, batch_size, num_experts)\n",
    "        selected_expert = gate_logits.argmax(dim=-1)  # (seq_length, batch_size)\n",
    "        output = torch.zeros_like(x)\n",
    "        seq_length, batch_size, d_model = x.shape\n",
    "        for expert_idx, expert in enumerate(self.experts):\n",
    "            mask = (selected_expert == expert_idx)  # (seq_length, batch_size)\n",
    "            if mask.sum() == 0:\n",
    "                continue\n",
    "            x_expert = x[mask].view(-1, d_model)\n",
    "            expert_output = expert(x_expert)\n",
    "            output[mask] = expert_output\n",
    "        return output\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1, use_moe=False, num_experts=4, hidden_layers=10):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=n_heads, dropout=dropout)\n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        if use_moe:\n",
    "            self.feedforward = SwitchFeedForward(d_model, num_experts, hidden_layers)\n",
    "        else:\n",
    "            ff_layers = []\n",
    "            for _ in range(hidden_layers):\n",
    "                ff_layers.append(nn.Linear(d_model, d_model))\n",
    "                ff_layers.append(nn.ReLU())\n",
    "            self.feedforward = nn.Sequential(*ff_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attn(x, x, x)\n",
    "        x = self.layernorm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feedforward(x)\n",
    "        x = self.layernorm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "# ----------------------------\n",
    "# GPT-2-like Generator with MOE (SwitchHead)\n",
    "# ----------------------------\n",
    "class GPT2Model(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=255, n_heads=5, n_layers=4, max_seq_length=128, dropout=0.1, use_moe=False):\n",
    "        super(GPT2Model, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_embedding = nn.Embedding(max_seq_length, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, dropout, use_moe=use_moe) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # input_ids: (batch_size, seq_length)\n",
    "        batch_size, seq_length = input_ids.size()\n",
    "        positions = torch.arange(0, seq_length, device=input_ids.device).unsqueeze(0).expand(batch_size, seq_length)\n",
    "        x = self.token_embedding(input_ids) + self.positional_embedding(positions)\n",
    "        x = x.transpose(0, 1)  # (seq_length, batch_size, d_model)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.ln_f(x)\n",
    "        x = x.transpose(0, 1)  # (batch_size, seq_length, d_model)\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, prompt_ids, max_length=50, temperature=1.0, device=\"cpu\"):\n",
    "        self.eval()\n",
    "        input_ids = prompt_ids.unsqueeze(0).to(device)  # shape: (1, seq_length)\n",
    "        generated = input_ids.tolist()[0]\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                input_seq = input_ids[:, -self.max_seq_length:]\n",
    "                logits = self.forward(input_seq)\n",
    "                logits = logits[:, -1, :] / temperature\n",
    "                probabilities = torch.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "                # Debug: print next token index if it is out-of-range\n",
    "                if torch.max(next_token).item() >= self.head.out_features:\n",
    "                    print(f\"Warning: Out-of-range token generated: {torch.max(next_token).item()} (vocab size: {self.head.out_features})\")\n",
    "                # Clamp next token to ensure it is within valid range\n",
    "                next_token = torch.clamp(next_token, 0, self.head.out_features - 1)\n",
    "                input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "                generated.append(next_token.item())\n",
    "                if next_token.item() == 3:  # <EOS>\n",
    "                    break\n",
    "        return torch.tensor(generated, device=device)\n",
    "\n",
    "# ----------------------------\n",
    "# Discriminator: A simple LSTM-based binary classifier\n",
    "# ----------------------------\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=2, dropout=0.1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)  # bidirectional LSTM\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # input_ids: (batch_size, seq_length)\n",
    "        emb = self.embedding(input_ids)\n",
    "        out, _ = self.lstm(emb)\n",
    "        # Use the last hidden state from both directions\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        logits = self.fc(out)\n",
    "        return logits\n",
    "\n",
    "# ----------------------------\n",
    "# Dataset Definition (real text)\n",
    "# ----------------------------\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, seq_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        tokens = tokenizer.encode(text)\n",
    "        self.sequences = []\n",
    "        for i in range(0, len(tokens) - seq_length, seq_length):\n",
    "            self.sequences.append(tokens[i:i+seq_length])\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        return torch.tensor(seq, dtype=torch.long)\n",
    "\n",
    "# ----------------------------\n",
    "# GAN Training Setup\n",
    "# ----------------------------\n",
    "def train_gan(generator, discriminator, real_dataloader, gen_optimizer, disc_optimizer, tokenizer, device, \n",
    "              num_epochs=3):\n",
    "    bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, real_batch in enumerate(real_dataloader):\n",
    "            real_batch = real_batch.to(device)\n",
    "            batch_size = real_batch.size(0)\n",
    "\n",
    "            ### --- Discriminator Training ---\n",
    "            disc_optimizer.zero_grad()\n",
    "            # Real text processing\n",
    "            real_logits = discriminator(real_batch)\n",
    "            real_labels = torch.ones((batch_size, 1), device=device)\n",
    "            real_loss = bce_loss(real_logits, real_labels)\n",
    "\n",
    "            # Generate fake sequences using the generator with a fixed prompt.\n",
    "            prompt_text = \"In a village\"\n",
    "            encoded_prompt = tokenizer.encode(prompt_text)\n",
    "            print(f\"Encoded prompt: {encoded_prompt}\")\n",
    "            prompt = torch.tensor(encoded_prompt, dtype=torch.long, device=device)\n",
    "            \n",
    "            fake_sequences = []\n",
    "            log_probs = []  # store log probabilities for REINFORCE update\n",
    "            for _ in range(batch_size):\n",
    "                gen_seq = generator.generate(prompt, max_length=generator.max_seq_length, device=device)\n",
    "                fake_sequences.append(gen_seq)\n",
    "                # Compute log probabilities for generated sequence (simplified)\n",
    "                with torch.no_grad():\n",
    "                    logits = generator(gen_seq.unsqueeze(0))\n",
    "                    # Gather log probabilities for each token in the sequence\n",
    "                    log_probs_seq = F.log_softmax(logits, dim=-1)\n",
    "                    # Reshape to (1, seq_length, vocab_size) and gather the corresponding token probabilities\n",
    "                    token_ids = gen_seq.unsqueeze(0).unsqueeze(-1)\n",
    "                    gen_log_prob = log_probs_seq.gather(2, token_ids).sum()\n",
    "                    log_probs.append(gen_log_prob)\n",
    "            fake_sequences = nn.utils.rnn.pad_sequence(fake_sequences, batch_first=True, \n",
    "                                                       padding_value=tokenizer.vocab.get(\"<PAD>\", 0))\n",
    "            fake_logits = discriminator(fake_sequences)\n",
    "            fake_labels = torch.zeros((batch_size, 1), device=device)\n",
    "            fake_loss = bce_loss(fake_logits, fake_labels)\n",
    "\n",
    "            disc_loss = real_loss + fake_loss\n",
    "            disc_loss.backward()\n",
    "            disc_optimizer.step()\n",
    "\n",
    "            ### --- Generator Training ---\n",
    "            gen_optimizer.zero_grad()\n",
    "            # Get rewards from discriminator for generated sequences\n",
    "            rewards = torch.sigmoid(discriminator(fake_sequences)).detach()  # rewards in [0,1]\n",
    "            gen_loss = 0.0\n",
    "            for r, lp in zip(rewards, log_probs):\n",
    "                gen_loss += -r * lp\n",
    "            gen_loss = gen_loss / batch_size\n",
    "            gen_loss.backward()\n",
    "            gen_optimizer.step()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1} Batch {batch_idx}: Disc Loss = {disc_loss.item():.4f}, Gen Loss = {gen_loss.item():.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Main Execution\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Load WikiText-2 raw training corpus from Hugging Face.\n",
    "    dataset_hf = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "    corpus = \"\\n\".join(dataset_hf[\"text\"])\n",
    "    print(\"Corpus loaded. Corpus length:\", len(corpus))\n",
    "\n",
    "    # Initialize tokenizer and dataset.\n",
    "    tokenizer = SimpleTokenizer(corpus)\n",
    "    real_dataset = TextDataset(corpus, tokenizer, seq_length=128)\n",
    "    real_dataloader = DataLoader(real_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    vocab_size = len(tokenizer.vocab)\n",
    "\n",
    "    # Instantiate generator (GPT-2 MOE model with SwitchHead) and discriminator.\n",
    "    generator = GPT2Model(vocab_size, d_model=255, n_heads=5, n_layers=4, max_seq_length=128, dropout=0.1, use_moe=True)\n",
    "    generator.to(device)\n",
    "    discriminator = Discriminator(vocab_size, embed_dim=128, hidden_dim=256, num_layers=2, dropout=0.1)\n",
    "    discriminator.to(device)\n",
    "\n",
    "    # Define optimizers.\n",
    "    gen_optimizer = optim.Adam(generator.parameters(), lr=1e-4)\n",
    "    disc_optimizer = optim.Adam(discriminator.parameters(), lr=1e-4)\n",
    "\n",
    "    # Train the GAN.\n",
    "    train_gan(generator, discriminator, real_dataloader, gen_optimizer, disc_optimizer, tokenizer, device, num_epochs=3)\n",
    "\n",
    "    # Generate sample text from the generator after GAN training.\n",
    "    prompt_text = \"In a village\"\n",
    "    encoded_prompt = tokenizer.encode(prompt_text)\n",
    "    prompt = torch.tensor(encoded_prompt, dtype=torch.long, device=device)\n",
    "    gen_ids = generator.generate(prompt, max_length=50, temperature=1.0, device=device)\n",
    "    print(\"\\nGenerated Text:\")\n",
    "    print(tokenizer.decode(gen_ids.tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
